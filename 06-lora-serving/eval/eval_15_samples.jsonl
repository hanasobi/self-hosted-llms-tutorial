{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Section: AWS DataSync FAQs > When to choose AWS DataSync > When do I use AWS DataSync, and when do I use Amazon S3 Transfer Acceleration?\n\nIf your applications are already integrated with the Amazon S3 API, and you want higher throughput for transferring large files to S3, you can use\nS3 Transfer Acceleration\n. If you want to transfer data from existing storage systems (e.g., Network Attached Storage), or from instruments that cannot be changed (e.g., DNA sequencers, video cameras), or if you want multiple destinations, you use AWS DataSync. DataSync also automates and simplifies the data transfer by providing additional functionality, such as built-in retry and network resiliency mechanisms, data integrity verification, and flexible configuration to suit your specific needs, including bandwidth throttling, etc.\n\nIf you currently use SFTP to exchange data with third parties,\nAWS Transfer Family\nprovides a fully managed SFTP, FTPS, FTP, and AS2 transfer directly into and out of Amazon S3, while reducing your operational burden.\nIf you want an accelerated and automated data transfer between NFS servers, SMB file shares, Hadoop clusters, self-managed or cloud object storage, Amazon S3, Amazon EFS, and Amazon FSx, you can use AWS DataSync. DataSync is ideal for customers who need online migrations for active data sets, timely transfers for continuously generated data, or replication for business continuity.\n\n", "question": "What service can be used to achieve higher throughput for transferring large files to Amazon S3?", "reference_answer": "S3 Transfer Acceleration can be used to achieve higher throughput for transferring large files to Amazon S3.", "prompt_inference": "[INST] Section: AWS DataSync FAQs > When to choose AWS DataSync > When do I use AWS DataSync, and when do I use Amazon S3 Transfer Acceleration?\n\nIf your applications are already integrated with the Amazon S3 API, and you want higher throughput for transferring large files to S3, you can use\nS3 Transfer Acceleration\n. If you want to transfer data from existing storage systems (e.g., Network Attached Storage), or from instruments that cannot be changed (e.g., DNA sequencers, video cameras), or if you want multiple destinations, you use AWS DataSync. DataSync also automates and simplifies the data transfer by providing additional functionality, such as built-in retry and network resiliency mechanisms, data integrity verification, and flexible configuration to suit your specific needs, including bandwidth throttling, etc.\n\nIf you currently use SFTP to exchange data with third parties,\nAWS Transfer Family\nprovides a fully managed SFTP, FTPS, FTP, and AS2 transfer directly into and out of Amazon S3, while reducing your operational burden.\nIf you want an accelerated and automated data transfer between NFS servers, SMB file shares, Hadoop clusters, self-managed or cloud object storage, Amazon S3, Amazon EFS, and Amazon FSx, you can use AWS DataSync. DataSync is ideal for customers who need online migrations for active data sets, timely transfers for continuously generated data, or replication for business continuity.\n\n\n\nQuestion: What service can be used to achieve higher throughput for transferring large files to Amazon S3? [/INST]", "metadata": {"service": "DATASYNC", "doc_type": "Guide", "question_type": "factual", "chunk_id": "datasync-faq-25"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Section: Amazon EC2 FAQs > Billing and purchase options > Billing\n\nQ: How will I be charged and billed for my use of Amazon EC2?\nYou pay only for what you use. Displayed pricing is an hourly rate but depending on which instances you choose, you pay by the hour or second (minimum of 60 seconds) for each instance type. Partial instance-hours consumed are billed based on instance usage. Data transferred between AWS services in different regions is charged at standard inter-region data transfer rates. Usage for other Amazon Web Services is billed separately from Amazon EC2.\nFor EC2 pricing information, please visit the\npricing section on the EC2 detail page\n.\nQ: When does billing of my Amazon EC2 systems begin and end?\nBilling commences when Amazon EC2 initiates the boot sequence of an AMI instance. Billing ends when the instance terminates, which could occur through a web services command, by running \"shutdown -h\", or through instance failure. When you stop an instance, we shut it down but don't charge hourly usage for a stopped instance, or data transfer fees, but we do charge for the storage for any Amazon EBS volumes. To learn more, visit the\nAWS Documentation\n.\nQ: What defines billable EC2 instance usage?\nInstance usages are billed for any time your instances are in a \"running\" state. If you no longer wish to be charged for your instance, you must \"stop\" or \"terminate\" the instance to avoid being billed for additional instance usage. Billing starts when an instance transitions into the running state.\nQ: If I have two instances in different availability zones, how will I be charged for regional data transfer?\nEach instance is charged for its data in and data out at corresponding Data Transfer rates. Therefore, if data is transferred between these two instances, it is charged at \"Data Transfer Out from EC2 to Another AWS Region\" for the first instance and at \"Data Transfer In from Another AWS Region\" for the second instance. Please refer to\nthis page\nfor detailed data transfer pricing.\nQ: If I have two instances in different Regions, how will I be charged for data transfer?\n", "question": "How do charges differ for data transfer between instances in different availability zones compared to instances in different regions?", "reference_answer": "For instances in different availability zones, each instance is charged for its data in and out at corresponding Data Transfer rates without additional regional charges. However, for instances in different regions, the first instance is charged at 'Data Transfer Out from EC2 to Another AWS Region', while the second instance is charged at 'Data Transfer In from Another AWS Region'.", "prompt_inference": "[INST] Section: Amazon EC2 FAQs > Billing and purchase options > Billing\n\nQ: How will I be charged and billed for my use of Amazon EC2?\nYou pay only for what you use. Displayed pricing is an hourly rate but depending on which instances you choose, you pay by the hour or second (minimum of 60 seconds) for each instance type. Partial instance-hours consumed are billed based on instance usage. Data transferred between AWS services in different regions is charged at standard inter-region data transfer rates. Usage for other Amazon Web Services is billed separately from Amazon EC2.\nFor EC2 pricing information, please visit the\npricing section on the EC2 detail page\n.\nQ: When does billing of my Amazon EC2 systems begin and end?\nBilling commences when Amazon EC2 initiates the boot sequence of an AMI instance. Billing ends when the instance terminates, which could occur through a web services command, by running \"shutdown -h\", or through instance failure. When you stop an instance, we shut it down but don't charge hourly usage for a stopped instance, or data transfer fees, but we do charge for the storage for any Amazon EBS volumes. To learn more, visit the\nAWS Documentation\n.\nQ: What defines billable EC2 instance usage?\nInstance usages are billed for any time your instances are in a \"running\" state. If you no longer wish to be charged for your instance, you must \"stop\" or \"terminate\" the instance to avoid being billed for additional instance usage. Billing starts when an instance transitions into the running state.\nQ: If I have two instances in different availability zones, how will I be charged for regional data transfer?\nEach instance is charged for its data in and data out at corresponding Data Transfer rates. Therefore, if data is transferred between these two instances, it is charged at \"Data Transfer Out from EC2 to Another AWS Region\" for the first instance and at \"Data Transfer In from Another AWS Region\" for the second instance. Please refer to\nthis page\nfor detailed data transfer pricing.\nQ: If I have two instances in different Regions, how will I be charged for data transfer?\n\n\nQuestion: How do charges differ for data transfer between instances in different availability zones compared to instances in different regions? [/INST]", "metadata": {"service": "EC2", "doc_type": "Guide", "question_type": "comparison", "chunk_id": "ec2-faq-125"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Databases\n›\nAmazon ElastiCache\n›\nFAQs\n\nGeneral\n6\nServerless\n6\nReserved Nodes\n11\nSecurity\n2\nCompliance\n6\n\nAmazon ElastiCache is a web service that streamlines deployment and running of Valkey, Memcached, or Redis OSS protocol-compliant caches in\nthe cloud\n. ElastiCache improves application performance by allowing you to retrieve information from a fast, managed, in-memory system instead of relying on slower disk-based systems. The service simplifies and offloads the management, monitoring, and operation of in-memory environments, enabling your engineering resources to focus on developing applications. Using ElastiCache, you can not only improve load and response times to user actions and queries but also reduce the cost associated with scaling web applications.\nElastiCache automates common administrative tasks required to operate a distributed in-memory key-value environment. Using ElastiCache, you can add a caching or in-memory layer to your application architecture in a matter of minutes with a few steps in the AWS Management Console. ElastiCache is designed to automatically maintain high availability, and provides a 99.99% availability Service Level Agreement (SLA). ElastiCache is protocol compliant with Valkey, Memcached, and Redis OSS, so code, applications, and popular tools that you use today with your existing Valkey, Memcached, or Redis OSS environments will seamlessly work with the service. There are no up-front investments required, and you pay only for the resources you use.\n\nThe in-memory caching provided by ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, and Q&A portals) or compute-intensive workloads (such as a recommendation engine). In-memory caching improves application performance by storing critical pieces of data in memory for low-latency access. Cached information can include the results of I/O-intensive database queries or the results of computationally intensive calculations.\n\n", "question": "How does Amazon ElastiCache improve application performance?", "reference_answer": "Amazon ElastiCache improves application performance by allowing retrieval of information from a fast, managed, in-memory system instead of relying on slower disk-based systems. It stores critical pieces of data in memory for low-latency access, which significantly improves latency and throughput for many read-heavy and compute-intensive application workloads.", "prompt_inference": "[INST] Databases\n›\nAmazon ElastiCache\n›\nFAQs\n\nGeneral\n6\nServerless\n6\nReserved Nodes\n11\nSecurity\n2\nCompliance\n6\n\nAmazon ElastiCache is a web service that streamlines deployment and running of Valkey, Memcached, or Redis OSS protocol-compliant caches in\nthe cloud\n. ElastiCache improves application performance by allowing you to retrieve information from a fast, managed, in-memory system instead of relying on slower disk-based systems. The service simplifies and offloads the management, monitoring, and operation of in-memory environments, enabling your engineering resources to focus on developing applications. Using ElastiCache, you can not only improve load and response times to user actions and queries but also reduce the cost associated with scaling web applications.\nElastiCache automates common administrative tasks required to operate a distributed in-memory key-value environment. Using ElastiCache, you can add a caching or in-memory layer to your application architecture in a matter of minutes with a few steps in the AWS Management Console. ElastiCache is designed to automatically maintain high availability, and provides a 99.99% availability Service Level Agreement (SLA). ElastiCache is protocol compliant with Valkey, Memcached, and Redis OSS, so code, applications, and popular tools that you use today with your existing Valkey, Memcached, or Redis OSS environments will seamlessly work with the service. There are no up-front investments required, and you pay only for the resources you use.\n\nThe in-memory caching provided by ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, and Q&A portals) or compute-intensive workloads (such as a recommendation engine). In-memory caching improves application performance by storing critical pieces of data in memory for low-latency access. Cached information can include the results of I/O-intensive database queries or the results of computationally intensive calculations.\n\n\n\nQuestion: How does Amazon ElastiCache improve application performance? [/INST]", "metadata": {"service": "ELASTICACHE", "doc_type": "Guide", "question_type": "conceptual", "chunk_id": "elasticache-faq-0"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Section: Amazon DynamoDB FAQs > Security > Does DynamoDB support audit logging for item level changes on the table?\n\nYes, DynamoDB does support audit logging for item level changes on tables.\nDynamoDB is integrated with AWS CloudTrail\n, a service that provides a record of actions taken by a user, role, or an AWS service in DynamoDB at the item level. Additional logging data captured includes creates, updates, deletes, and any conditional check failures. Customers can access these log records stored in CloudWatch Logs and build applications to analyze item level changes for auditing, monitoring, or other purposes. Audit logging provides visibility into data changes at a granular level without impacting normal read/write performance of the DynamoDB table.\n\nYes, you can use Amazon DynamoDB to build\nHIPAA-compliant\napplications and store healthcare-related information, including protected health information under an executed Business Associate Agreement (BAA) with AWS.\n\nDynamoDB fulfills many compliance certifications including\nHIPAA eligible\n, FedRAMP, ISO 27001, SOC 1/SSAE 16/ISAE 3402 (formerly SAS 70), and SOC 2. For more information, see\nCompliance validation by industry for DynamoDB\n. AWS compliance reports are available for download in\nAWS Artifact\n.\n\nAmazon DynamoDB global tables is a fully managed, serverless, multi-Region, and multi-active database. Global tables provides\n99.999% availability\n, increased application resiliency, and improved business continuity. It replicates your DynamoDB tables automatically across your choice of AWS Regions, so you can achieve fast local read and write performance. Global tables use the same APIs as single Region DynamoDB tables, so you can easily make your DynamoDB tables globally available without application changes.\nA global table is a collection of one or more replica tables, all owned by a single AWS account. A single Amazon DynamoDB global table can only have one replica table per AWS Region.\n\nYou should use global tables to improve your application's resiliency across multiple Regions. Global tables also enables applications to maintain high availability in the unlikely event of isolation or degradation of an entire Region.\n\n", "question": "Does DynamoDB support audit logging for item level changes on tables?", "reference_answer": "Yes, DynamoDB does support audit logging for item level changes on tables. It is integrated with AWS CloudTrail, which provides a record of actions taken by a user, role, or an AWS service in DynamoDB at the item level.", "prompt_inference": "[INST] Section: Amazon DynamoDB FAQs > Security > Does DynamoDB support audit logging for item level changes on the table?\n\nYes, DynamoDB does support audit logging for item level changes on tables.\nDynamoDB is integrated with AWS CloudTrail\n, a service that provides a record of actions taken by a user, role, or an AWS service in DynamoDB at the item level. Additional logging data captured includes creates, updates, deletes, and any conditional check failures. Customers can access these log records stored in CloudWatch Logs and build applications to analyze item level changes for auditing, monitoring, or other purposes. Audit logging provides visibility into data changes at a granular level without impacting normal read/write performance of the DynamoDB table.\n\nYes, you can use Amazon DynamoDB to build\nHIPAA-compliant\napplications and store healthcare-related information, including protected health information under an executed Business Associate Agreement (BAA) with AWS.\n\nDynamoDB fulfills many compliance certifications including\nHIPAA eligible\n, FedRAMP, ISO 27001, SOC 1/SSAE 16/ISAE 3402 (formerly SAS 70), and SOC 2. For more information, see\nCompliance validation by industry for DynamoDB\n. AWS compliance reports are available for download in\nAWS Artifact\n.\n\nAmazon DynamoDB global tables is a fully managed, serverless, multi-Region, and multi-active database. Global tables provides\n99.999% availability\n, increased application resiliency, and improved business continuity. It replicates your DynamoDB tables automatically across your choice of AWS Regions, so you can achieve fast local read and write performance. Global tables use the same APIs as single Region DynamoDB tables, so you can easily make your DynamoDB tables globally available without application changes.\nA global table is a collection of one or more replica tables, all owned by a single AWS account. A single Amazon DynamoDB global table can only have one replica table per AWS Region.\n\nYou should use global tables to improve your application's resiliency across multiple Regions. Global tables also enables applications to maintain high availability in the unlikely event of isolation or degradation of an entire Region.\n\n\n\nQuestion: Does DynamoDB support audit logging for item level changes on tables? [/INST]", "metadata": {"service": "DYNAMODB", "doc_type": "Guide", "question_type": "factual", "chunk_id": "dynamodb-faq-6"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Section: AWS Key Management Service FAQs > External key store > Is automatic key rotation possible with XKS keys?\n\nYes. Automatic key rotation is supported by the XKS specification, and it is a capability provided by most vendors that support XKS. Automatic rotation for XKS keys occurs entirely in the external key manager and works in a similar way to the automatic key rotation for AWS KMS keys created and managed within KMS. When you use a rotated KMS XKS key to encrypt data, your external key manager uses the current key material. When you use the rotated XKS key to decrypt ciphertext, your external key manager uses the version of the key material that was used to encrypt it. As long as previous XKS keys used to create earlier ciphertexts are still enabled in external key manager, you will be able to successfully make Decrypt API request under those versions of you XKS keys.\n\nFor services that do not cache keys, the next API call using this XKS KMS key will fail. Some services implement data key caching or other key derivation schemes for performance, latency, or KMS cost management. Caching of these keys can vary from 5 mins to 24 hrs. Any protected resource that is currently in use (such as RDS database or EC2 instance) will respond differently after you deny access to the key. See the relevant AWS service documentation for details.\n\nTo authenticate to your external key store proxy, AWS KMS signs all requests to the proxy using AWS SigV4 credentials that you configure on your proxy and provide to KMS. AWS KMS authenticates your external key store proxy using server-side TLS certificates. Optionally, your proxy can enable mutual TLS for additional assurance that it only accepts requests from AWS KMS.\n\n", "question": "What happens to API calls using an XKS KMS key if the key is denied access?", "reference_answer": "For services that do not cache keys, the next API call using the denied XKS KMS key will fail. However, some services implement data key caching, which can allow for continued operation for a certain period, varying from 5 minutes to 24 hours, depending on the caching scheme in place.", "prompt_inference": "[INST] Section: AWS Key Management Service FAQs > External key store > Is automatic key rotation possible with XKS keys?\n\nYes. Automatic key rotation is supported by the XKS specification, and it is a capability provided by most vendors that support XKS. Automatic rotation for XKS keys occurs entirely in the external key manager and works in a similar way to the automatic key rotation for AWS KMS keys created and managed within KMS. When you use a rotated KMS XKS key to encrypt data, your external key manager uses the current key material. When you use the rotated XKS key to decrypt ciphertext, your external key manager uses the version of the key material that was used to encrypt it. As long as previous XKS keys used to create earlier ciphertexts are still enabled in external key manager, you will be able to successfully make Decrypt API request under those versions of you XKS keys.\n\nFor services that do not cache keys, the next API call using this XKS KMS key will fail. Some services implement data key caching or other key derivation schemes for performance, latency, or KMS cost management. Caching of these keys can vary from 5 mins to 24 hrs. Any protected resource that is currently in use (such as RDS database or EC2 instance) will respond differently after you deny access to the key. See the relevant AWS service documentation for details.\n\nTo authenticate to your external key store proxy, AWS KMS signs all requests to the proxy using AWS SigV4 credentials that you configure on your proxy and provide to KMS. AWS KMS authenticates your external key store proxy using server-side TLS certificates. Optionally, your proxy can enable mutual TLS for additional assurance that it only accepts requests from AWS KMS.\n\n\n\nQuestion: What happens to API calls using an XKS KMS key if the key is denied access? [/INST]", "metadata": {"service": "KMS", "doc_type": "Guide", "question_type": "comparison", "chunk_id": "kms-faq-20"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Section: Amazon Kinesis Data Streams FAQs > Provisioned mode > What happens if the capacity limits of an Amazon Kinesis data stream are exceeded while the data producer adds data to the data stream in provisioned mode?\n\nIn provisioned mode, the capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded either by data throughput or by the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception. If this is due to a temporary rise of the data stream’s input data rate, retry by the data producer will eventually lead to completion of the requests. If it’s due to a sustained rise of the data stream’s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed. In both cases, Amazon CloudWatch metrics allow you to learn about the change of the data stream’s input data rate and the occurrence of ProvisionedThroughputExceeded exceptions.\n\nIn provisioned mode, the capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded either by data throughput or by the number of read data calls. While the capacity limits are exceeded, the read data call will be rejected with a ProvisionedThroughputExceeded exception. If this is due to a temporary rise of the data stream’s output data rate, retry by the Amazon Kinesis application will eventually lead to completion of the requests. If it’s due to a sustained rise of the data stream’s output data rate, you should increase the number of shards within your data stream to provide enough capacity for the read data calls to consistently succeed. In both cases, Amazon CloudWatch metrics allow you to learn about the change of the data stream’s output data rate and the occurrence of ProvisionedThroughputExceeded exceptions.\n\nThe default retention period of 24 hours covers scenarios where intermittent lags in processing require catch-up with the real-time data. A seven-day retention lets you reprocess data for up to seven days to resolve potential downstream data losses. Long-term data retention greater than seven days and up to 365 days lets you reprocess old data for use cases such as algorithm back testing, data store backfills, and auditing.\n\n", "question": "What is the default retention period for a Kinesis data stream?", "reference_answer": "The default retention period for a Kinesis data stream is 24 hours.", "prompt_inference": "[INST] Section: Amazon Kinesis Data Streams FAQs > Provisioned mode > What happens if the capacity limits of an Amazon Kinesis data stream are exceeded while the data producer adds data to the data stream in provisioned mode?\n\nIn provisioned mode, the capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded either by data throughput or by the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception. If this is due to a temporary rise of the data stream’s input data rate, retry by the data producer will eventually lead to completion of the requests. If it’s due to a sustained rise of the data stream’s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed. In both cases, Amazon CloudWatch metrics allow you to learn about the change of the data stream’s input data rate and the occurrence of ProvisionedThroughputExceeded exceptions.\n\nIn provisioned mode, the capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded either by data throughput or by the number of read data calls. While the capacity limits are exceeded, the read data call will be rejected with a ProvisionedThroughputExceeded exception. If this is due to a temporary rise of the data stream’s output data rate, retry by the Amazon Kinesis application will eventually lead to completion of the requests. If it’s due to a sustained rise of the data stream’s output data rate, you should increase the number of shards within your data stream to provide enough capacity for the read data calls to consistently succeed. In both cases, Amazon CloudWatch metrics allow you to learn about the change of the data stream’s output data rate and the occurrence of ProvisionedThroughputExceeded exceptions.\n\nThe default retention period of 24 hours covers scenarios where intermittent lags in processing require catch-up with the real-time data. A seven-day retention lets you reprocess data for up to seven days to resolve potential downstream data losses. Long-term data retention greater than seven days and up to 365 days lets you reprocess old data for use cases such as algorithm back testing, data store backfills, and auditing.\n\n\n\nQuestion: What is the default retention period for a Kinesis data stream? [/INST]", "metadata": {"service": "KINESIS", "doc_type": "Guide", "question_type": "factual", "chunk_id": "kinesis-faq-9"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Section: Amazon ElastiCache FAQs > Read Replica > What does it mean to run a node as a read replica?\n\nRead replicas serve two purposes in Redis OSS:\nFailure handling\nRead scaling\nWhen you run a cache with a read replica, the primary serves both writes and reads. The replica serves exclusively read traffic and is also available as a warm standby in the event that the primary becomes impaired.\n\nWith ElastiCache Serverless, read replicas are automatically maintained by the service. When designing your own cache, there are a variety of scenarios where deploying one or more read replicas for a given primary node might make sense. Common reasons for deploying a read replica include:\nScaling beyond the compute or I/O capacity of a single primary node for read-heavy workloads: This excess read traffic can be directed to one or more read replicas.\nServing read traffic while the primary is unavailable: If your primary node cannot take I/O requests (for example, due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your read replicas. For this use case, keep in mind that the data on the read replica might be stale, since the primary instance is unavailable. The read replica can also be used warmed up to restart a failed primary.\nData protection scenarios: In the unlikely event of primary node failure or that the AZ in which your primary node resides becomes unavailable, you can promote a read replica in a different AZ to become the new primary.\n\nYou can connect to a read replica just as you would connect to a primary cache node. If you have multiple read replicas, it is up to your application to determine how read traffic will be distributed amongst them. Here are more details:\nRedis (cluster mode disabled) OSS clusters, use the individual node endpoints for read operations. (In the API/CLI these are referred to as read endpoints.)\nRedis OSS (cluster mode enabled) clusters, use the cluster's configuration endpoint for all operations. You can still read from individual node endpoints. (In the API and CLI these are referred to as read endpoints.)\n\nElastiCache allows you to create up to five (5) read replicas for a given primary cache node.\n\n", "question": "Why might someone choose to deploy read replicas for a primary node in ElastiCache?", "reference_answer": "Someone might choose to deploy read replicas for a primary node in ElastiCache to scale beyond the compute or I/O capacity of a single primary node for read-heavy workloads, to serve read traffic while the primary is unavailable, or for data protection scenarios in the event of primary node failure or availability issues in the availability zone (AZ) where the primary node resides.", "prompt_inference": "[INST] Section: Amazon ElastiCache FAQs > Read Replica > What does it mean to run a node as a read replica?\n\nRead replicas serve two purposes in Redis OSS:\nFailure handling\nRead scaling\nWhen you run a cache with a read replica, the primary serves both writes and reads. The replica serves exclusively read traffic and is also available as a warm standby in the event that the primary becomes impaired.\n\nWith ElastiCache Serverless, read replicas are automatically maintained by the service. When designing your own cache, there are a variety of scenarios where deploying one or more read replicas for a given primary node might make sense. Common reasons for deploying a read replica include:\nScaling beyond the compute or I/O capacity of a single primary node for read-heavy workloads: This excess read traffic can be directed to one or more read replicas.\nServing read traffic while the primary is unavailable: If your primary node cannot take I/O requests (for example, due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your read replicas. For this use case, keep in mind that the data on the read replica might be stale, since the primary instance is unavailable. The read replica can also be used warmed up to restart a failed primary.\nData protection scenarios: In the unlikely event of primary node failure or that the AZ in which your primary node resides becomes unavailable, you can promote a read replica in a different AZ to become the new primary.\n\nYou can connect to a read replica just as you would connect to a primary cache node. If you have multiple read replicas, it is up to your application to determine how read traffic will be distributed amongst them. Here are more details:\nRedis (cluster mode disabled) OSS clusters, use the individual node endpoints for read operations. (In the API/CLI these are referred to as read endpoints.)\nRedis OSS (cluster mode enabled) clusters, use the cluster's configuration endpoint for all operations. You can still read from individual node endpoints. (In the API and CLI these are referred to as read endpoints.)\n\nElastiCache allows you to create up to five (5) read replicas for a given primary cache node.\n\n\n\nQuestion: Why might someone choose to deploy read replicas for a primary node in ElastiCache? [/INST]", "metadata": {"service": "ELASTICACHE", "doc_type": "Guide", "question_type": "conceptual", "chunk_id": "elasticache-faq-29"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Section: Amazon Lookout for Vision FAQs > General > Q: My training has failed. Will I be charged?\n\nNo. You will not be charged for the compute resources if your training fails.\n\nThe number of parallel inference compute resources needed depends on how many images you need to process at a given point in time. The throughput of a single resource depends on factors including the size of the images, the complexity of those images, and the complexity of the defect detection model. We recommend that you monitor the frequency at which you need to provision your model, and the number of images that need to be processed at a single time, in order to schedule provisioning of your model most efficiently. After you complete your production line integration, you should start provisioning your model at a scheduled time, process all your images, and then stop provisioning. If you don’t stop provisioning, you will be charged even if no images are processed.\n\nAmazon Lookout for Vision currently supports the JPEG and PNG image formats. You can submit images as a byte array.\n\nAmazon Lookout for Vision supports image file sizes up to 5 MB.\n\nYou can use Amazon Lookout for Vision with Amazon Augmented AI (Amazon A2I) so you can route low confidence predictions from Amazon Lookout for Vision to human reviewers (process engineers, quality managers, or operators). You can specify the conditions under which Amazon A2I routes predictions to reviewers, which can be either a confidence score threshold, or a random sampling percentage. You can adjust these thresholds to achieve the right balance between accuracy and cost- effectiveness. This can help you implement audits to monitor the prediction accuracy regularly. Amazon A2I also provides reviewers with a web interface consisting of all the instructions and tools they need to complete their review tasks. For more information about implementing human review with Amazon Lookout for Vision, see the\nAmazon A2I webpage\n.\n\nYes, Amazon Lookout for Vision models can be used on the edge using AWS IoT Greengrass.  For more information, see the\nGetting Started Guide\n.\n\n", "question": "What type of data should be used for Amazon Lookout for Equipment?", "reference_answer": "The data set should be time series data generated from an industrial asset such as a pump, compressor, or motor. It should be generated from sensors (tags) and representative of the condition and/or operation of the asset.", "prompt_inference": "[INST] Section: Amazon Lookout for Vision FAQs > General > Q: My training has failed. Will I be charged?\n\nNo. You will not be charged for the compute resources if your training fails.\n\nThe number of parallel inference compute resources needed depends on how many images you need to process at a given point in time. The throughput of a single resource depends on factors including the size of the images, the complexity of those images, and the complexity of the defect detection model. We recommend that you monitor the frequency at which you need to provision your model, and the number of images that need to be processed at a single time, in order to schedule provisioning of your model most efficiently. After you complete your production line integration, you should start provisioning your model at a scheduled time, process all your images, and then stop provisioning. If you don’t stop provisioning, you will be charged even if no images are processed.\n\nAmazon Lookout for Vision currently supports the JPEG and PNG image formats. You can submit images as a byte array.\n\nAmazon Lookout for Vision supports image file sizes up to 5 MB.\n\nYou can use Amazon Lookout for Vision with Amazon Augmented AI (Amazon A2I) so you can route low confidence predictions from Amazon Lookout for Vision to human reviewers (process engineers, quality managers, or operators). You can specify the conditions under which Amazon A2I routes predictions to reviewers, which can be either a confidence score threshold, or a random sampling percentage. You can adjust these thresholds to achieve the right balance between accuracy and cost- effectiveness. This can help you implement audits to monitor the prediction accuracy regularly. Amazon A2I also provides reviewers with a web interface consisting of all the instructions and tools they need to complete their review tasks. For more information about implementing human review with Amazon Lookout for Vision, see the\nAmazon A2I webpage\n.\n\nYes, Amazon Lookout for Vision models can be used on the edge using AWS IoT Greengrass.  For more information, see the\nGetting Started Guide\n.\n\n\n\nQuestion: What type of data should be used for Amazon Lookout for Equipment? [/INST]", "metadata": {"service": "LOOKOUT", "doc_type": "Guide", "question_type": "factual", "chunk_id": "lookout-faq-3"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Section: Amazon SQS FAQs > Features, functionality, and interfaces > What is the AmazonSQSBufferedAsyncClient for Java?\n\nThe AmazonSQSBufferedAsyncClient for Java provides an implementation of the AmazonSQSAsyncClient interface and adds several important features:\nAutomatic batching of multiple SendMessage, DeleteMessage, or ChangeMessageVisibility requests without any required changes to the application\nPrefetching of messages into a local buffer that allows your application to immediately process messages from Amazon SQS without waiting for the messages to be retrieved\nWorking together, automatic batching and prefetching increase the throughput and reduce the latency of your application while reducing your costs by making fewer Amazon SQS requests. For more information, see\nClient-Side Buffering and Request Batching\nin the\nAmazon SQS Developer Guide\n.\n\nYou can download the AmazonSQSBufferedAsyncClient as part of the\nAWS SDK for Java\n.\n\nNo. The AmazonSQSBufferedAsyncClient for Java is implemented as a drop-in replacement for the existing AmazonSQSAsyncClient.\nIf you update your application to use the latest AWS SDK and change your client to use the AmazonSQSBufferedAsyncClient for Java instead of the AmazonSQSAsyncClient, your application will receive the added benefits of automatic batching and prefetching.\n\nIn the Amazon SQS console, select an Amazon SQS standard queue.\nUnder Queue Actions, select Subscribe Queue to SNS Topic from the drop-down list.\nIn the dialog box, select the topic from the Choose a Topic drop-down list, and click Subscribe.\nFor more information, see\nSubscribing a Queue to an Amazon SNS Topic\nin the\nAmazon SQS Developer Guide\n.\n\nYes. You can delete all messages in an Amazon SQS message queue using the PurgeQueue action.\nWhen you purge a message queue, all the messages previously sent to the message queue are deleted. Because your message queue and its attributes remain, there is no need to reconfigure the message queue; you can continue using it.\nTo delete only specific messages, use the DeleteMessage or DeleteMessageBatch actions.\nFor more information, see this Tutorial:\nPurging Messages from an Amazon SQS Queue\n.\n\n", "question": "How does the AmazonSQSBufferedAsyncClient for Java compare to the existing AmazonSQSAsyncClient?", "reference_answer": "The AmazonSQSBufferedAsyncClient for Java is implemented as a drop-in replacement for the existing AmazonSQSAsyncClient. Updating to the latest AWS SDK and switching to the AmazonSQSBufferedAsyncClient provides additional benefits such as automatic batching and prefetching without requiring changes to the application.", "prompt_inference": "[INST] Section: Amazon SQS FAQs > Features, functionality, and interfaces > What is the AmazonSQSBufferedAsyncClient for Java?\n\nThe AmazonSQSBufferedAsyncClient for Java provides an implementation of the AmazonSQSAsyncClient interface and adds several important features:\nAutomatic batching of multiple SendMessage, DeleteMessage, or ChangeMessageVisibility requests without any required changes to the application\nPrefetching of messages into a local buffer that allows your application to immediately process messages from Amazon SQS without waiting for the messages to be retrieved\nWorking together, automatic batching and prefetching increase the throughput and reduce the latency of your application while reducing your costs by making fewer Amazon SQS requests. For more information, see\nClient-Side Buffering and Request Batching\nin the\nAmazon SQS Developer Guide\n.\n\nYou can download the AmazonSQSBufferedAsyncClient as part of the\nAWS SDK for Java\n.\n\nNo. The AmazonSQSBufferedAsyncClient for Java is implemented as a drop-in replacement for the existing AmazonSQSAsyncClient.\nIf you update your application to use the latest AWS SDK and change your client to use the AmazonSQSBufferedAsyncClient for Java instead of the AmazonSQSAsyncClient, your application will receive the added benefits of automatic batching and prefetching.\n\nIn the Amazon SQS console, select an Amazon SQS standard queue.\nUnder Queue Actions, select Subscribe Queue to SNS Topic from the drop-down list.\nIn the dialog box, select the topic from the Choose a Topic drop-down list, and click Subscribe.\nFor more information, see\nSubscribing a Queue to an Amazon SNS Topic\nin the\nAmazon SQS Developer Guide\n.\n\nYes. You can delete all messages in an Amazon SQS message queue using the PurgeQueue action.\nWhen you purge a message queue, all the messages previously sent to the message queue are deleted. Because your message queue and its attributes remain, there is no need to reconfigure the message queue; you can continue using it.\nTo delete only specific messages, use the DeleteMessage or DeleteMessageBatch actions.\nFor more information, see this Tutorial:\nPurging Messages from an Amazon SQS Queue\n.\n\n\n\nQuestion: How does the AmazonSQSBufferedAsyncClient for Java compare to the existing AmazonSQSAsyncClient? [/INST]", "metadata": {"service": "SQS", "doc_type": "Guide", "question_type": "comparison", "chunk_id": "sqs-faq-7"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Security, Identity and Compliance\n›\nAmazon Inspector\n›\nAmazon Inspector FAQs\n\nGeneral\n9\nGetting started\n8\nWorking with Amazon Inspector\n33\n\nWorking with Amazon Inspector\n\nAmazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container images in Amazon ECR and within continuous integration and continuous delivery (CI/CD) tools, in near-real time for software vulnerabilities and unintended network exposure.\n\nAmazon Inspector removes the operational overhead associated with deploying and configuring a vulnerability management solution by allowing you to deploy Amazon Inspector across all accounts with a single step. Additional benefits include:\nAutomated discovery and continual scanning that delivers near real-time vulnerability findings\nCentral management, configuration, and view of findings for all your organization’s accounts by setting a Delegated Administrator (DA) account\nA highly contextualized and meaningful Amazon Inspector risk score for each finding to help you set more accurate response priorities\nAn intuitive Amazon Inspector dashboard for coverage metrics, including accounts, Amazon EC2 instances, Lambda functions, and container images in Amazon ECR and within CI/CD tools, and code repositories within your Source Code Management (SCM) platform, in near-real time.\nMaximize vulnerability assessment coverage by seamlessly scanning EC2 instances, switching between agent-based and agentless scanning.\nCentrally manage software bill of materials (SBOM) exports for all monitored resources.\nIntegration with AWS Security Hub and Amazon EventBridge to automate workflows and ticket routing\n\nYou can deactivate Amazon Inspector Classic by simply deleting all assessment templates in your account. To access findings for existing assessment runs, you can download them as reports or export them using the Amazon Inspector API. You can activate the new Amazon Inspector with a few steps in the AWS Management Console, or by using the new Amazon Inspector APIs. You can find the detailed migration steps in the\nAmazon Inspector Classic User Guide\n.\n\n", "question": "How does Amazon Inspector reduce operational overhead for vulnerability management?", "reference_answer": "Amazon Inspector reduces operational overhead by allowing users to deploy it across all accounts with a single step, thus eliminating the need for individual deployment and configuration of a vulnerability management solution.", "prompt_inference": "[INST] Security, Identity and Compliance\n›\nAmazon Inspector\n›\nAmazon Inspector FAQs\n\nGeneral\n9\nGetting started\n8\nWorking with Amazon Inspector\n33\n\nWorking with Amazon Inspector\n\nAmazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container images in Amazon ECR and within continuous integration and continuous delivery (CI/CD) tools, in near-real time for software vulnerabilities and unintended network exposure.\n\nAmazon Inspector removes the operational overhead associated with deploying and configuring a vulnerability management solution by allowing you to deploy Amazon Inspector across all accounts with a single step. Additional benefits include:\nAutomated discovery and continual scanning that delivers near real-time vulnerability findings\nCentral management, configuration, and view of findings for all your organization’s accounts by setting a Delegated Administrator (DA) account\nA highly contextualized and meaningful Amazon Inspector risk score for each finding to help you set more accurate response priorities\nAn intuitive Amazon Inspector dashboard for coverage metrics, including accounts, Amazon EC2 instances, Lambda functions, and container images in Amazon ECR and within CI/CD tools, and code repositories within your Source Code Management (SCM) platform, in near-real time.\nMaximize vulnerability assessment coverage by seamlessly scanning EC2 instances, switching between agent-based and agentless scanning.\nCentrally manage software bill of materials (SBOM) exports for all monitored resources.\nIntegration with AWS Security Hub and Amazon EventBridge to automate workflows and ticket routing\n\nYou can deactivate Amazon Inspector Classic by simply deleting all assessment templates in your account. To access findings for existing assessment runs, you can download them as reports or export them using the Amazon Inspector API. You can activate the new Amazon Inspector with a few steps in the AWS Management Console, or by using the new Amazon Inspector APIs. You can find the detailed migration steps in the\nAmazon Inspector Classic User Guide\n.\n\n\n\nQuestion: How does Amazon Inspector reduce operational overhead for vulnerability management? [/INST]", "metadata": {"service": "INSPECTOR", "doc_type": "Guide", "question_type": "conceptual", "chunk_id": "inspector-faq-0"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Section: Amazon EC2 FAQs > Platform > Optimize CPUs\n\nQ: What is Optimize CPUs?\nOptimize CPUs gives you greater control of your EC2 instances on two fronts. First, you can specify a custom number of vCPUs when launching new instances to save on vCPU-based licensing costs. Second, you can disable Intel Hyper-Threading Technology (Intel HT Technology) for workloads that perform well with single-threaded CPUs, such as certain HPC applications.\nQ: Why should I use Optimize CPUs feature?\nYou should use Optimize CPUs if:\nYou are running EC2 workloads that are not compute bound and are incurring vCPU-based licensing costs. By launching instances with a custom number of vCPUs you may be able to optimize your licensing spend.\nYou are running workloads that will benefit from disabling hyper-threading on EC2 instances.\nQ: How will the CPU optimized instances be priced?\nCPU optimized instances will be priced the same as equivalent full-sized instances.\nQ: How will my application performance change when using Optimize CPUs on EC2?\nYour application performance change with Optimize CPUs will be largely dependent on the workloads you are running on EC2. We encourage you to benchmark your application performance with Optimize CPUs to arrive at the right number of vCPUs and optimal hyper-threading behavior for your application.\nQ: Can I use Optimize CPUs on EC2 Bare Metal instance types (such as i3.metal)?\nNo. You can use Optimize CPUs with only virtualized EC2 instances.\nQ: How can I get started with using Optimize CPUs for EC2 Instances?\nFor more information on how to get started with Optimize CPUs and supported instance types, please visit the Optimize CPUs documentation page\nhere\n.\n\n", "question": "How does the pricing of CPU optimized instances compare to full-sized instances?", "reference_answer": "CPU optimized instances will be priced the same as equivalent full-sized instances, meaning there is no difference in pricing based on the optimization for CPUs.", "prompt_inference": "[INST] Section: Amazon EC2 FAQs > Platform > Optimize CPUs\n\nQ: What is Optimize CPUs?\nOptimize CPUs gives you greater control of your EC2 instances on two fronts. First, you can specify a custom number of vCPUs when launching new instances to save on vCPU-based licensing costs. Second, you can disable Intel Hyper-Threading Technology (Intel HT Technology) for workloads that perform well with single-threaded CPUs, such as certain HPC applications.\nQ: Why should I use Optimize CPUs feature?\nYou should use Optimize CPUs if:\nYou are running EC2 workloads that are not compute bound and are incurring vCPU-based licensing costs. By launching instances with a custom number of vCPUs you may be able to optimize your licensing spend.\nYou are running workloads that will benefit from disabling hyper-threading on EC2 instances.\nQ: How will the CPU optimized instances be priced?\nCPU optimized instances will be priced the same as equivalent full-sized instances.\nQ: How will my application performance change when using Optimize CPUs on EC2?\nYour application performance change with Optimize CPUs will be largely dependent on the workloads you are running on EC2. We encourage you to benchmark your application performance with Optimize CPUs to arrive at the right number of vCPUs and optimal hyper-threading behavior for your application.\nQ: Can I use Optimize CPUs on EC2 Bare Metal instance types (such as i3.metal)?\nNo. You can use Optimize CPUs with only virtualized EC2 instances.\nQ: How can I get started with using Optimize CPUs for EC2 Instances?\nFor more information on how to get started with Optimize CPUs and supported instance types, please visit the Optimize CPUs documentation page\nhere\n.\n\n\n\nQuestion: How does the pricing of CPU optimized instances compare to full-sized instances? [/INST]", "metadata": {"service": "EC2", "doc_type": "Guide", "question_type": "comparison", "chunk_id": "ec2-faq-175"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Section: Amazon CloudFront FAQs > Security > What is the difference between SNI Custom SSL and Dedicated IP Custom SSL of Amazon CloudFront?\n\nDedicated IP Custom SSL\nallocates dedicated IP addresses to serve your SSL content at each CloudFront edge location. Because there is a one to one mapping between IP addresses and SSL certificates, Dedicated IP Custom SSL works with browsers and other clients that do not support SNI. Due to the current IP address costs, Dedicated IP Custom SSL is $600/month prorated by the hour.\nSNI Custom SSL\nrelies on the SNI extension of the Transport Layer Security protocol, which allows multiple domains to serve SSL traffic over the same IP address by including the hostname viewers are trying to connect to. As with Dedicated IP Custom SSL, CloudFront delivers content from each Amazon CloudFront edge location and with the same security as the Dedicated IP Custom SSL feature. SNI Custom SSL works with most modern browsers, including Chrome version 6 and later (running on Windows XP and later or OS X 10.5.7 and later), Safari version 3 and later (running on Windows Vista and later or Mac OS X 10.5.6. and later), Firefox 2.0 and later, and Internet Explorer 7 and later (running on Windows Vista and later). Older browsers that do not support SNI cannot establish a connection with CloudFront to load the HTTPS version of your content. SNI Custom SSL is available at no additional cost beyond standard CloudFront data transfer and request fees.\n\nServer Name Indication (SNI) is an extension of the Transport Layer Security (TLS) protocol. This mechanism identifies the domain (server name) of the associated SSL request so the proper certificate can be used in the SSL handshake. This allows a single IP address to be used across multiple servers. SNI requires browser support to add the server name, and while most modern browsers support it, there are a few legacy browsers that do not. For more details see the SNI section of the\nCloudFront Developer Guide\nor the\nSNI Wikipedia article\n.\n\n", "question": "What is the monthly cost of Dedicated IP Custom SSL?", "reference_answer": "Dedicated IP Custom SSL costs $600 per month, prorated by the hour.", "prompt_inference": "[INST] Section: Amazon CloudFront FAQs > Security > What is the difference between SNI Custom SSL and Dedicated IP Custom SSL of Amazon CloudFront?\n\nDedicated IP Custom SSL\nallocates dedicated IP addresses to serve your SSL content at each CloudFront edge location. Because there is a one to one mapping between IP addresses and SSL certificates, Dedicated IP Custom SSL works with browsers and other clients that do not support SNI. Due to the current IP address costs, Dedicated IP Custom SSL is $600/month prorated by the hour.\nSNI Custom SSL\nrelies on the SNI extension of the Transport Layer Security protocol, which allows multiple domains to serve SSL traffic over the same IP address by including the hostname viewers are trying to connect to. As with Dedicated IP Custom SSL, CloudFront delivers content from each Amazon CloudFront edge location and with the same security as the Dedicated IP Custom SSL feature. SNI Custom SSL works with most modern browsers, including Chrome version 6 and later (running on Windows XP and later or OS X 10.5.7 and later), Safari version 3 and later (running on Windows Vista and later or Mac OS X 10.5.6. and later), Firefox 2.0 and later, and Internet Explorer 7 and later (running on Windows Vista and later). Older browsers that do not support SNI cannot establish a connection with CloudFront to load the HTTPS version of your content. SNI Custom SSL is available at no additional cost beyond standard CloudFront data transfer and request fees.\n\nServer Name Indication (SNI) is an extension of the Transport Layer Security (TLS) protocol. This mechanism identifies the domain (server name) of the associated SSL request so the proper certificate can be used in the SSL handshake. This allows a single IP address to be used across multiple servers. SNI requires browser support to add the server name, and while most modern browsers support it, there are a few legacy browsers that do not. For more details see the SNI section of the\nCloudFront Developer Guide\nor the\nSNI Wikipedia article\n.\n\n\n\nQuestion: What is the monthly cost of Dedicated IP Custom SSL? [/INST]", "metadata": {"service": "CLOUDFRONT", "doc_type": "Guide", "question_type": "factual", "chunk_id": "cloudfront-faq-17"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Section: Amazon Lex FAQs > Data and Security > Who has access to my content that is processed and stored by Amazon Lex?\n\nOnly authorized employees will have access to your content that is processed by Amazon Lex. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\n\nYou always retain ownership of your content and we will only use your content with your consent.\n\nAny content processed by Amazon Lex is encrypted and stored at rest in the AWS region where you are using Amazon Lex. Some portion of content processed by Amazon Lex may be stored in another AWS region solely in connection with the continuous improvement and development of your Amazon Lex customer experience and other Amazon machine-learning/artificial-intelligence technologies. You can request deletion of voice and text inputs associated with your account by contacting AWS Support. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\n\nYes, subject to your compliance with the\nAmazon Lex Service Terms\n, including your obligation to provide any required notices and obtain any required verifiable parental consent under COPPA, you may use Amazon Lex in connection with websites, programs, or other applications that are directed or targeted, in whole or in part, to children under age 13. Amazon Lex does not store or retain voice or text utterance information from websites, programs, or applications that are identified by customers in accordance with the Amazon Lex Service Terms as being directed or targeted, in whole or in part, to children under age 13 and subject to COPPA.\n\n", "question": "How does Amazon Lex handle ownership and consent regarding user content?", "reference_answer": "Users always retain ownership of their content, and Amazon Lex will only use the content with the user's consent.", "prompt_inference": "[INST] Section: Amazon Lex FAQs > Data and Security > Who has access to my content that is processed and stored by Amazon Lex?\n\nOnly authorized employees will have access to your content that is processed by Amazon Lex. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\n\nYou always retain ownership of your content and we will only use your content with your consent.\n\nAny content processed by Amazon Lex is encrypted and stored at rest in the AWS region where you are using Amazon Lex. Some portion of content processed by Amazon Lex may be stored in another AWS region solely in connection with the continuous improvement and development of your Amazon Lex customer experience and other Amazon machine-learning/artificial-intelligence technologies. You can request deletion of voice and text inputs associated with your account by contacting AWS Support. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\n\nYes, subject to your compliance with the\nAmazon Lex Service Terms\n, including your obligation to provide any required notices and obtain any required verifiable parental consent under COPPA, you may use Amazon Lex in connection with websites, programs, or other applications that are directed or targeted, in whole or in part, to children under age 13. Amazon Lex does not store or retain voice or text utterance information from websites, programs, or applications that are identified by customers in accordance with the Amazon Lex Service Terms as being directed or targeted, in whole or in part, to children under age 13 and subject to COPPA.\n\n\n\nQuestion: How does Amazon Lex handle ownership and consent regarding user content? [/INST]", "metadata": {"service": "LEX", "doc_type": "Guide", "question_type": "conceptual", "chunk_id": "lex-faq-7"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Section: Amazon EBS FAQs > Performance > What level of performance consistency can I expect to see from my HDD-backed volumes?\n\nWhen attached to EBS-optimized instances, Throughput Optimized HDD (st1) and Cold HDD (sc1) volumes are designed to deliver within 10% of the expected throughput performance 99% of the time in a given year. Your exact performance depends on your application’s I/O requirements and the performance of your EC2 instance.\n\nYes. The throughput rate you get depends on the I/O size of your application reads and writes. HDD-backed volumes process reads and writes in I/O sizes of 1MB. Sequential I/Os are merged and processed as 1 MB units while each non-sequential I/O is processed as 1MB even if the actual I/O size is smaller. Thus, while a transactional workload with small, random IOs, such as a database, won't perform well on HDD-backed volumes, sequential I/Os and large I/O sizes will achieve the advertised performance of st1 and sc1 for a longer period of time.\n\n", "question": "How does the I/O size of an application affect the throughput rate for HDD-backed volumes?", "reference_answer": "The throughput rate for HDD-backed volumes depends on the I/O size of the application's reads and writes. These volumes process reads and writes in I/O sizes of 1MB, where sequential I/Os are merged and processed as 1MB units, and each non-sequential I/O is processed as 1MB even if the actual I/O size is smaller. Therefore, transactional workloads with small, random I/Os, such as databases, won't perform well, while sequential I/Os and large I/O sizes can achieve the advertised performance for a longer period.", "prompt_inference": "[INST] Section: Amazon EBS FAQs > Performance > What level of performance consistency can I expect to see from my HDD-backed volumes?\n\nWhen attached to EBS-optimized instances, Throughput Optimized HDD (st1) and Cold HDD (sc1) volumes are designed to deliver within 10% of the expected throughput performance 99% of the time in a given year. Your exact performance depends on your application’s I/O requirements and the performance of your EC2 instance.\n\nYes. The throughput rate you get depends on the I/O size of your application reads and writes. HDD-backed volumes process reads and writes in I/O sizes of 1MB. Sequential I/Os are merged and processed as 1 MB units while each non-sequential I/O is processed as 1MB even if the actual I/O size is smaller. Thus, while a transactional workload with small, random IOs, such as a database, won't perform well on HDD-backed volumes, sequential I/Os and large I/O sizes will achieve the advertised performance of st1 and sc1 for a longer period of time.\n\n\n\nQuestion: How does the I/O size of an application affect the throughput rate for HDD-backed volumes? [/INST]", "metadata": {"service": "EBS", "doc_type": "Guide", "question_type": "conceptual", "chunk_id": "ebs-faq-5"}}
{"system": "You are an expert assistant for AWS (Amazon Web Services) certification preparation.\n\nCRITICAL: You must answer ONLY based on the provided context below. Follow these rules strictly:\n\nRules for answers:\n- Extract and provide ALL relevant information from the context\n- NEVER add information not explicitly stated in the context\n- NEVER use external knowledge or your training data - only use what's in the given context\n- Be as detailed as the context allows - short context = short answer, detailed context = detailed answer\n- Write in complete, helpful sentences as if answering a colleague\n- If comparing items, ONLY compare aspects explicitly mentioned in the context\n- If the context doesn't provide enough information to answer the question, respond with: \"The provided context does not contain sufficient information to answer this question.\"\n- Answers should be in English\n- Do not reference or mention \"the context\" in your answer - answer naturally as if you had this knowledge\n", "context": "Section: FAQs > Resources > How does data replication work in Amazon MSK?\n\nAmazon MSK uses Apache Kafka’s leader-follower replication to replicate data between brokers. Amazon MSK makes it easy to deploy clusters with Multi-AZ replication. With Standard brokers, you have the option to use a custom replication strategy by topic. Express brokers guarantee higher availability by always replicating your data across three Availability Zones. Leader and follower brokers will be deployed and isolated using the broker type and replication strategy specified. For example, if you select Standard brokers with three Availability Zone broker replication strategy and with one broker per Availability Zone cluster, Amazon MSK will create a cluster of three brokers (one broker in three Availability Zones in a Region), and by default (unless you choose to override the topic replication factor), the topic replication factor will also be three. To learn more about what happens during client failover, see our\nclient failover documentation\n.\n\nYes, Amazon MSK allows you to create custom configurations and apply them to new and existing clusters. Express brokers protect more of the configurations from suboptimal values that may affect availability and durability. Express brokers also offer a simpler experience by abstracting configurations that have to do with storage since Amazon MSK fully manages the storage layer. For more information on custom configurations, see the\nconfiguration documentation\n.\n\nOnce your Apache Kafka cluster has been created, you can create topics using the Apache Kafka APIs. All topic and partition-level actions and configurations are performed using Apache Kafka APIs. The following command is an example of creating a topic using Apache Kafka APIs and the configuration details available for your cluster:\n<path-to-your-kafka-installation>/bin/kafka-topics.sh --create — bootstrap-server <BootstrapBrokerString> --replication-factor 3 --partitions 1 -- topic TopicName\n\n", "question": "What is the relationship between input configuration and data streams in an Apache Flink application?", "reference_answer": "In an Apache Flink application, the input configuration maps the streaming sources to data streams. Data flows from the data sources into the data streams, where it is processed using the application code. The processed data can then be sent to subsequent data streams or destinations.", "prompt_inference": "[INST] Section: FAQs > Resources > How does data replication work in Amazon MSK?\n\nAmazon MSK uses Apache Kafka’s leader-follower replication to replicate data between brokers. Amazon MSK makes it easy to deploy clusters with Multi-AZ replication. With Standard brokers, you have the option to use a custom replication strategy by topic. Express brokers guarantee higher availability by always replicating your data across three Availability Zones. Leader and follower brokers will be deployed and isolated using the broker type and replication strategy specified. For example, if you select Standard brokers with three Availability Zone broker replication strategy and with one broker per Availability Zone cluster, Amazon MSK will create a cluster of three brokers (one broker in three Availability Zones in a Region), and by default (unless you choose to override the topic replication factor), the topic replication factor will also be three. To learn more about what happens during client failover, see our\nclient failover documentation\n.\n\nYes, Amazon MSK allows you to create custom configurations and apply them to new and existing clusters. Express brokers protect more of the configurations from suboptimal values that may affect availability and durability. Express brokers also offer a simpler experience by abstracting configurations that have to do with storage since Amazon MSK fully manages the storage layer. For more information on custom configurations, see the\nconfiguration documentation\n.\n\nOnce your Apache Kafka cluster has been created, you can create topics using the Apache Kafka APIs. All topic and partition-level actions and configurations are performed using Apache Kafka APIs. The following command is an example of creating a topic using Apache Kafka APIs and the configuration details available for your cluster:\n<path-to-your-kafka-installation>/bin/kafka-topics.sh --create — bootstrap-server <BootstrapBrokerString> --replication-factor 3 --partitions 1 -- topic TopicName\n\n\n\nQuestion: What is the relationship between input configuration and data streams in an Apache Flink application? [/INST]", "metadata": {"service": "MANAGED", "doc_type": "Guide", "question_type": "comparison", "chunk_id": "managed-faq-4"}}
