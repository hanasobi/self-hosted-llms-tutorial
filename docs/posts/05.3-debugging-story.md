# Blog Post 5.3: Debugging Story ‚Äì Das pad_token Problem
**Lesezeit:** ~10 Minuten | **Level:** Intermediate  
**Serie:** Self-Hosted LLMs f√ºr Datensouver√§nit√§t | **Code:** [GitHub](https://github.com/hanasobi/self-hosted-llms-tutorial.git)

> **Hinweis:** Dieser Post ist ein Bonus-Kapitel zur Blog-Serie. Er dokumentiert eine echte Debugging-Journey und zeigt, dass nicht alles beim ersten Mal glatt l√§uft ‚Äì selbst wenn die Metrics perfekt aussehen.

---

## TL;DR ‚Äì F√ºr eilige Leser

**Das Problem:** Nach erfolgreichem Training (Loss 0.33, Validation Loss 0.33) generierte das Model endlos weiter statt nach der Antwort zu stoppen. Statt einer pr√§zisen Antwort kam: "Answer... Question: ... [/INST] Answer... Question: ..." bis `max_new_tokens` erreicht war.

**Die Symptome:** 
- Training Metrics sahen perfekt aus ‚úì
- Loss niedrig, konvergiert, kein Overfitting ‚úì
- Aber: 13 von 15 Test-Samples stoppten nicht ‚úó

**Die Root Cause:** `pad_token = eos_token` f√ºhrte dazu, dass der DataCollator **alle EOS Tokens maskierte**. Das Model sah EOS in den Daten, aber lernte es nie ‚Äì weil alle EOS-Token-Labels auf -100 gesetzt wurden (= ignore in Loss-Berechnung).

**Die L√∂sung:** 
1. EOS Token explizit zu jedem Sample hinzuf√ºgen
2. `pad_token = unk_token` statt `eos_token` in 4 Files √§ndern

**Das Ergebnis:** 20/20 Samples stoppen korrekt. Problem vollst√§ndig gel√∂st.

**Key Learning:** Low Loss ‚â† Good Model. Qualitative Inspection ist Pflicht, nicht optional.

---

## Inhaltsverzeichnis

- [Die Story: 16:30 Uhr, Training abgeschlossen](#die-story-1630-uhr-training-abgeschlossen)
- [Tag 1: Hypothese "Chunk-Gruppierung"](#tag-1-hypothese-chunk-gruppierung)
- [Tag 2: Hypothese "Fehlende EOS Tokens"](#tag-2-hypothese-fehlende-eos-tokens)
- [Tag 2 PM: Der Durchbruch ‚Äì Community Research](#tag-2-pm-der-durchbruch--community-research)
- [Die Root Cause: DataCollator maskiert EOS](#die-root-cause-datacollator-maskiert-eos)
- [Der Fix: Vier Zeilen Code](#der-fix-vier-zeilen-code)
- [Validation: 20/20 Success Rate](#validation-2020-success-rate)
- [Lessons Learned](#lessons-learned)
- [Fazit](#fazit)

---

## Die Story: 16:30 Uhr, Training abgeschlossen

**Setting:**  
Datum: 25. Januar 2026, 16:30 Uhr  
Status: LoRA Training finished  
Loss: 0.33 (Training), 0.33 (Validation)  
Gef√ºhl: Zufrieden üòä

Das Training ist durch. Die Metrics sehen gut aus ‚Äì Loss ist niedrig, Validation Loss ebenfalls, kein Overfitting erkennbar. Alles l√§uft nach Plan. Zeit, das Model zu testen.

**Der Moment der Wahrheit:**

```bash
python inspect_model_response.py --num_samples 15
```

Das Script l√§dt das fine-tuned Model, generiert Antworten auf 15 zuf√§llige Testfragen, und zeigt die Outputs. Ich erwarte pr√§zise, kompakte Antworten im AWS-Dokumentationsstil.

**Das Ergebnis:**

```
Sample 1:
Question: What is Amazon EC2?
Generated Answer: Amazon EC2 is a web service that provides 
resizable compute capacity in the cloud. You can launch virtual 
servers, configure security and networking, and manage storage. 
Question: What is Amazon S3? [/INST] Answer: Amazon S3 is an 
object storage service... Question: What is AWS Lambda? [/INST] 
Answer: AWS Lambda lets you run code without provisioning servers...

[continues until max_new_tokens=128 reached]
```

**Reaktion:** üò± 

Das Model antwortet korrekt auf die erste Frage ‚Äì aber es stoppt nicht. Stattdessen generiert es weitere Fragen im exakt gleichen Format wie die Training-Daten. Es verh√§lt sich, als w√ºrde es den Trainingsdatensatz fortsetzen.

**Aber warum?** Die Loss ist doch niedrig. Training hat konvergiert. Was l√§uft hier schief?

---

## Tag 1: Hypothese "Chunk-Gruppierung"

**Die erste Hypothese:**

Das Dataset wurde aus AWS-Dokumentation generiert ‚Äì jedes Dokument wurde in Chunks aufgeteilt, aus jedem Chunk wurden QA-Paare extrahiert (siehe Post 4). Vielleicht sind die Training-Daten nach Chunks gruppiert?

**Die Logik:**
- Wenn das Model sieht: "Context X ‚Üí QA1, Context X ‚Üí QA2, Context X ‚Üí QA3"
- Dann lernt es m√∂glicherweise: "Nach Antwort aus Context X kommt n√§chste Frage aus Context X"
- ‚Üí Model generiert im Training-Format weiter

Das w√ºrde erkl√§ren, warum es nach der ersten Antwort nicht stoppt, sondern weitere Fragen generiert.

**Der Test:**

Ich schreibe ein Diagnose-Script, das pr√ºft, ob aufeinanderfolgende Samples aus dem gleichen Chunk stammen.

```python
# diagnose_chunk_order.py (vereinfacht)
# Vollst√§ndige Version: siehe GitHub Repository

import json

def analyze_chunk_order(filepath):
    with open(filepath) as f:
        samples = [json.loads(line) for line in f]
    
    consecutive_same_chunk = 0
    for i in range(len(samples) - 1):
        if samples[i]['chunk_id'] == samples[i+1]['chunk_id']:
            consecutive_same_chunk += 1
    
    total_samples = len(samples)
    percentage = (consecutive_same_chunk / total_samples) * 100
    
    print(f"Total samples: {total_samples}")
    print(f"Consecutive same-chunk pairs: {consecutive_same_chunk} ({percentage:.1f}%)")
    print(f"Conclusion: {'Well-shuffled!' if percentage < 5 else 'Grouped by chunk!'}")

analyze_chunk_order("data/train.jsonl")
```

**Das Ergebnis:**

```
Total samples: 2100
Consecutive same-chunk pairs: 2 (0.1%)
Conclusion: Well-shuffled!
```

**Reaktion:** Hypothese widerlegt! 

Die Daten sind bereits perfekt gemischt. Chunk-Gruppierung war nicht das Problem. Zur√ºck zum Drawing Board.

---

## Tag 2: Hypothese "Fehlende EOS Tokens"

**Die zweite Hypothese:**

Wenn das Model nie sieht "hier endet die Antwort", wie soll es lernen, wann es stoppen soll?

Vielleicht fehlen die EOS (End-of-Sequence) Tokens in den Training-Daten? Der Mistral Tokenizer f√ºgt automatisch BOS (Beginning-of-Sequence) hinzu ‚Äì aber macht er das auch mit EOS?

**Der Test:**

Ich teste das Tokenizer-Verhalten direkt:

```python
# check_eos_token.py (Original-Code aus Repository)

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

# Test: Was passiert beim Encoding?
text = "This is a test."
encoded = tokenizer.encode(text)

print(f"Encoded: {encoded}")
print(f"Has BOS ({tokenizer.bos_token_id})? {tokenizer.bos_token_id in encoded}")
print(f"Has EOS ({tokenizer.eos_token_id})? {tokenizer.eos_token_id in encoded}")

# Test: Was kommt beim Decoding raus?
decoded = tokenizer.decode(encoded)
print(f"Decoded: {decoded}")
```

**Output:**

```
Encoded: [1, 851, 349, 264, 1369, 28723]
Has BOS (1)? True
Has EOS (2)? False  <-- PROBLEM!
Decoded: <s> This is a test.
```

**Erkenntnis:** 

- ‚úÖ BOS Token (ID 1) wird automatisch hinzugef√ºgt
- ‚ùå EOS Token (ID 2) wird NICHT automatisch hinzugef√ºgt

Das ist inkonsistent und nicht intuitiv ‚Äì aber es erkl√§rt das Problem. Wenn die Training-Daten keinen EOS Token enthalten, kann das Model nicht lernen, wann es stoppen soll.

**Fix 1 implementiert:**

Ich √§ndere die Dataset-Creation in `utils.py`, um EOS explizit hinzuzuf√ºgen:

```python
# utils.py (vereinfacht)
# Vollst√§ndige Version: siehe GitHub Repository

def create_dataset(file_path: str, tokenizer: AutoTokenizer, max_length: int = 512):
    with open(file_path) as f:
        raw_data = [json.loads(line) for line in f]
    
    # Extract pre-formatted prompts
    formatted_texts = [record['prompt_training'] for record in raw_data]
    
    # CRITICAL FIX: Add EOS token to each sample
    # Mistral tokenizer does NOT add EOS automatically!
    formatted_texts_with_eos = [
        text + tokenizer.eos_token for text in formatted_texts
    ]
    
    # Tokenize
    tokenized = tokenizer(
        formatted_texts_with_eos,  # <-- Mit EOS!
        truncation=True,
        max_length=max_length,
        padding=False,
        return_tensors=None
    )
    
    tokenized['labels'] = tokenized['input_ids'].copy()
    return Dataset.from_dict(tokenized)
```

**Status:** Fix implementiert, Re-Training gestartet...

Aber w√§hrend das Training l√§uft, mache ich noch etwas Community Research.

---

## Tag 2 PM: Der Durchbruch ‚Äì Community Research

W√§hrend des Re-Trainings google ich nach "model doesn't stop generating" und "llm fine-tuning endless generation". 

**Der Fund ‚Äì HuggingFace Discussions:**

In einem Thread finde ich diesen Kommentar:

> "Check if `pad_token == eos_token`. This is a common anti-pattern! The DataCollator will mask all EOS tokens because it masks everything with `pad_token_id`."

**üí° Das ist es!**

**Die neue Hypothese:**

Ich √ºberpr√ºfe meinen Code ‚Äì und tats√§chlich, in allen 4 relevanten Files steht:

```python
# train_lora.py, utils.py, evaluate_intrinsic.py, inspect_model_response.py
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token  # ‚ùå DAS PROBLEM!
    tokenizer.pad_token_id = tokenizer.eos_token_id
```

**Die Logik des Bugs:**

1. `pad_token = eos_token` ‚Üí `pad_token_id = 2`
2. `eos_token_id = 2` (per Definition)
3. DataCollator maskiert alle Tokens mit `pad_token_id`
4. ‚Üí Alle Tokens mit ID 2 werden zu -100 in Labels
5. ‚Üí Das schlie√üt ALLE EOS Tokens ein!
6. ‚Üí Model sieht EOS in den Input-Daten, aber lernt es nicht (weil Labels = -100)

Das erkl√§rt perfekt, warum:
- Loss niedrig ist (Model lernt die Content-Tokens korrekt)
- Training konvergiert (kein technisches Problem)
- Aber Model nicht stoppt (es hat nie gelernt, EOS zu generieren)

---

## Die Root Cause: DataCollator maskiert EOS

**Wie DataCollator funktioniert:**

Der `DataCollatorForLanguageModeling` von HuggingFace paddet Sequences auf gleiche L√§nge und maskiert die Padding-Tokens in den Labels (setzt sie auf -100), damit sie nicht in der Loss-Berechnung ber√ºcksichtigt werden.

```python
# Vereinfachte Darstellung des DataCollator-Verhaltens
# In der HuggingFace Implementierung:

# Step 1: Padding hinzuf√ºgen
batch["input_ids"] = pad_sequence(batch["input_ids"], pad_token_id)

# Step 2: Labels padden und maskieren
batch["labels"] = pad_sequence(batch["labels"], pad_token_id)
batch["labels"][batch["labels"] == pad_token_id] = -100
```

**Das Problem bei `pad_token_id == eos_token_id`:**

```python
# WENN:
pad_token_id = 2  # weil pad_token = eos_token
eos_token_id = 2  # per Definition

# DANN:
labels[labels == 2] = -100  
# ‚Üí Maskiert ALLES mit ID 2
# ‚Üí Inkl. ALLER EOS Tokens!
```

**Visualisierung:**

```
Vorher (mit Bug):
Input IDs:  [1, 234, 567, 890, 2, 2, 2]  # 1=BOS, 2=EOS/PAD
Labels:     [234, 567, 890, -100, -100, -100]  # ALLE 2er maskiert!
            ^^^^^^^^^^^^^^^^ nur diese 3 Tokens werden gelernt

Nachher (mit Fix):
pad_token_id = 0  # unk_token  
Input IDs:  [1, 234, 567, 890, 2, 0, 0]  # 1=BOS, 2=EOS, 0=PAD
Labels:     [234, 567, 890, 2, -100, -100]  # EOS bleibt!
            ^^^^^^^^^^^^^^^^^^^^ diese 4 Tokens werden gelernt
```

**Warum das schwer zu finden war:**

1. **Loss sieht gut aus:** Model lernt Content-Tokens korrekt ‚Üí Loss sinkt normal
2. **Training konvergiert:** Kein technisches Problem, keine Errors
3. **Viele Tutorials haben denselben Bug:** Copy-Paste aus ungepr√ºften Quellen
4. **Tokenizer-Verhalten ist non-intuitiv:** BOS automatisch, EOS nicht

Ohne qualitative Inspection h√§tte ich das Problem nie entdeckt ‚Äì die Metrics alleine h√§tten mich get√§uscht.

---

## Der Fix: Vier Zeilen Code

**Ge√§nderte Files:** 4 (`train_lora.py`, `utils.py`, `evaluate_intrinsic.py`, `inspect_model_response.py`)

**Der komplette Diff:**

```diff
  if tokenizer.pad_token is None:
-     tokenizer.pad_token = tokenizer.eos_token
-     tokenizer.pad_token_id = tokenizer.eos_token_id
+     tokenizer.pad_token = tokenizer.unk_token
+     tokenizer.pad_token_id = tokenizer.unk_token_id
```

**Das war's.** Vier Zeilen Code, in vier Files. Ein Bug, der 2 Tage Debugging gekostet hat.

**Wichtig:** Beide Fixes waren n√∂tig:
- **Fix 1** (EOS explizit hinzuf√ºgen): Model SIEHT den EOS Token
- **Fix 2** (pad_token ‚â† eos_token): Model LERNT den EOS Token

Nur Fix 1 alleine h√§tte nicht gereicht ‚Äì EOS w√§re immer noch maskiert worden.

---

## Validation: 20/20 Success Rate

**Re-Training mit korrektem Setup:**

```bash
python train_lora.py --lora_config standard

# Output:
# Loading model: mistralai/Mistral-7B-v0.1
# LoRA Config: standard_r8_qkvo
# Starting training...
# Epoch 1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 525/525 [02:45:32<00:00]
# Training Loss: 0.33
# Validation Loss: 0.33
# Training completed ‚úì
```

**Test mit 20 Samples:**

```bash
python inspect_model_response.py --num_samples 20

# Output:
Sample 1:  ‚úì Stops correctly with EOS
Sample 2:  ‚úì Stops correctly with EOS
Sample 3:  ‚úì Stops correctly with EOS
...
Sample 20: ‚úì Stops correctly with EOS

Success Rate: 20/20 (100%)
```

**Vorher:** 13/15 Samples korrekt (87%)  
**Nachher:** 20/20 Samples korrekt (100%)

**Problem gel√∂st!** üéâ

---

## Lessons Learned

### Lesson 1: Low Loss ‚â† Good Model

**Die Erkenntnis:** Training Metrics alleine sagen nichts √ºber Output Quality.

Loss 0.33 sah perfekt aus. Validation Loss ebenfalls. Training konvergierte ohne Overfitting. Aber das Model war unbrauchbar ‚Äì es stoppte nicht nach der Antwort.

**Best Practice:** Nach jedem Training qualitative Inspection durchf√ºhren. 10-20 Samples manuell anschauen, bevor man das Model deployed.

```python
# Immer ein inspect_model_response.py Script haben
# Nicht nur Metrics loggen, sondern echte Outputs anschauen
```

### Lesson 2: Systematisches Debugging

**Die Erkenntnis:** Nicht raten, sondern messen. Hypothesen aufstellen, empirisch testen, iterieren.

**Unsere Timeline:**
1. Hypothese 1 (Chunk-Gruppierung) ‚Üí getestet ‚Üí widerlegt
2. Hypothese 2 (Fehlende EOS) ‚Üí getestet ‚Üí teilweise best√§tigt
3. Hypothese 3 (pad_token Masking) ‚Üí getestet ‚Üí best√§tigt

**Best Practice:** Diagnostic Scripts schreiben (wie `diagnose_chunk_order.py`, `check_eos_token.py`). Einmal geschrieben, k√∂nnen sie bei jedem zuk√ºnftigen Projekt helfen.

### Lesson 3: Community Research zahlt sich aus

**Die Erkenntnis:** Andere hatten wahrscheinlich das gleiche Problem.

2 Tage selbst debuggen vs. 10 Minuten HuggingFace Discussions lesen. Der Community-Tipp zu `pad_token == eos_token` war der Durchbruch.

**Best Practice:** Bei obscuren Problemen erst googlen, dann debuggen. HuggingFace Discussions, GitHub Issues und Reddit sind Gold wert.

### Lesson 4: Token-Level Mechaniken verstehen

**Die Erkenntnis:** DataCollator, Tokenizer und Model interagieren auf Token-Ebene. Subtile Konfigurationsfehler haben massive Auswirkungen.

**Best Practice:** Immer testen, nie annehmen. Nur weil ein Tokenizer BOS automatisch hinzuf√ºgt, hei√üt das nicht, dass er auch EOS hinzuf√ºgt.

### Die Anti-Pattern Regel

**Merke dir:**

```python
# NEVER do this:
tokenizer.pad_token = tokenizer.eos_token  # ‚ùå

# ALWAYS do this:
tokenizer.pad_token = tokenizer.unk_token  # ‚úì
# Or add a new special token, but NEVER use eos_token!
```

**F√ºr die Zukunft ‚Äì Pr√§vention durch Assertion:**

```python
def setup_tokenizer(tokenizer):
    """Setup tokenizer with proper pad_token configuration."""
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.unk_token
        tokenizer.pad_token_id = tokenizer.unk_token_id
    
    # ASSERTION: Catch the bug early
    assert tokenizer.pad_token_id != tokenizer.eos_token_id, \
        "pad_token must not be the same as eos_token! " \
        "This causes DataCollator to mask all EOS tokens."
    
    return tokenizer
```

Diese Assertion h√§tte mir 2 Tage Debugging erspart.

---

## Fazit

**Die Journey in K√ºrze:**

1. Problem erkannt ‚Üí Model generiert endlos statt zu stoppen
2. Hypothese 1 (Chunk-Gruppierung) ‚Üí Diagnose-Script ‚Üí Widerlegt (0.1% consecutive)
3. Hypothese 2 (Fehlende EOS) ‚Üí Tokenizer-Test ‚Üí Best√§tigt (kein auto-EOS)
4. Fix 1 implementiert ‚Üí EOS explizit hinzugef√ºgt
5. Community Research ‚Üí pad_token Anti-Pattern entdeckt
6. Root Cause Analysis ‚Üí DataCollator maskiert alle EOS Tokens
7. Fix 2 implementiert ‚Üí pad_token = unk_token in 4 Files
8. Re-Training ‚Üí Validation ‚Üí 100% Success Rate ‚úì

**Das Takeaway:** ML Debugging ist wie Software Debugging, nur schwieriger ‚Äì weil "falsche" Outputs nicht immer Bugs sind. Systematisches Vorgehen ist essentiell: Hypothesen aufstellen, empirisch testen, Community nutzen, Token-Level Mechaniken verstehen. Und vor allem: Qualitative Inspection ist kein Nice-to-have, sondern Pflicht. Low Loss bedeutet nicht automatisch gutes Model.

**Im n√§chsten Post:** Nachdem wir jetzt ein funktionierendes, fine-tuned Model haben, deployen wir es mit vLLM auf Kubernetes. Von Training zur Production ‚Äì mit LoRA-Adapters, Monitoring und Load Testing.

{% include blog_nav.html current="05.3-debugging-story" %}