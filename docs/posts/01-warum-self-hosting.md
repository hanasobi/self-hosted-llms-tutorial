# Blog Post 1: Warum Self-Hosting? Der Business Case f√ºr Datensouver√§nit√§t

**Lesezeit:** ~10 Minuten | **Level:** Einsteiger  
**Serie:** Self-Hosted LLMs f√ºr Datensouver√§nit√§t | **Code:** [GitHub](https://github.com/hanasobi/self-hosted-llms-tutorial)

---

## TL;DR ‚Äì F√ºr eilige Leser

**Das Problem:** Unternehmen wollen generative KI nutzen, aber sensible Daten d√ºrfen nicht an externe APIs.

**Die L√∂sung:** Self-Hosted LLMs bieten volle Kontrolle √ºber Daten und Modellverhalten.

**Diese Tutorial-Serie zeigt:**
- ‚úÖ Von "erstes LLM deployen" bis "vollst√§ndige Datensouver√§nit√§t"
- ‚úÖ Echte Debugging-Stories statt "happy path"
- ‚úÖ Production-Grade Stack: Kubernetes, vLLM, MLflow, Prometheus
- ‚úÖ Schrittweiser Aufbau ‚Äì nach Post 2 l√§uft dein erstes selbst gehostetes LLM

**F√ºr wen?** ML Engineers, Data Scientists, Tech Leads und technische Entscheider im DACH-Raum.

---

## Inhaltsverzeichnis

- [Das Problem: KI nutzen, Daten behalten](#das-problem-ki-nutzen-daten-behalten)
- [Die L√∂sung: Self-Hosted LLMs](#die-l√∂sung-self-hosted-llms)
- [Wann Self-Hosting? Die Entscheidungsmatrix](#wann-self-hosting-die-entscheidungsmatrix)
- [Infrastruktur-Voraussetzungen](#infrastruktur-voraussetzungen)
- [Was diese Tutorial-Serie bietet](#was-diese-tutorial-serie-bietet)
- [Fazit](#fazit)

---

## Das Problem: KI nutzen, Daten behalten

Generative KI ist in aller Munde. ChatGPT, Claude, Gemini ‚Äì die M√∂glichkeiten scheinen grenzenlos. Doch wenn es um den Unternehmenseinsatz geht, stellt sich schnell eine entscheidende Frage: **Wohin gehen unsere Daten?**

Die Realit√§t in vielen deutschen Unternehmen sieht so aus: Es gibt einen Use Case, der perfekt f√ºr ein LLM w√§re ‚Äì Kundenanfragen automatisiert beantworten, interne Dokumentation durchsuchbar machen, Berichte zusammenfassen. Die Fachabteilung ist begeistert. Dann kommen IT-Security und Datenschutz ins Spiel.

**Typische H√ºrden:**

Der erste Einwand betrifft den Datenschutz. Kundendaten, Vertragsinformationen, interne Strategiepapiere ‚Äì all das darf nicht an externe Server gesendet werden. Die DSGVO schreibt vor, dass personenbezogene Daten nur mit entsprechender Rechtsgrundlage verarbeitet werden d√ºrfen. Bei US-amerikanischen Cloud-Diensten kommt zus√§tzlich die Problematik des transatlantischen Datentransfers hinzu.

Dann gibt es branchenspezifische Regulierung. Finanzdienstleister unterliegen BaFin-Anforderungen zur Auslagerung von IT-Dienstleistungen. Gesundheitsunternehmen m√ºssen besondere Anforderungen an den Schutz von Patientendaten erf√ºllen. Beh√∂rden und √∂ffentliche Einrichtungen haben eigene Vorgaben zur Datenverarbeitung.

Doch es muss nicht immer Regulierung sein. Viele Unternehmen wollen schlicht ihre Betriebsgeheimnisse sch√ºtzen ‚Äì propriet√§re Prozesse, interne Analysen, Strategiedokumente, die einen Wettbewerbsvorteil darstellen. Diese Informationen an externe APIs zu senden, f√ºhlt sich falsch an, selbst wenn es rechtlich zul√§ssig w√§re. Und f√ºr Unternehmen aus der Kreativ- und Medienbranche kommt ein weiterer Aspekt hinzu: Sie wollen nicht, dass ihre Inhalte ungefragt oder unentgeltlich zum Training fremder Modelle verwendet werden. Die Nutzungsbedingungen vieler LLM-Anbieter erlauben genau das ‚Äì oder sind zumindest nicht eindeutig genug, um das Gegenteil zu garantieren.

Schlie√ülich sorgt sich das Management um Vendor Lock-in und Kontrollverlust. Was passiert, wenn OpenAI die Preise erh√∂ht? Was, wenn der Anbieter das Modellverhalten √§ndert? Wie erkl√§rt man Kunden, dass ihre Daten bei einem US-Unternehmen verarbeitet werden?

Das Ergebnis: Viele vielversprechende KI-Projekte versanden, bevor sie richtig starten.

---

## Die L√∂sung: Self-Hosted LLMs

Self-Hosted Large Language Models l√∂sen dieses Dilemma. Du betreibst das Modell auf eigener Infrastruktur ‚Äì in deinem Rechenzentrum, in einer europ√§ischen Cloud-Region, oder auf On-Premise-Hardware. Die Daten verlassen nie deine Kontrolle.

**Was bedeutet das konkret?**

Volle Datensouver√§nit√§t ist der erste Vorteil. Jede Anfrage, jede Antwort, jedes Training bleibt auf deiner Infrastruktur. Keine Daten flie√üen zu externen APIs. Du bestimmst, wer Zugriff hat und wie lange Daten gespeichert werden.

Compliance wird einfacher. Wenn die Datenverarbeitung vollst√§ndig unter deiner Kontrolle stattfindet, vereinfacht das die Dokumentation f√ºr Audits und Zertifizierungen erheblich. Du kannst nachweisen, wo deine Daten sind ‚Äì weil sie nirgendwo anders hingehen.

Anpassbarkeit ist ein weiterer Vorteil. Durch Fine-tuning kannst du das Modell auf deinen spezifischen Use Case zuschneiden. Das Ergebnis ist oft besser als ein generisches Modell mit noch so ausgefeiltem Prompt, weil das Modell deine Dom√§ne, dein Format, deine Anforderungen "verinnerlicht" hat. Wie Fine-tuning funktioniert und wann es sinnvoll ist, behandeln wir ausf√ºhrlich in Post 3.

Langfristige Planbarkeit kommt hinzu. Die Kosten f√ºr Inference sind vorhersagbar ‚Äì keine √úberraschungen bei der n√§chsten Rechnung. Das Modellverhalten √§ndert sich nicht √ºber Nacht, weil der Anbieter ein Update ausrollt.

---

## Wann Self-Hosting? Die Entscheidungsmatrix

Self-Hosting ist nicht f√ºr jeden Use Case die richtige Wahl. Die folgende Matrix hilft bei der Orientierung.

| Kriterium | Self-Hosting | Cloud-API |
|---|---|---|
| Sensible Daten im Prompt | ‚úÖ Pflicht | ‚ùå Risiko |
| Regulatorische Anforderungen (BaFin, DSGVO) | ‚úÖ Pflicht | ‚ùå Oft nicht m√∂glich |
| Hohes Anfragevolumen (Mio. Tokens/Tag) | ‚úÖ G√ºnstiger | üí∞ Teuer |
| Latenz-kritisch | ‚úÖ Volle Kontrolle | ‚ö†Ô∏è Abh√§ngig vom Anbieter |
| Exploration / Prototyping | ‚ö†Ô∏è Aufw√§ndig | ‚úÖ Schneller Start |
| Geringe Nutzung | üí∞ Overhead | ‚úÖ Pay-per-Use |
| Keine sensiblen Daten | ‚ö†Ô∏è Unn√∂tiger Aufwand | ‚úÖ Einfacher |
| Kein ML-Ops Know-how | ‚ùå Hohe Einstiegsh√ºrde | ‚úÖ Managed |


**Starke Indikatoren f√ºr Self-Hosting:**

Regulatorische Anforderungen machen Self-Hosting oft zur einzigen Option. Wenn Branchenvorschriften explizit verbieten, dass bestimmte Daten an externe Dienste √ºbermittelt werden, bleibt keine Alternative. Das betrifft h√§ufig den Finanzsektor, das Gesundheitswesen und den √∂ffentlichen Dienst.

Sensible Daten im Prompt sind ein weiterer klarer Indikator. Wenn dein Use Case erfordert, dass Kundendaten, Vertragsinformationen oder Gesch√§ftsgeheimnisse an das LLM gesendet werden, ist Self-Hosting der sichere Weg. Bei Cloud-APIs verlierst du die Kontrolle dar√ºber, was mit diesen Daten passiert.

Hohes Anfragevolumen kann Self-Hosting wirtschaftlich attraktiv machen. Ab einer gewissen Schwelle ‚Äì typischerweise mehrere Millionen Tokens pro Tag ‚Äì werden die API-Kosten so hoch, dass eigene Infrastruktur g√ºnstiger ist. Die genaue Schwelle h√§ngt vom Modell und deinen Anforderungen ab.

Latenz-kritische Anwendungen profitieren davon, dass der Inference-Server im eigenen Netzwerk steht. Keine Internet-Roundtrips, keine geteilte Kapazit√§t mit anderen Kunden, volle Kontrolle √ºber die Hardware.

**Situationen, in denen Cloud-APIs sinnvoller sein k√∂nnen:**

Exploration und Prototyping gehen mit Cloud-APIs schneller. Wenn du noch nicht wei√üt, ob LLMs f√ºr deinen Use Case √ºberhaupt funktionieren, ist es effizienter, erst mit einer API zu experimentieren, bevor du Infrastruktur aufbaust.

Geringe Nutzung macht Self-Hosting unwirtschaftlich. Wenn du nur wenige hundert Anfragen pro Tag hast, lohnt sich der Aufwand f√ºr eigene Infrastruktur nicht. Die Fixkosten f√ºr GPU-Hardware √ºbersteigen die API-Kosten bei weitem.

Keine sensiblen Daten im Spiel vereinfachen die Entscheidung. Wenn dein Use Case nur mit √∂ffentlich verf√ºgbaren Informationen arbeitet ‚Äì etwa Zusammenfassungen von Nachrichtenartikeln oder allgemeine Wissensfragen ‚Äì spricht weniger gegen Cloud-APIs.

Fehlende Expertise oder Infrastruktur sind ebenfalls Faktoren. Self-Hosting erfordert Know-how in ML-Ops, Kubernetes und GPU-Management. Wenn dieses Wissen im Team fehlt und auch nicht aufgebaut werden soll, ist eine Cloud-API die pragmatischere Wahl.

**Die Grauzone:**

Viele Szenarien fallen in die Mitte. Hier hilft ein strukturierter Ansatz: Beginne mit einer Cloud-API f√ºr den Proof of Concept. Wenn der Use Case sich bew√§hrt und einer der starken Indikatoren f√ºr Self-Hosting zutrifft, plane die Migration. Diese Tutorial-Serie gibt dir das Werkzeug daf√ºr.

---

## Infrastruktur-Voraussetzungen

Self-Hosted LLMs stellen Anforderungen an die Infrastruktur. Hier ein √úberblick √ºber das Minimum f√ºr den Einstieg:

**Hardware:** Mindestens eine GPU mit 16 GB VRAM (z.B. NVIDIA T4) f√ºr kleinere Modelle wie Mistral-7B mit Quantisierung. F√ºr gr√∂√üere Modelle oder h√∂heren Durchsatz entsprechend mehr. Die Details zu GPU-Sizing behandeln wir in Post 2.

**Container-Orchestrierung:** Kubernetes ist der De-facto-Standard f√ºr Production-Deployments. Alternativen wie Docker Compose funktionieren f√ºr Experimente, skalieren aber nicht.

**Netzwerk:** Interne Erreichbarkeit des Inference-Servers. Je nach Use Case Load Balancer und Ingress.

**Storage:** Persistent Volumes f√ºr Model Weights (mehrere GB pro Modell) und optional f√ºr Logs und Metriken.

**Monitoring:** Prometheus und Grafana f√ºr Metriken. Nicht zwingend f√ºr den Einstieg, aber essentiell f√ºr Production.

Die genaue Konfiguration h√§ngt von deinem Use Case ab. In den folgenden Posts zeigen wir konkrete Setups mit allen Details.

---

## Was diese Tutorial-Serie bietet

Viele LLM-Tutorials enden dort, wo die echten Herausforderungen beginnen. Diese Serie geht bewusst weiter und zeigt den vollst√§ndigen Weg von der ersten Installation bis zur vollst√§ndigen Datensouver√§nit√§t.

**Unser Ansatz:**

Schneller erster Erfolg. Nach Post 2 hast du ein funktionierendes LLM auf deiner eigenen Infrastruktur. Das motiviert und gibt dir eine Basis zum Experimentieren.

Schrittweiser Aufbau. Die Serie folgt einem klaren Lernpfad: Erst verstehen, wie Self-Hosting funktioniert. Dann lernen, wie man das Modell anpasst. Schlie√ülich alle externen Abh√§ngigkeiten eliminieren.

Echte Debugging-Stories. Wir dokumentieren unsere Fehler und Sackgassen. Zum Beispiel haben wir 20 Stunden damit verbracht, ein EOS-Token-Problem zu debuggen. Das zeigen wir ‚Äì nicht weil wir stolz darauf sind, sondern weil du daraus lernst.

Design Trade-offs. Jede Entscheidung hat Konsequenzen. Context Window ‚Üí Memory ‚Üí Hardware ‚Üí Kosten. Wir erkl√§ren, warum wir uns wof√ºr entschieden haben und welche Alternativen wir verworfen haben.

Der Weg zur vollst√§ndigen Datensouver√§nit√§t. Zu Beginn nutzen wir OpenAI f√ºr die Dataset-Generierung ‚Äì das ist pragmatisch und beschleunigt den Einstieg. Aber wir zeigen auch den Weg zur vollst√§ndigen Unabh√§ngigkeit: Self-hosted Dataset-Generierung, Training, Inference und Evaluation ohne externe APIs.

Production-Grade Infrastructure. Kubernetes, vLLM, MLflow, Prometheus, Grafana ‚Äì das sind die Tools, die in Produktion funktionieren und in vielen Unternehmen bereits im Einsatz sind.

---

## Fazit

Self-Hosted LLMs sind kein Allheilmittel. Sie erfordern Investition in Infrastruktur, Expertise und Zeit. Aber f√ºr viele Unternehmen sind sie der einzige gangbare Weg, generative KI mit sensiblen Daten zu nutzen.

Diese Tutorial-Serie gibt dir das Werkzeug, um fundierte Entscheidungen zu treffen. Nicht durch Theorie und Benchmarks, sondern durch praktische Erfahrung. Bau einen PoC auf deiner Infrastruktur, mit deinen Daten, f√ºr deinen Use Case. Dann wei√üt du, was funktioniert und was nicht.

**Im n√§chsten Post** deployen wir dein erstes selbst gehostetes LLM. Mistral-7B auf Kubernetes mit vLLM ‚Äì Schritt f√ºr Schritt, bis du ein funktionierendes System hast, das du √ºber eine OpenAI-kompatible API ansprechen kannst.


{% include blog_nav.html current="01-warum-self-hosting" %}