# Blog Post 3: Warum Fine-tuning? Wenn RAG und Prompting nicht reichen

**Lesezeit:** ~12 Minuten | **Level:** Einsteiger-Intermediate  
**Serie:** Self-Hosted LLMs fÃ¼r DatensouverÃ¤nitÃ¤t | **Code:** [GitHub](https://github.com/hanasobi/self-hosted-llms-tutorial.git)

---

## TL;DR â€“ FÃ¼r eilige Leser

**Die Ausgangslage:** Dein LLM lÃ¤uft ([Post 2](./02-vllm-kubernetes-basics.md)), aber das Verhalten ist inkonsistent. Prompting allein reicht nicht.

**Die drei AnsÃ¤tze:**
- **Prompting:** Flexibel, aber variable Ergebnisse
- **RAG:** FÃ¼gt Wissen hinzu, aber keine Verhaltensgarantien
- **Fine-tuning:** Brennt konsistentes Verhalten ein

**Wann Fine-tuning?**
- Spezifische Output-Formate (JSON, strukturierte Daten)
- 100% Regeleinhaltung erforderlich
- Latenz-Optimierung durch kÃ¼rzere Prompts
- Kleinere, spezialisierte Modelle statt groÃŸe Generalisten

**Unser Ansatz:** Instruction Fine-tuning mit QLoRA â€“ ressourcenschonend und kombinierbar mit RAG.

---

## Inhaltsverzeichnis

- [Das Problem: Dein LLM lÃ¤uft, aber...](#das-problem-dein-llm-lÃ¤uft-aber)
- [Die drei AnsÃ¤tze im Vergleich](#die-drei-ansÃ¤tze-im-vergleich)
- [Wann lohnt sich Fine-tuning?](#wann-lohnt-sich-fine-tuning)
- [Wann reicht Prompting oder RAG?](#wann-reicht-prompting-oder-rag)
- [Unser Use Case: AWS Documentation Q&A](#unser-use-case-aws-documentation-qa)
- [Der Fine-tuning-Ansatz: LoRA und Instruction Tuning](#der-fine-tuning-ansatz-lora-und-instruction-tuning)
- [Fazit](#fazit)

---

## Das Problem: Dein LLM lÃ¤uft, aber...

In Post 2 hast du dein erstes selbst gehostetes LLM deployed â€“ Mistral-7B auf Kubernetes mit vLLM. Du kannst es Ã¼ber eine OpenAI-kompatible API ansprechen. Die Infrastruktur steht. Und jetzt?

Die ersten Tests sind vielversprechend. Das Modell antwortet, versteht Fragen, generiert Text. Aber je mehr du es fÃ¼r deinen spezifischen Use Case einsetzt, desto mehr Probleme tauchen auf.

**Inkonsistentes Verhalten trotz sorgfÃ¤ltigem Prompting.** Du hast einen detaillierten System-Prompt geschrieben. Das Modell soll Fragen zu deiner Dokumentation beantworten und dabei nur Informationen aus dem bereitgestellten Kontext verwenden. Meistens funktioniert das. Aber manchmal halluziniert das Modell trotzdem â€“ erfindet Fakten, die nicht im Kontext stehen. Oder es ignoriert deine Formatvorgaben. Der gleiche Prompt, die gleiche Frage, unterschiedliche Ergebnisse.

**RAG hilft, aber lÃ¶st nicht alles.** Du hast Retrieval-Augmented Generation implementiert. Relevante Dokumente werden dem Prompt hinzugefÃ¼gt. Das Wissen ist jetzt da. Aber das Modell nutzt es nicht immer korrekt. Es vermischt Kontext-Informationen mit seinem Vorwissen. Es hÃ¤lt sich nicht an deine Anweisung, nur aus dem Kontext zu antworten.

**Spezifische Anforderungen werden nicht zuverlÃ¤ssig erfÃ¼llt.** Dein Use Case erfordert ein bestimmtes Antwortformat â€“ etwa JSON mit definierten Feldern. Oder Antworten in einem bestimmten Stil. Oder die strikte Einhaltung von Regeln. Das Modell schafft das in 70% der FÃ¤lle. Aber 70% reicht nicht fÃ¼r Produktion.

Das sind keine Bugs. Das ist das erwartete Verhalten eines generischen Modells, das nicht fÃ¼r deinen spezifischen Use Case trainiert wurde.

---

## Die drei AnsÃ¤tze im Vergleich

Es gibt drei grundlegende AnsÃ¤tze, um LLM-Verhalten zu steuern. Jeder hat seine StÃ¤rken und Grenzen.

**Prompting** ist der einfachste Ansatz. Du nutzt ein vortrainiertes Modell und steuerst sein Verhalten durch sorgfÃ¤ltig formulierte Anweisungen. Das funktioniert Ã¼berraschend gut fÃ¼r viele Aufgaben â€“ besonders wenn die Anforderungen nicht zu spezifisch sind und das Modell genug Allgemeinwissen mitbringt. Der Nachteil: Die Ergebnisse sind variabel. Das gleiche Prompt kann zu unterschiedlichen Outputs fÃ¼hren. Und je komplexer deine Anforderungen, desto lÃ¤nger und fragiler wird der Prompt.

**Retrieval-Augmented Generation (RAG)** erweitert diesen Ansatz. Statt alles im Prompt zu erklÃ¤ren, fÃ¼gst du relevante Dokumente als Kontext hinzu. Das Modell kann so auf Wissen zugreifen, das nicht im Training enthalten war â€“ deine interne Dokumentation, aktuelle Informationen, domÃ¤nenspezifisches Wissen. RAG lÃ¶st das Wissensproblem, aber nicht das Verhaltensproblem. Das Modell weiÃŸ jetzt mehr, aber ob es dieses Wissen korrekt nutzt, ist eine andere Frage.

**Fine-tuning** geht einen Schritt weiter. Du trainierst das Modell auf Beispielen, die zeigen, wie es sich verhalten soll. Das Modell lernt nicht nur Wissen, sondern Patterns â€“ wie es antworten soll, in welchem Format, nach welchen Regeln. Diese Patterns werden "eingebrannt" und sind dadurch konsistenter als Prompt-Instruktionen.

**Vergleichstabelle:**

| Aspekt | Prompting | RAG | Fine-tuning |
|--------|-----------|-----|-------------|
| **Aufwand** | Gering | Mittel | Hoch |
| **Neues Wissen hinzufÃ¼gen** | Begrenzt (Context Window) | âœ… Stark | âŒ AufwÃ¤ndig |
| **Konsistentes Verhalten** | âŒ Variabel | âŒ Variabel | âœ… Stark |
| **Spezifische Formate** | MÃ¶glich, aber fragil | MÃ¶glich, aber fragil | âœ… Robust |
| **Latenz** | Basis | HÃ¶her (Retrieval + lÃ¤ngerer Prompt) | Niedriger (kÃ¼rzere Prompts mÃ¶glich) |
| **Aktualisierung** | Sofort | Schnell (Index update) | Langsam (Retraining) |

Die wichtigste Erkenntnis: Diese AnsÃ¤tze schlieÃŸen sich nicht gegenseitig aus. RAG und Fine-tuning lassen sich hervorragend kombinieren â€“ RAG liefert das aktuelle DomÃ¤nenwissen, Fine-tuning sorgt fÃ¼r konsistentes Antwortverhalten. Genau diese Kombination zeigen wir in dieser Tutorial-Serie.

---

## Wann lohnt sich Fine-tuning?

Fine-tuning ist Aufwand. Du brauchst Trainingsdaten, Compute-Ressourcen, Expertise. Das lohnt sich nicht fÃ¼r jeden Use Case. Aber es gibt klare Indikatoren, wann Fine-tuning der richtige Weg ist.

**Konsistentes Output-Format** ist ein starkes Argument. Wenn dein Modell zuverlÃ¤ssig JSON in einem bestimmten Schema ausgeben soll, oder immer einem bestimmten Antwortmuster folgen soll, ist Fine-tuning oft der robustere Weg. Prompting kann das auch erreichen, aber Fine-tuning macht es konsistenter. In Produktion zÃ¤hlt nicht der Durchschnitt, sondern der schlechteste Fall.

**Strikte Regeleinhaltung** ist ein weiterer Grund. "Antworte nur basierend auf dem bereitgestellten Kontext" â€“ das klingt einfach, ist aber schwer durchzusetzen. Ein fine-tunetes Modell, das auf tausenden Beispielen gelernt hat, wie "nur aus dem Kontext antworten" aussieht, hÃ¤lt sich zuverlÃ¤ssiger daran als ein generisches Modell mit Prompt-Instruktion.

**DomÃ¤nenspezifische Patterns** profitieren von Fine-tuning. Wenn das Modell bestimmte Fachbegriffe, AbkÃ¼rzungen oder ZusammenhÃ¤nge verstehen soll, die es aus dem allgemeinen Training nicht kennt, hilft Fine-tuning. RAG kann Wissen hinzufÃ¼gen, aber Fine-tuning verÃ¤ndert, wie das Modell dieses Wissen verarbeitet.

**Latenzanforderungen** spielen ebenfalls eine Rolle. Ein fine-tunetes Modell braucht keinen langen System-Prompt, um Verhalten und Format zu erzwingen â€“ das ist bereits trainiert. Selbst in Kombination mit RAG spart das Tokens und damit Latenz. Bei zeitkritischen Anwendungen kann das den Unterschied machen.

**Kostenoptimierung bei hohem Volumen** ist ein wirtschaftlicher Aspekt. Wenn du Millionen von Anfragen pro Monat hast, summieren sich die Kosten fÃ¼r lange Prompts und RAG-Kontexte. Ein fine-tunetes Modell, das mit kÃ¼rzeren Inputs auskommt, kann langfristig gÃ¼nstiger sein.

**Kleinere Modelle mit Spezialisierung** statt groÃŸer Generalisten sind ein oft unterschÃ¤tzter Vorteil. Ein fine-tunetes 7B-Modell kann bei einer spezifischen Aufgabe mit deutlich grÃ¶ÃŸeren Modellen mithalten oder sie sogar Ã¼bertreffen. In unserem Tutorial erreicht das fine-tuned Mistral-7B eine deutlich hÃ¶here Erfolgsrate bei der RAG-QA-Aufgabe als das Base Model. Der Trade-off: Das kleine Modell ist ein Spezialist, kein Generalist. FÃ¼r viele Enterprise-Use-Cases ist das aber genau richtig. Die praktischen Implikationen sind erheblich: weniger VRAM bedeutet gÃ¼nstigere GPU-Hardware, weniger Parameter bedeuten schnellere Inference und niedrigere Latenz.

> **ðŸ’¡ Praxis-Beispiele aus dem DACH-Markt:**
> - **Compliance-Bot:** Versicherung braucht 100% regelkonforme Antworten zu DSGVO-Anfragen â†’ Fine-tuning fÃ¼r strikte Kontexttreue
> - **Ticket-Routing:** Support-System muss Anfragen als JSON mit `{priority, category, department}` klassifizieren â†’ Fine-tuning fÃ¼r konsistentes Format
> - **Echtzeit-Ãœbersetzung:** Meeting-Tool braucht <500ms Latenz â†’ Fine-tuning erlaubt kÃ¼rzere Prompts, kleineres Modell

---

## Wann reicht Prompting oder RAG?

Fine-tuning ist nicht immer die richtige Antwort. Es gibt Szenarien, in denen einfachere AnsÃ¤tze ausreichen.

**In der explorativen Phase** starte mit Prompting. Wenn du noch nicht genau weiÃŸt, was du brauchst, ist es schneller zu iterieren. Du lernst, was funktioniert und was nicht, bevor du in Fine-tuning investierst.

**Wenn ein passendes Instruct Model existiert.** Wenn ein bestehendes Open-Source-Modell â€“ etwa Mistral Instruct, Llama Chat oder Qwen â€“ die Aufgabe bereits gut erledigt, lohnt sich Fine-tuning mÃ¶glicherweise nicht. Hier ist die Frage: Passen die Hosting-Kosten? Ein 70B-Modell braucht deutlich mehr GPU-Ressourcen als ein fine-tunetes 7B-Modell, das die gleiche Aufgabe spezialisiert lÃ¶st. Aber wenn das grÃ¶ÃŸere Modell ohne zusÃ¤tzlichen Trainingsaufwand funktioniert, kann das der pragmatischere Weg sein.

**Bei geringem Anfragevolumen** Ã¼berwiegen die Kosten fÃ¼r Fine-tuning mÃ¶glicherweise nicht den Nutzen. Zeit, Compute, Expertise â€“ das alles hat seinen Preis. Wenn du nur hundert Anfragen pro Tag hast, rechnet sich das selten.

---

## Unser Use Case: AWS Documentation Q&A

Um die Konzepte praktisch zu zeigen, brauchen wir einen konkreten Use Case. Wir haben uns fÃ¼r ein RAG-QA-System fÃ¼r AWS-Dokumentation entschieden.

**Das Szenario:** Ein Benutzer stellt Fragen zu AWS-Services. Das System ruft relevante Dokumentations-Abschnitte ab (RAG) und generiert eine Antwort basierend auf diesem Kontext.

**Warum dieses Beispiel?**

Es ist realistisch. Dokumentations-Assistenten sind einer der hÃ¤ufigsten LLM-Use-Cases in Unternehmen. Die Herausforderungen â€“ Halluzination vermeiden, im Kontext bleiben, konsistentes Format â€“ sind universell.

Es ist reproduzierbar. AWS-Dokumentation ist Ã¶ffentlich verfÃ¼gbar. Du kannst das komplette Tutorial nachvollziehen, ohne eigene sensible Daten zu verwenden.

Es zeigt die Kombination von RAG und Fine-tuning. Das Wissen kommt aus dem Retrieval (die Dokumentations-Chunks). Das Verhalten â€“ nur aus dem Kontext antworten, bestimmtes Format einhalten â€“ kommt aus dem Fine-tuning.

**Was wollen wir erreichen?**

Das fine-tunete Modell soll lernen, Fragen ausschlieÃŸlich basierend auf dem bereitgestellten Kontext zu beantworten. Wenn die Antwort nicht im Kontext steht, soll es das sagen â€“ nicht halluzinieren. Das Format soll konsistent sein. Die Antworten sollen prÃ¤zise und hilfreich sein.

Das Base Model (Mistral-7B ohne Fine-tuning) erreicht bei dieser Aufgabe eine Erfolgsrate von weniger als 50%. Das fine-tuned Modell erreicht nahezu 100%. Dieser Unterschied zeigt den Wert von Fine-tuning fÃ¼r spezifische Verhaltensanforderungen.

---

## Der Fine-tuning-Ansatz: LoRA und Instruction Tuning

Es gibt verschiedene Wege, ein LLM anzupassen. FÃ¼r die meisten praktischen Use Cases ist die Kombination aus **LoRA** und **Instruction Fine-tuning** der sinnvollste Ansatz.

**Continued Pre-training vs. Instruction Fine-tuning**

Continued Pre-training trainiert das Modell auf groÃŸen Mengen DomÃ¤nen-Text, um neues Wissen zu verinnerlichen â€“ Fakten, Terminologie, ZusammenhÃ¤nge. Das braucht viel Daten (Gigabytes bis Terabytes) und erhebliche Compute-Ressourcen. FÃ¼r die meisten Unternehmens-Use-Cases ist das Ã¼berdimensioniert.

Instruction Fine-tuning hingegen passt das Verhalten des Modells an: Wie es antwortet, in welchem Format, mit welchem Stil. Du trainierst auf Beispielen von Input-Output-Paaren, die das gewÃ¼nschte Verhalten demonstrieren. Das ist ressourcenschonender und fÃ¼r die meisten Use Cases der praktikablere Weg â€“ besonders in Kombination mit RAG, das das DomÃ¤nenwissen liefert.

Diese Tutorial-Serie fokussiert auf Instruction Fine-tuning.

**Full Fine-tuning vs. LoRA**

Bei Full Fine-tuning werden alle Parameter des Modells angepasst. Das erfordert viel GPU-Speicher (das gesamte Modell muss im VRAM liegen, plus Optimizer States) und produziert ein komplett neues Modell, das genauso groÃŸ ist wie das Original.

**LoRA (Low-Rank Adaptation)** ist ein effizienter Ansatz. Statt alle Parameter zu Ã¤ndern, werden kleine "Adapter" trainiert â€“ zusÃ¤tzliche Gewichte, die auf das Base Model aufgesetzt werden. Das hat mehrere Vorteile: deutlich weniger Speicherbedarf beim Training, die Adapter sind nur wenige Megabytes groÃŸ (statt Gigabytes fÃ¼r ein volles Modell), und du kannst mehrere Adapter fÃ¼r verschiedene Tasks auf einem Base Model betreiben.

**QLoRA** geht noch einen Schritt weiter: Das Base Model wird quantisiert (z.B. auf 4-bit), was den Speicherbedarf nochmals reduziert. Training auf Consumer-Hardware wird damit mÃ¶glich.

FÃ¼r diese Tutorial-Serie nutzen wir QLoRA â€“ das Base Model wird in 4-bit geladen, die LoRA-Adapter werden in hÃ¶herer PrÃ¤zision trainiert. Das ermÃ¶glicht Training von Mistral-7B auf einer einzelnen L4-GPU mit 24 GB VRAM.

**Base Model vs. Instruct Model**

Eine hÃ¤ufige Frage: Soll ich ein Base Model oder ein bereits instruction-tuned Modell als Ausgangspunkt nehmen?

Base Models (z.B. `mistralai/Mistral-7B-v0.1`) sind auf Text-Completion trainiert. Sie haben kein eingebautes "Assistenten-Verhalten". Der Vorteil: Du startest mit einer neutralen Basis und trainierst genau das Verhalten, das du willst.

Instruct Models (z.B. `mistralai/Mistral-7B-Instruct-v0.2`) sind bereits auf Instruktionen und Dialog trainiert. Sie verhalten sich "out of the box" wie Assistenten. Der Vorteil: Du baust auf bestehendem Verhalten auf.

FÃ¼r unseren Use Case starten wir mit einem Base Model, weil wir sehr spezifisches Verhalten trainieren wollen. In anderen Szenarien kann ein Instruct Model als Basis sinnvoller sein.

---

## Fazit

Fine-tuning ist das richtige Werkzeug, wenn du konsistentes Verhalten brauchst, strikte Regeleinhaltung durchsetzen willst, oder spezialisierte Performance aus einem kleineren Modell herausholen musst. Der SchlÃ¼ssel zum Erfolg liegt nicht im Training selbst, sondern in der Vorbereitung: den richtigen Ansatz wÃ¤hlen und vor allem hochwertige Trainingsdaten erstellen.

**Im nÃ¤chsten Post** zeigen wir genau das: wie du von Rohdokumenten zu einem hochwertigen Instruction-Dataset kommst â€“ Token-aware Chunking, synthetische QA-Generierung und automatisierte Quality Checks.

{% include blog_nav.html current="03-warum-finetuning" %}