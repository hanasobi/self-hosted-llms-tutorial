{"chunk_id": "amazon-faq-6", "model": "mistral-7b", "chunk": "With Amazon MQ, you pay only for what you use. You are charged for the broker instance usage, storage usage, and standard data transfer fees.\nSee\nAmazon MQ pricing\nfor details.\nStarting July 15, 2025, new AWS customers will receive up to $200 in AWS Free Tier credits, which can be applied towards eligible AWS services, including Amazon MQ. At account sign-up, you can choose between a free plan and a paid plan. The free plan will be available for 6 months after account creation. If you upgrade to a paid plan, any remaining Free Tier credit balance will automatically apply to your AWS bills. All Free Tier credits must be used within 12 months of your account creation date. To learn more about the AWS Free Tier program, refer to\nAWS Free Tier website\nand\nAWS Free Tier documentation\n.\n\nNo, all inter-node data transfer is included with the service at no additional charge.\n\nNo, Amazon MQ does not offer reserved instance pricing.\n\nNo, each broker you provision includes boot volume storage managed by the Amazon MQ service.\n\nAmazon MQ is compliant or eligible for the following programs:\nHIPAA eligible\nPCI DSS compliant\nSOC 1,2,3 compliant\nISO 9001, 27001, 27017, and 27018 certified\nFor a complete list of AWS services and compliance programs, please see\nAWS Services in Scope by Compliance Program.\n\n", "question": "What are the charges for Amazon MQ based on the text passage?", "answer": "You pay for broker instance usage, storage usage, and standard data transfer fees with Amazon MQ. For detailed pricing information, refer to Amazon MQ pricing.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "amazon-faq-6", "model": "mistral-7b", "chunk": "With Amazon MQ, you pay only for what you use. You are charged for the broker instance usage, storage usage, and standard data transfer fees.\nSee\nAmazon MQ pricing\nfor details.\nStarting July 15, 2025, new AWS customers will receive up to $200 in AWS Free Tier credits, which can be applied towards eligible AWS services, including Amazon MQ. At account sign-up, you can choose between a free plan and a paid plan. The free plan will be available for 6 months after account creation. If you upgrade to a paid plan, any remaining Free Tier credit balance will automatically apply to your AWS bills. All Free Tier credits must be used within 12 months of your account creation date. To learn more about the AWS Free Tier program, refer to\nAWS Free Tier website\nand\nAWS Free Tier documentation\n.\n\nNo, all inter-node data transfer is included with the service at no additional charge.\n\nNo, Amazon MQ does not offer reserved instance pricing.\n\nNo, each broker you provision includes boot volume storage managed by the Amazon MQ service.\n\nAmazon MQ is compliant or eligible for the following programs:\nHIPAA eligible\nPCI DSS compliant\nSOC 1,2,3 compliant\nISO 9001, 27001, 27017, and 27018 certified\nFor a complete list of AWS services and compliance programs, please see\nAWS Services in Scope by Compliance Program.\n\n", "question": "What benefits does a new AWS customer receive in terms of Amazon MQ and the Free Tier program?", "answer": "New AWS customers receive up to $200 in AWS Free Tier credits, which can be applied towards Amazon MQ starting from July 15, 2025. The credits are available for 6 months after account creation, and any remaining balance automatically applies to AWS bills when upgrading to a paid plan.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "amazon-faq-6", "model": "mistral-7b", "chunk": "With Amazon MQ, you pay only for what you use. You are charged for the broker instance usage, storage usage, and standard data transfer fees.\nSee\nAmazon MQ pricing\nfor details.\nStarting July 15, 2025, new AWS customers will receive up to $200 in AWS Free Tier credits, which can be applied towards eligible AWS services, including Amazon MQ. At account sign-up, you can choose between a free plan and a paid plan. The free plan will be available for 6 months after account creation. If you upgrade to a paid plan, any remaining Free Tier credit balance will automatically apply to your AWS bills. All Free Tier credits must be used within 12 months of your account creation date. To learn more about the AWS Free Tier program, refer to\nAWS Free Tier website\nand\nAWS Free Tier documentation\n.\n\nNo, all inter-node data transfer is included with the service at no additional charge.\n\nNo, Amazon MQ does not offer reserved instance pricing.\n\nNo, each broker you provision includes boot volume storage managed by the Amazon MQ service.\n\nAmazon MQ is compliant or eligible for the following programs:\nHIPAA eligible\nPCI DSS compliant\nSOC 1,2,3 compliant\nISO 9001, 27001, 27017, and 27018 certified\nFor a complete list of AWS services and compliance programs, please see\nAWS Services in Scope by Compliance Program.\n\n", "question": "How does inter-node data transfer compare in terms of charges in Amazon MQ?", "answer": "Inter-node data transfer is included with the service at no additional charge, according to the text passage.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "amplify-faq-7", "model": "mistral-7b", "chunk": "Prices are the same across all regions.\n\n", "question": "In which AWS regions are prices identical?", "answer": "Prices are identical across all AWS regions.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "amplify-faq-7", "model": "mistral-7b", "chunk": "Prices are the same across all regions.\n\n", "question": "Why is there price uniformity across different AWS regions?", "answer": "The text passage states that prices are the same across all regions, but it does not provide information on the reason for this price uniformity.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "amplify-faq-7", "model": "mistral-7b", "chunk": "Prices are the same across all regions.\n\n", "question": "How does pricing consistency in AWS regions compare to pricing variability in other cloud providers?", "answer": "The text passage does not provide enough context to make a comparison between pricing consistency in AWS regions and pricing variability in other cloud providers.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "api-faq-3", "model": "mistral-7b", "chunk": "Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs.\nHTTP API\n: HTTP APIs are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads. They do not currently offer API management functionality.\nREST API\n: REST APIs offer API proxy functionality and API management features in a single solution. REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs.\nWebSocket API:\nWebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. With WebSocket APIs in API Gateway, you can define backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint to be invoked when messages are received from the connected clients.\n\nTo get started with HTTP APIs, you can use the Amazon API Gateway console, the AWS CLI, AWS SDKs, or AWS CloudFormation. To learn more about getting started with HTTP APIs, visit our\ndocumentation\n.\n\nTo get started with REST APIs, you can use the Amazon API Gateway console, the AWS CLI, or AWS SDKs. To learn more about getting started with REST APIs, visit our\ndocumentation\n.\n\n", "question": "What is the primary use case for Amazon API Gateway's HTTP APIs?", "answer": "HTTP APIs in Amazon API Gateway are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "api-faq-3", "model": "mistral-7b", "chunk": "Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs.\nHTTP API\n: HTTP APIs are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads. They do not currently offer API management functionality.\nREST API\n: REST APIs offer API proxy functionality and API management features in a single solution. REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs.\nWebSocket API:\nWebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. With WebSocket APIs in API Gateway, you can define backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint to be invoked when messages are received from the connected clients.\n\nTo get started with HTTP APIs, you can use the Amazon API Gateway console, the AWS CLI, AWS SDKs, or AWS CloudFormation. To learn more about getting started with HTTP APIs, visit our\ndocumentation\n.\n\nTo get started with REST APIs, you can use the Amazon API Gateway console, the AWS CLI, or AWS SDKs. To learn more about getting started with REST APIs, visit our\ndocumentation\n.\n\n", "question": "How does Amazon API Gateway's REST APIs differ from HTTP APIs in functionality?", "answer": "REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs in addition to API proxy functionality.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "api-faq-3", "model": "mistral-7b", "chunk": "Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs.\nHTTP API\n: HTTP APIs are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads. They do not currently offer API management functionality.\nREST API\n: REST APIs offer API proxy functionality and API management features in a single solution. REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs.\nWebSocket API:\nWebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. With WebSocket APIs in API Gateway, you can define backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint to be invoked when messages are received from the connected clients.\n\nTo get started with HTTP APIs, you can use the Amazon API Gateway console, the AWS CLI, AWS SDKs, or AWS CloudFormation. To learn more about getting started with HTTP APIs, visit our\ndocumentation\n.\n\nTo get started with REST APIs, you can use the Amazon API Gateway console, the AWS CLI, or AWS SDKs. To learn more about getting started with REST APIs, visit our\ndocumentation\n.\n\n", "question": "Why would you choose to use Amazon API Gateway's WebSocket APIs over HTTP or REST APIs?", "answer": "WebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. They can be used to invoke backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint when messages are received from the connected clients.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "appflow-faq-0", "model": "mistral-7b", "chunk": "Analytics\n›\nAmazon Appflow\n›\nFAQs\n\nGeneral\n7\nSaaS Integrations\n4\nPrivacy and Security\n2\nAWS Services\n5\n\nPrivacy and Security\n\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. With AppFlow, you can run data flows at nearly any scale at the frequency you choose - on a schedule, in response to a business event, or on demand. You can configure powerful data transformation capabilities like filtering and validation to generate rich, ready-to-use data as part of the flow itself, without additional steps. AppFlow automatically encrypts data in motion, and allows users to restrict data from flowing over the public Internet for SaaS applications that are integrated with AWS PrivateLink, reducing exposure to security threats.\n\n", "question": "What SaaS applications can Amazon AppFlow integrate with?", "answer": "Amazon AppFlow can integrate with applications like Salesforce, Marketo, Slack, and ServiceNow.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "appflow-faq-0", "model": "mistral-7b", "chunk": "Analytics\n›\nAmazon Appflow\n›\nFAQs\n\nGeneral\n7\nSaaS Integrations\n4\nPrivacy and Security\n2\nAWS Services\n5\n\nPrivacy and Security\n\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. With AppFlow, you can run data flows at nearly any scale at the frequency you choose - on a schedule, in response to a business event, or on demand. You can configure powerful data transformation capabilities like filtering and validation to generate rich, ready-to-use data as part of the flow itself, without additional steps. AppFlow automatically encrypts data in motion, and allows users to restrict data from flowing over the public Internet for SaaS applications that are integrated with AWS PrivateLink, reducing exposure to security threats.\n\n", "question": "How does Amazon AppFlow enhance data security?", "answer": "Amazon AppFlow enhances data security by automatically encrypting data in motion and allowing users to restrict data from flowing over the public Internet for AWS PrivateLink integrated applications.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "appflow-faq-0", "model": "mistral-7b", "chunk": "Analytics\n›\nAmazon Appflow\n›\nFAQs\n\nGeneral\n7\nSaaS Integrations\n4\nPrivacy and Security\n2\nAWS Services\n5\n\nPrivacy and Security\n\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. With AppFlow, you can run data flows at nearly any scale at the frequency you choose - on a schedule, in response to a business event, or on demand. You can configure powerful data transformation capabilities like filtering and validation to generate rich, ready-to-use data as part of the flow itself, without additional steps. AppFlow automatically encrypts data in motion, and allows users to restrict data from flowing over the public Internet for SaaS applications that are integrated with AWS PrivateLink, reducing exposure to security threats.\n\n", "question": "How does Amazon AppFlow compare to other AWS services for data transfer between SaaS applications and AWS services?", "answer": "Amazon AppFlow is a fully managed integration service that allows you to securely transfer data between SaaS applications and AWS services with data transformation capabilities, automatic encryption, and the ability to restrict data from flowing over the public Internet.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "application-faq-3", "model": "mistral-7b", "chunk": "Please refer to the\nAWS Regional Services List\nfor the most up-to-date information.\n\n", "question": "What document should I consult for the latest information about AWS regions?", "answer": "The AWS Regional Services List", "question_type": "", "original_pair_id": ""}
{"chunk_id": "application-faq-3", "model": "mistral-7b", "chunk": "Please refer to the\nAWS Regional Services List\nfor the most up-to-date information.\n\n", "question": "How does AWS provide information about the availability of its services across different regions?", "answer": "They maintain a list called the AWS Regional Services List, which is the most up-to-date source.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "application-faq-3", "model": "mistral-7b", "chunk": "Please refer to the\nAWS Regional Services List\nfor the most up-to-date information.\n\n", "question": "Which document should I consult if I want to know about the availability of AWS services in different regions and their current status?", "answer": "You should refer to the AWS Regional Services List.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "appsync-faq-3", "model": "mistral-7b", "chunk": "AWS AppSync enables customers to use custom domain names with their AWS AppSync API to access their GraphQl endpoint and real-time endpoint. To create a custom domain name in AppSync, you simply provide a domain name you own and indicate a valid AWS Certificate Manager (ACM) certificate that covers your domain. Once the custom domain name is created, you can associate the domain name with any available AppSync API in your account. After you have updated your DNS record to map to to the AppSync-provided domain name, you can configure your applications to use the new GraphQL and real-time endpoints. You can change the API association on your custom domain at any time without having to update your applications. When AppSync receives a request on the custom domain endpoint, it routes it to the associated API for handling.\n\nYes, AWS AppSync supports private APIs. With private APIs, you can create GraphQL APIs that can only be accessed from your Amazon Virtual Private Cloud (VPC).\n\nAn \"event\" is a data record expressing an occurrence and its context. Events are routed from an event producer (the source) to interested event consumers. AppSync Events gives developers the ability to publish events using the HTTP protocol and JSON format. Subscribing is done over WebSocket channels.\n\nA channel is a routing concept. It is the logical destination of an event. It allows a publisher to say: “this is where my event should be routed.” It allows a subscriber to say: “this is the route that I want to monitor for arriving events”.\n\nA namespace is a logical construct that is used to define capabilities shared by channels that live within it. For example, you can define multiple authorization modes for your Event API, then attach specific modes on your namespaces (e.g.: only allow IAM_AUTH mode on namespace /chat).\n\nYes. Simply attach an Event Handler that specifies an onPublish function. In that handler, implement your transformation logic.\n\n", "question": "What is required to create a custom domain name in AWS AppSync?", "answer": "To create a custom domain name in AWS AppSync, you need to provide a domain name you own and indicate a valid AWS Certificate Manager (ACM) certificate that covers your domain.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "appsync-faq-3", "model": "mistral-7b", "chunk": "AWS AppSync enables customers to use custom domain names with their AWS AppSync API to access their GraphQl endpoint and real-time endpoint. To create a custom domain name in AppSync, you simply provide a domain name you own and indicate a valid AWS Certificate Manager (ACM) certificate that covers your domain. Once the custom domain name is created, you can associate the domain name with any available AppSync API in your account. After you have updated your DNS record to map to to the AppSync-provided domain name, you can configure your applications to use the new GraphQL and real-time endpoints. You can change the API association on your custom domain at any time without having to update your applications. When AppSync receives a request on the custom domain endpoint, it routes it to the associated API for handling.\n\nYes, AWS AppSync supports private APIs. With private APIs, you can create GraphQL APIs that can only be accessed from your Amazon Virtual Private Cloud (VPC).\n\nAn \"event\" is a data record expressing an occurrence and its context. Events are routed from an event producer (the source) to interested event consumers. AppSync Events gives developers the ability to publish events using the HTTP protocol and JSON format. Subscribing is done over WebSocket channels.\n\nA channel is a routing concept. It is the logical destination of an event. It allows a publisher to say: “this is where my event should be routed.” It allows a subscriber to say: “this is the route that I want to monitor for arriving events”.\n\nA namespace is a logical construct that is used to define capabilities shared by channels that live within it. For example, you can define multiple authorization modes for your Event API, then attach specific modes on your namespaces (e.g.: only allow IAM_AUTH mode on namespace /chat).\n\nYes. Simply attach an Event Handler that specifies an onPublish function. In that handler, implement your transformation logic.\n\n", "question": "How does AWS AppSync handle requests on a custom domain endpoint?", "answer": "When AppSync receives a request on the custom domain endpoint, it routes it to the associated API for handling.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "appsync-faq-3", "model": "mistral-7b", "chunk": "AWS AppSync enables customers to use custom domain names with their AWS AppSync API to access their GraphQl endpoint and real-time endpoint. To create a custom domain name in AppSync, you simply provide a domain name you own and indicate a valid AWS Certificate Manager (ACM) certificate that covers your domain. Once the custom domain name is created, you can associate the domain name with any available AppSync API in your account. After you have updated your DNS record to map to to the AppSync-provided domain name, you can configure your applications to use the new GraphQL and real-time endpoints. You can change the API association on your custom domain at any time without having to update your applications. When AppSync receives a request on the custom domain endpoint, it routes it to the associated API for handling.\n\nYes, AWS AppSync supports private APIs. With private APIs, you can create GraphQL APIs that can only be accessed from your Amazon Virtual Private Cloud (VPC).\n\nAn \"event\" is a data record expressing an occurrence and its context. Events are routed from an event producer (the source) to interested event consumers. AppSync Events gives developers the ability to publish events using the HTTP protocol and JSON format. Subscribing is done over WebSocket channels.\n\nA channel is a routing concept. It is the logical destination of an event. It allows a publisher to say: “this is where my event should be routed.” It allows a subscriber to say: “this is the route that I want to monitor for arriving events”.\n\nA namespace is a logical construct that is used to define capabilities shared by channels that live within it. For example, you can define multiple authorization modes for your Event API, then attach specific modes on your namespaces (e.g.: only allow IAM_AUTH mode on namespace /chat).\n\nYes. Simply attach an Event Handler that specifies an onPublish function. In that handler, implement your transformation logic.\n\n", "question": "What is the difference between a channel and a namespace in AWS AppSync?", "answer": "A channel is a logical destination for an event, while a namespace is a logical construct used to define capabilities shared by channels that live within it.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "artifact-faq-0", "model": "mistral-7b", "chunk": "Security, Identity and Compliance\n›\nAWS Artifact\n›\nAWS Artifact FAQs\n\nGeneral\n8\nAWS Reports\n3\nThird-Party Reports\n4\nAgreements\n20\nBusiness Associate Addendum (BAA)\n18\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n9\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n9\nTroubleshooting\n2\nNotifications\n8\n\nBusiness Associate Addendum (BAA)\n\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n\nAWS Artifact, available in the console, is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS’ compliance documentation and AWS agreements.\nYou can use AWS Artifact Reports to download AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports.\nYou can use AWS Artifact Agreements to review, accept, and track the status of AWS agreements such as the Business Associate Addendum (BAA).\n\nAll AWS Accounts have access to AWS Artifact. Root users and IAM users with admin permissions can download all audit artifacts available to their account by agreeing to the associated terms and conditions.\nYou will need to grant IAM users with non-admin permissions access to AWS Artifact using IAM permissions. This allows you to grant a user access to AWS Artifact, while restricting access to other services and resources within your AWS Account. For information on how to grant access using IAM, refer to this help topic in the\nAWS Artifact documentation\n.\n\nAn Agreement can have two states:\nInactive:\nAn Agreement is in Inactive state if the Agreement has not been accepted by the user or a previously accepted Agreement has been terminated by the user\nActive:\nAn Agreement is in Active state if the Agreement has been accepted by the user\n\n", "question": "What type of documents can I download using AWS Artifact Reports?", "answer": "AWS Artifact Reports allow you to download AWS security and compliance documents such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "artifact-faq-0", "model": "mistral-7b", "chunk": "Security, Identity and Compliance\n›\nAWS Artifact\n›\nAWS Artifact FAQs\n\nGeneral\n8\nAWS Reports\n3\nThird-Party Reports\n4\nAgreements\n20\nBusiness Associate Addendum (BAA)\n18\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n9\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n9\nTroubleshooting\n2\nNotifications\n8\n\nBusiness Associate Addendum (BAA)\n\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n\nAWS Artifact, available in the console, is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS’ compliance documentation and AWS agreements.\nYou can use AWS Artifact Reports to download AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports.\nYou can use AWS Artifact Agreements to review, accept, and track the status of AWS agreements such as the Business Associate Addendum (BAA).\n\nAll AWS Accounts have access to AWS Artifact. Root users and IAM users with admin permissions can download all audit artifacts available to their account by agreeing to the associated terms and conditions.\nYou will need to grant IAM users with non-admin permissions access to AWS Artifact using IAM permissions. This allows you to grant a user access to AWS Artifact, while restricting access to other services and resources within your AWS Account. For information on how to grant access using IAM, refer to this help topic in the\nAWS Artifact documentation\n.\n\nAn Agreement can have two states:\nInactive:\nAn Agreement is in Inactive state if the Agreement has not been accepted by the user or a previously accepted Agreement has been terminated by the user\nActive:\nAn Agreement is in Active state if the Agreement has been accepted by the user\n\n", "question": "What are the two states of an Agreement in AWS Artifact?", "answer": "An Agreement in AWS Artifact can have two states: Inactive and Active. An Agreement is in Inactive state if it has not been accepted by the user or if a previously accepted Agreement has been terminated by the user. An Agreement is in Active state if it has been accepted by the user.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "artifact-faq-0", "model": "mistral-7b", "chunk": "Security, Identity and Compliance\n›\nAWS Artifact\n›\nAWS Artifact FAQs\n\nGeneral\n8\nAWS Reports\n3\nThird-Party Reports\n4\nAgreements\n20\nBusiness Associate Addendum (BAA)\n18\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n9\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n9\nTroubleshooting\n2\nNotifications\n8\n\nBusiness Associate Addendum (BAA)\n\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n\nAWS Artifact, available in the console, is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS’ compliance documentation and AWS agreements.\nYou can use AWS Artifact Reports to download AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports.\nYou can use AWS Artifact Agreements to review, accept, and track the status of AWS agreements such as the Business Associate Addendum (BAA).\n\nAll AWS Accounts have access to AWS Artifact. Root users and IAM users with admin permissions can download all audit artifacts available to their account by agreeing to the associated terms and conditions.\nYou will need to grant IAM users with non-admin permissions access to AWS Artifact using IAM permissions. This allows you to grant a user access to AWS Artifact, while restricting access to other services and resources within your AWS Account. For information on how to grant access using IAM, refer to this help topic in the\nAWS Artifact documentation\n.\n\nAn Agreement can have two states:\nInactive:\nAn Agreement is in Inactive state if the Agreement has not been accepted by the user or a previously accepted Agreement has been terminated by the user\nActive:\nAn Agreement is in Active state if the Agreement has been accepted by the user\n\n", "question": "What is the difference between the Business Associate Addendum (BAA) and the AWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum) in AWS Artifact?", "answer": "The Business Associate Addendum (BAA) and the AWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum) are both agreements available in AWS Artifact. The BAA is used for compliance with the Health Insurance Portability and Accountability Act (HIPAA) in Australia. The NZNDB Addendum is used for compliance with the New Zealand Notifiable Data Breach Act.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "athena-faq-3", "model": "mistral-7b", "chunk": "AWS Glue\nis a fully managed extract, transform, and load (ETL) service. AWS Glue has three main components: 1) a crawler that automatically scans your data sources, identifies data formats, and infers schemas, 2) a fully managed ETL service that allows you to transform and move data to various destinations, and 3) a Data Catalog that stores metadata information about databases and tables either stored in S3 or an\nODBC\n- or\nJDBC\n-compliant data store. To use the benefits of AWS Glue, you must upgrade from using Athena’s internal Data Catalog to the Glue Data Catalog.\nBenefits of upgrading to the Data Catalog include the following:\nUnified metadata repository: AWS Glue is integrated across various AWS services. AWS Glue supports data stored in Amazon Aurora, Amazon Relational Database Service (RDS) for MySQL, Amazon RDS for PostgreSQL, Amazon Redshift, and S3, as well as MySQL and PostgreSQL databases in your Amazon Virtual Private Cloud (VPC) running on Amazon Elastic Compute Cloud (EC2). AWS Glue provides out-of-the-box integration with Athena, Amazon EMR, Amazon Redshift Spectrum, and applications compatible with Apache Hive metastore.\nAutomatic schema and partition recognition: AWS Glue automatically crawls your data sources, identifies data formats, and suggests schemas and transformations. Crawlers can help automate table creation and automatic loading of partitions.\nTo learn more about the Data Catalog, review the\nAWS Glue\nwebpage.\n\nYes. For a step-by-step process, review the Amazon Athena User Guide:\nIntegration with AWS Glue\n.\n\nFor details of Athena service availability by Region, review the\nAWS Regional Services List\n.\n\nTo learn more about service limits, review the Amazon Athena User Guide: Service Quotas.\n\n", "question": "What are the three main components of AWS Glue?", "answer": "AWS Glue has three main components: 1) a crawler that automatically scans your data sources, identifies data formats, and infers schemas, 2) a fully managed ETL service that allows you to transform and move data to various destinations, and 3) a Data Catalog that stores metadata information about databases and tables either stored in S3 or an ODBC- or JDBC-compliant data store.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "athena-faq-3", "model": "mistral-7b", "chunk": "AWS Glue\nis a fully managed extract, transform, and load (ETL) service. AWS Glue has three main components: 1) a crawler that automatically scans your data sources, identifies data formats, and infers schemas, 2) a fully managed ETL service that allows you to transform and move data to various destinations, and 3) a Data Catalog that stores metadata information about databases and tables either stored in S3 or an\nODBC\n- or\nJDBC\n-compliant data store. To use the benefits of AWS Glue, you must upgrade from using Athena’s internal Data Catalog to the Glue Data Catalog.\nBenefits of upgrading to the Data Catalog include the following:\nUnified metadata repository: AWS Glue is integrated across various AWS services. AWS Glue supports data stored in Amazon Aurora, Amazon Relational Database Service (RDS) for MySQL, Amazon RDS for PostgreSQL, Amazon Redshift, and S3, as well as MySQL and PostgreSQL databases in your Amazon Virtual Private Cloud (VPC) running on Amazon Elastic Compute Cloud (EC2). AWS Glue provides out-of-the-box integration with Athena, Amazon EMR, Amazon Redshift Spectrum, and applications compatible with Apache Hive metastore.\nAutomatic schema and partition recognition: AWS Glue automatically crawls your data sources, identifies data formats, and suggests schemas and transformations. Crawlers can help automate table creation and automatic loading of partitions.\nTo learn more about the Data Catalog, review the\nAWS Glue\nwebpage.\n\nYes. For a step-by-step process, review the Amazon Athena User Guide:\nIntegration with AWS Glue\n.\n\nFor details of Athena service availability by Region, review the\nAWS Regional Services List\n.\n\nTo learn more about service limits, review the Amazon Athena User Guide: Service Quotas.\n\n", "question": "Why would I use AWS Glue's Data Catalog instead of Athena's?", "answer": "Upgrading to AWS Glue's Data Catalog provides a unified metadata repository, as Glue is integrated across various AWS services, supporting data stored in Amazon Aurora, Amazon RDS for MySQL, Amazon RDS for PostgreSQL, Amazon Redshift, and S3, as well as MySQL and PostgreSQL databases in your Amazon Virtual Private Cloud (VPC) running on Amazon Elastic Compute Cloud (EC2). Glue also offers automatic schema and partition recognition through its crawlers, helping automate table creation and partition loading.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "athena-faq-3", "model": "mistral-7b", "chunk": "AWS Glue\nis a fully managed extract, transform, and load (ETL) service. AWS Glue has three main components: 1) a crawler that automatically scans your data sources, identifies data formats, and infers schemas, 2) a fully managed ETL service that allows you to transform and move data to various destinations, and 3) a Data Catalog that stores metadata information about databases and tables either stored in S3 or an\nODBC\n- or\nJDBC\n-compliant data store. To use the benefits of AWS Glue, you must upgrade from using Athena’s internal Data Catalog to the Glue Data Catalog.\nBenefits of upgrading to the Data Catalog include the following:\nUnified metadata repository: AWS Glue is integrated across various AWS services. AWS Glue supports data stored in Amazon Aurora, Amazon Relational Database Service (RDS) for MySQL, Amazon RDS for PostgreSQL, Amazon Redshift, and S3, as well as MySQL and PostgreSQL databases in your Amazon Virtual Private Cloud (VPC) running on Amazon Elastic Compute Cloud (EC2). AWS Glue provides out-of-the-box integration with Athena, Amazon EMR, Amazon Redshift Spectrum, and applications compatible with Apache Hive metastore.\nAutomatic schema and partition recognition: AWS Glue automatically crawls your data sources, identifies data formats, and suggests schemas and transformations. Crawlers can help automate table creation and automatic loading of partitions.\nTo learn more about the Data Catalog, review the\nAWS Glue\nwebpage.\n\nYes. For a step-by-step process, review the Amazon Athena User Guide:\nIntegration with AWS Glue\n.\n\nFor details of Athena service availability by Region, review the\nAWS Regional Services List\n.\n\nTo learn more about service limits, review the Amazon Athena User Guide: Service Quotas.\n\n", "question": "How does the schema and partition recognition of AWS Glue differ from Athena?", "answer": "AWS Glue's schema and partition recognition automatically crawls your data sources and suggests schemas and transformations, helping automate table creation and partition loading. Athena, on the other hand, does not have this functionality and requires users to manually create table schemas and partition configurations.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "audit-faq-3", "model": "mistral-7b", "chunk": "A control is a prescriptive description that explains how to implement a procedure to conform to a given rule, such as a compliance requirement. It provides a reasonable assurance that the resources used by your organization operate as intended, that data is reliable, and that your organization is in compliance with applicable laws and regulations.\n\nAWS Audit Manager enables you to define your own controls to collect evidence from specific data sources to help you meet unique compliance requirements.\nA common control collects evidence that can support a range of overlapping compliance obligations. Each common control consists of one or more core controls that collects evidence from a predefined group of AWS managed data sources. AWS updates these underlying data sources for you when regulations and standards change and new data sources are identified.\n\nAn AWS Audit Manager assessment is an implementation of an AWS Audit Manager framework. Using a framework as a starting point, you can create an assessment and define the AWS accounts that you want to include in the scope of your audit. After your assessment is created, AWS Audit Manager begins to automatically assess resources in your AWS accounts and services based on the controls deﬁned in the framework. Next, it collects the relevant evidence and converts it into an auditor-friendly format, and then attaches it to the controls in your assessment.\n\nA resource assessment is a process that collects, stores, and manages evidence, which you can use to assess risk and compliance with industry standards and regulations. When you define and launch an assessment based on an assessment framework, Audit Manager will execute a resource assessment for each individual resource, such as your Amazon EC2 instances, Amazon RDS instances, Amazon S3 buckets, or Amazon VPC subnets.\n\nEvidence is a record that contains the information needed to demonstrate compliance with the requirements specified by a control. Examples of evidence could be a change activity triggered by a user, or a system configuration snapshot.\n\nAn assessment report is a finalized document generated from an AWS Audit Manager assessment. The report summarizes the relevant evidence collected for your audit. The report links to the relevant evidence folders, which are named and organized according to the controls that are specified in your assessment.\n\n", "question": "What is a control in the context of AWS Audit Manager?", "answer": "A control is a prescriptive description that explains how to implement a procedure to conform to a given rule, providing reasonable assurance that resources operate as intended and data is reliable, as well as compliance with applicable laws and regulations.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "audit-faq-3", "model": "mistral-7b", "chunk": "A control is a prescriptive description that explains how to implement a procedure to conform to a given rule, such as a compliance requirement. It provides a reasonable assurance that the resources used by your organization operate as intended, that data is reliable, and that your organization is in compliance with applicable laws and regulations.\n\nAWS Audit Manager enables you to define your own controls to collect evidence from specific data sources to help you meet unique compliance requirements.\nA common control collects evidence that can support a range of overlapping compliance obligations. Each common control consists of one or more core controls that collects evidence from a predefined group of AWS managed data sources. AWS updates these underlying data sources for you when regulations and standards change and new data sources are identified.\n\nAn AWS Audit Manager assessment is an implementation of an AWS Audit Manager framework. Using a framework as a starting point, you can create an assessment and define the AWS accounts that you want to include in the scope of your audit. After your assessment is created, AWS Audit Manager begins to automatically assess resources in your AWS accounts and services based on the controls deﬁned in the framework. Next, it collects the relevant evidence and converts it into an auditor-friendly format, and then attaches it to the controls in your assessment.\n\nA resource assessment is a process that collects, stores, and manages evidence, which you can use to assess risk and compliance with industry standards and regulations. When you define and launch an assessment based on an assessment framework, Audit Manager will execute a resource assessment for each individual resource, such as your Amazon EC2 instances, Amazon RDS instances, Amazon S3 buckets, or Amazon VPC subnets.\n\nEvidence is a record that contains the information needed to demonstrate compliance with the requirements specified by a control. Examples of evidence could be a change activity triggered by a user, or a system configuration snapshot.\n\nAn assessment report is a finalized document generated from an AWS Audit Manager assessment. The report summarizes the relevant evidence collected for your audit. The report links to the relevant evidence folders, which are named and organized according to the controls that are specified in your assessment.\n\n", "question": "How does AWS Audit Manager help with defining controls for unique compliance requirements?", "answer": "AWS Audit Manager allows users to define their own controls and collect evidence from specific data sources to help meet unique compliance requirements.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "audit-faq-3", "model": "mistral-7b", "chunk": "A control is a prescriptive description that explains how to implement a procedure to conform to a given rule, such as a compliance requirement. It provides a reasonable assurance that the resources used by your organization operate as intended, that data is reliable, and that your organization is in compliance with applicable laws and regulations.\n\nAWS Audit Manager enables you to define your own controls to collect evidence from specific data sources to help you meet unique compliance requirements.\nA common control collects evidence that can support a range of overlapping compliance obligations. Each common control consists of one or more core controls that collects evidence from a predefined group of AWS managed data sources. AWS updates these underlying data sources for you when regulations and standards change and new data sources are identified.\n\nAn AWS Audit Manager assessment is an implementation of an AWS Audit Manager framework. Using a framework as a starting point, you can create an assessment and define the AWS accounts that you want to include in the scope of your audit. After your assessment is created, AWS Audit Manager begins to automatically assess resources in your AWS accounts and services based on the controls deﬁned in the framework. Next, it collects the relevant evidence and converts it into an auditor-friendly format, and then attaches it to the controls in your assessment.\n\nA resource assessment is a process that collects, stores, and manages evidence, which you can use to assess risk and compliance with industry standards and regulations. When you define and launch an assessment based on an assessment framework, Audit Manager will execute a resource assessment for each individual resource, such as your Amazon EC2 instances, Amazon RDS instances, Amazon S3 buckets, or Amazon VPC subnets.\n\nEvidence is a record that contains the information needed to demonstrate compliance with the requirements specified by a control. Examples of evidence could be a change activity triggered by a user, or a system configuration snapshot.\n\nAn assessment report is a finalized document generated from an AWS Audit Manager assessment. The report summarizes the relevant evidence collected for your audit. The report links to the relevant evidence folders, which are named and organized according to the controls that are specified in your assessment.\n\n", "question": "How does a resource assessment process differ from an assessment report in AWS Audit Manager?", "answer": "A resource assessment is a process that collects, stores, and manages evidence for individual resources, while an assessment report is a finalized document that summarizes the relevant evidence collected for an audit and links to the evidence folders.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "augmented-faq-1", "model": "mistral-7b", "chunk": "Amazon A2I provides a managed experience where you can set up an entire human review workflow in a few easy steps. To get started with Amazon A2I, sign in to your AWS Console, and navigate to the Amazon SageMaker console. From there, select Human review workflows under Augmented AI. First, as a part of the human review workflow, you provide a pointer to the S3 bucket where the review results should be stored. Next, you select the appropriate task type and define conditions when a human review should be triggered. Amazon A2I provides pre-built workflows where you only need to enter a few choices and provide instructions on how your objects should be reviewed by humans. Alternatively, you can create your own custom workflow and use your own custom review templates. Once created, the workflow can be used directly in your applications using a generated unique identifier for this workflow.\n\nWith A2I, you can define what is an acceptable prediction confidence for your business problem. You can define business rules for the machine learning predictions, based on which a human review is triggered. For Amazon Rekognition image moderation tasks, you can use the confidence score that Amazon Rekognition provides for each label it outputs to trigger human review. For Amazon Textract tasks, you can trigger a human review when specific form keys are missing or when form key detection confidence is low. You can also trigger a human review if, after evaluating all form keys in the text, confidence is lower than your required threshold for any form key. For your own custom workflow, you can write the code for business conditions in AWS Lambda or directly in your client application.\n\nWith Amazon A2I, you can choose from three workforce options: (1) Amazon Mechanical Turk; (2) Third party data labeling service providers available through the AWS Marketplace; and (3) Your own employees. See the\nAmazon A2I developer guide\nfor more information.\n\nPlease see the\nAmazon A2I pricing page\nfor the current pricing information.\n\nThe\nAWS Region Table\nlists all the AWS regions where Amazon A2I is currently available.\n\n", "question": "What is the process for setting up a human review workflow in Amazon A2I?", "answer": "To set up a human review workflow in Amazon A2I, sign in to your AWS Console, navigate to the Amazon SageMaker console, select Human review workflows under Augmented AI. Provide a pointer to the S3 bucket where the review results should be stored, select the appropriate task type and define conditions when a human review should be triggered. Amazon A2I provides pre-built workflows or allows users to create their own custom workflows and use their own custom review templates. Once created, the workflow can be used directly in applications using a generated unique identifier.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "augmented-faq-1", "model": "mistral-7b", "chunk": "Amazon A2I provides a managed experience where you can set up an entire human review workflow in a few easy steps. To get started with Amazon A2I, sign in to your AWS Console, and navigate to the Amazon SageMaker console. From there, select Human review workflows under Augmented AI. First, as a part of the human review workflow, you provide a pointer to the S3 bucket where the review results should be stored. Next, you select the appropriate task type and define conditions when a human review should be triggered. Amazon A2I provides pre-built workflows where you only need to enter a few choices and provide instructions on how your objects should be reviewed by humans. Alternatively, you can create your own custom workflow and use your own custom review templates. Once created, the workflow can be used directly in your applications using a generated unique identifier for this workflow.\n\nWith A2I, you can define what is an acceptable prediction confidence for your business problem. You can define business rules for the machine learning predictions, based on which a human review is triggered. For Amazon Rekognition image moderation tasks, you can use the confidence score that Amazon Rekognition provides for each label it outputs to trigger human review. For Amazon Textract tasks, you can trigger a human review when specific form keys are missing or when form key detection confidence is low. You can also trigger a human review if, after evaluating all form keys in the text, confidence is lower than your required threshold for any form key. For your own custom workflow, you can write the code for business conditions in AWS Lambda or directly in your client application.\n\nWith Amazon A2I, you can choose from three workforce options: (1) Amazon Mechanical Turk; (2) Third party data labeling service providers available through the AWS Marketplace; and (3) Your own employees. See the\nAmazon A2I developer guide\nfor more information.\n\nPlease see the\nAmazon A2I pricing page\nfor the current pricing information.\n\nThe\nAWS Region Table\nlists all the AWS regions where Amazon A2I is currently available.\n\n", "question": "What are the three workforce options available with Amazon A2I?", "answer": "Amazon A2I offers three workforce options: (1) Amazon Mechanical Turk; (2) Third party data labeling service providers available through the AWS Marketplace; and (3) Your own employees.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "augmented-faq-1", "model": "mistral-7b", "chunk": "Amazon A2I provides a managed experience where you can set up an entire human review workflow in a few easy steps. To get started with Amazon A2I, sign in to your AWS Console, and navigate to the Amazon SageMaker console. From there, select Human review workflows under Augmented AI. First, as a part of the human review workflow, you provide a pointer to the S3 bucket where the review results should be stored. Next, you select the appropriate task type and define conditions when a human review should be triggered. Amazon A2I provides pre-built workflows where you only need to enter a few choices and provide instructions on how your objects should be reviewed by humans. Alternatively, you can create your own custom workflow and use your own custom review templates. Once created, the workflow can be used directly in your applications using a generated unique identifier for this workflow.\n\nWith A2I, you can define what is an acceptable prediction confidence for your business problem. You can define business rules for the machine learning predictions, based on which a human review is triggered. For Amazon Rekognition image moderation tasks, you can use the confidence score that Amazon Rekognition provides for each label it outputs to trigger human review. For Amazon Textract tasks, you can trigger a human review when specific form keys are missing or when form key detection confidence is low. You can also trigger a human review if, after evaluating all form keys in the text, confidence is lower than your required threshold for any form key. For your own custom workflow, you can write the code for business conditions in AWS Lambda or directly in your client application.\n\nWith Amazon A2I, you can choose from three workforce options: (1) Amazon Mechanical Turk; (2) Third party data labeling service providers available through the AWS Marketplace; and (3) Your own employees. See the\nAmazon A2I developer guide\nfor more information.\n\nPlease see the\nAmazon A2I pricing page\nfor the current pricing information.\n\nThe\nAWS Region Table\nlists all the AWS regions where Amazon A2I is currently available.\n\n", "question": "How does the confidence score from Amazon Rekognition impact human review triggering in Amazon A2I?", "answer": "The confidence score that Amazon Rekognition provides for each label it outputs can be used to trigger human review in Amazon A2I for image moderation tasks. If the confidence score is below a specified threshold, a human review will be triggered.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "autoscaling-faq-3", "model": "mistral-7b", "chunk": "You should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling lets you define dynamic scaling policies for multiple EC2 Auto Scaling groups or other resources using predefined scaling strategies. Using AWS Auto Scaling to configure scaling policies for all of the scalable resources in your application is faster than managing scaling policies for each resource via its individual service console. It’s also easier, as AWS Auto Scaling includes predefined scaling strategies that simplify the setup of scaling policies. You should also use AWS Auto Scaling if you want to create predictive scaling for EC2 resources.\nYou should use EC2 Auto Scaling if you only need to scale Amazon EC2 Auto Scaling groups, or if you are only interested in maintaining the health of your EC2 fleet. You should also use EC2 Auto Scaling if you need to create or configure Amazon EC2 Auto Scaling groups, or if you need to set up scheduled or step scaling policies (as AWS Auto Scaling supports only target tracking scaling policies).\nEC2 Auto Scaling groups must be created and configured outside of AWS Auto Scaling, such as through the EC2 console, Auto Scaling API or via CloudFormation. AWS Auto Scaling can help you configure dynamic scaling policies for your existing EC2 Auto Scaling groups.\n\nYou should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling enables unified scaling for multiple resources, and has predefined guidance that helps make it easier and faster to configure scaling. If you prefer, you can instead choose to use the individual service consoles, Auto Scaling API, or Application Auto Scaling API to scale individual AWS services. You should also use the individual consoles or API if you want to setup step scaling policies or scheduled scaling, as AWS Auto Scaling creates target tracking scaling policies only.\n\nPredictive Scaling is a feature of AWS Auto Scaling that looks at historic traffic patterns and forecasts them into the future to schedule changes in the number of EC2 instances at the appropriate times going forward. Predictive Scaling uses machine learning models to forecast daily and weekly patterns.\nAuto Scaling enhanced with Predictive Scaling delivers faster, simpler, and more accurate capacity provisioning resulting in lower cost and more responsive applications. By predicting traffic changes, Predictive Scaling provisions EC2 instances in advance of changing traffic, making Auto Scaling faster and more accurate.\n\n", "question": "What is the advantage of using AWS Auto Scaling over EC2 Auto Scaling for managing scaling across multiple resources?", "answer": "AWS Auto Scaling lets you define dynamic scaling policies for multiple resources across multiple services, while EC2 Auto Scaling is only for managing Amazon EC2 Auto Scaling groups. AWS Auto Scaling includes predefined scaling strategies that simplify the setup of scaling policies, and it offers predictive scaling for EC2 resources.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "autoscaling-faq-3", "model": "mistral-7b", "chunk": "You should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling lets you define dynamic scaling policies for multiple EC2 Auto Scaling groups or other resources using predefined scaling strategies. Using AWS Auto Scaling to configure scaling policies for all of the scalable resources in your application is faster than managing scaling policies for each resource via its individual service console. It’s also easier, as AWS Auto Scaling includes predefined scaling strategies that simplify the setup of scaling policies. You should also use AWS Auto Scaling if you want to create predictive scaling for EC2 resources.\nYou should use EC2 Auto Scaling if you only need to scale Amazon EC2 Auto Scaling groups, or if you are only interested in maintaining the health of your EC2 fleet. You should also use EC2 Auto Scaling if you need to create or configure Amazon EC2 Auto Scaling groups, or if you need to set up scheduled or step scaling policies (as AWS Auto Scaling supports only target tracking scaling policies).\nEC2 Auto Scaling groups must be created and configured outside of AWS Auto Scaling, such as through the EC2 console, Auto Scaling API or via CloudFormation. AWS Auto Scaling can help you configure dynamic scaling policies for your existing EC2 Auto Scaling groups.\n\nYou should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling enables unified scaling for multiple resources, and has predefined guidance that helps make it easier and faster to configure scaling. If you prefer, you can instead choose to use the individual service consoles, Auto Scaling API, or Application Auto Scaling API to scale individual AWS services. You should also use the individual consoles or API if you want to setup step scaling policies or scheduled scaling, as AWS Auto Scaling creates target tracking scaling policies only.\n\nPredictive Scaling is a feature of AWS Auto Scaling that looks at historic traffic patterns and forecasts them into the future to schedule changes in the number of EC2 instances at the appropriate times going forward. Predictive Scaling uses machine learning models to forecast daily and weekly patterns.\nAuto Scaling enhanced with Predictive Scaling delivers faster, simpler, and more accurate capacity provisioning resulting in lower cost and more responsive applications. By predicting traffic changes, Predictive Scaling provisions EC2 instances in advance of changing traffic, making Auto Scaling faster and more accurate.\n\n", "question": "How does predictive scaling work in AWS Auto Scaling?", "answer": "Predictive scaling is a feature of AWS Auto Scaling that uses machine learning models to forecast daily and weekly traffic patterns and schedules changes in the number of EC2 instances accordingly.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "autoscaling-faq-3", "model": "mistral-7b", "chunk": "You should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling lets you define dynamic scaling policies for multiple EC2 Auto Scaling groups or other resources using predefined scaling strategies. Using AWS Auto Scaling to configure scaling policies for all of the scalable resources in your application is faster than managing scaling policies for each resource via its individual service console. It’s also easier, as AWS Auto Scaling includes predefined scaling strategies that simplify the setup of scaling policies. You should also use AWS Auto Scaling if you want to create predictive scaling for EC2 resources.\nYou should use EC2 Auto Scaling if you only need to scale Amazon EC2 Auto Scaling groups, or if you are only interested in maintaining the health of your EC2 fleet. You should also use EC2 Auto Scaling if you need to create or configure Amazon EC2 Auto Scaling groups, or if you need to set up scheduled or step scaling policies (as AWS Auto Scaling supports only target tracking scaling policies).\nEC2 Auto Scaling groups must be created and configured outside of AWS Auto Scaling, such as through the EC2 console, Auto Scaling API or via CloudFormation. AWS Auto Scaling can help you configure dynamic scaling policies for your existing EC2 Auto Scaling groups.\n\nYou should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling enables unified scaling for multiple resources, and has predefined guidance that helps make it easier and faster to configure scaling. If you prefer, you can instead choose to use the individual service consoles, Auto Scaling API, or Application Auto Scaling API to scale individual AWS services. You should also use the individual consoles or API if you want to setup step scaling policies or scheduled scaling, as AWS Auto Scaling creates target tracking scaling policies only.\n\nPredictive Scaling is a feature of AWS Auto Scaling that looks at historic traffic patterns and forecasts them into the future to schedule changes in the number of EC2 instances at the appropriate times going forward. Predictive Scaling uses machine learning models to forecast daily and weekly patterns.\nAuto Scaling enhanced with Predictive Scaling delivers faster, simpler, and more accurate capacity provisioning resulting in lower cost and more responsive applications. By predicting traffic changes, Predictive Scaling provisions EC2 instances in advance of changing traffic, making Auto Scaling faster and more accurate.\n\n", "question": "What is the difference between AWS Auto Scaling and EC2 Auto Scaling in terms of scaling policies?", "answer": "AWS Auto Scaling supports target tracking scaling policies, while EC2 Auto Scaling supports step scaling policies and scheduled scaling policies.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "aws-faq-3", "model": "mistral-7b", "chunk": "FTPS and SFTP can both be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer a secure tunnel for transmission of commands and data. SFTP is a newer protocol and uses a single channel for commands and data, requiring fewer port openings than FTPS.\n\nSFTP, FTPS, and AS2 can all be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer secure transmission of data. Aside from support for encrypted and signed messages, AS2’s built in mechanism for Message Disposition Notification (MDN) alerts the sender that the message has been successfully received and decrypted by the recipient. This provides proof to the sender that their message was delivered without being tampered in transit. Use of AS2 is prevalent in workflows operating in retail, e-commerce, payments, supply chain for interacting with business partners who are also able to use AS2 to transact messages so that it is securely transmitted and delivered. AS2 provides you with options to ensure identity of the sender and receiver, integrity of the message, and confirm whether the message was successfully delivered and decrypted by the receiver.\n\nYes. Any existing file transfer client application will continue to work as long as you have enabled your endpoint for the chosen protocols. Examples of commonly used SFTP/FTPS/FTP clients include WinSCP, FileZilla, CyberDuck, lftp, and OpenSSH clients.\n\nYou can use AWS Transfer SFTP connectors to access files stored on external SFTP sites. Refer to\nSFTP connectors documentation\nto get started with SFTP connectors\n\nYou can use AWS Transfer Family’s fully managed SFTP/FTPS/AS2 capabilities to receive EDI documents that are generated from your trading partner’s business systems. EDI documents received using AWS Transfer Family’s connectivity capabilities are automatically uploaded to Amazon S3 where they can then be transformed into JSON and XML formatted outputs using AWS B2B Data Interchange. Alternatively, you can use any other any other EDI connectivity tool to upload EDI documents to S3.\n\n", "question": "What is the main difference between SFTP and FTPS in terms of port openings?", "answer": "SFTP uses a single channel for commands and data, requiring fewer port openings than FTPS which uses separate channels for control and data.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "aws-faq-3", "model": "mistral-7b", "chunk": "FTPS and SFTP can both be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer a secure tunnel for transmission of commands and data. SFTP is a newer protocol and uses a single channel for commands and data, requiring fewer port openings than FTPS.\n\nSFTP, FTPS, and AS2 can all be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer secure transmission of data. Aside from support for encrypted and signed messages, AS2’s built in mechanism for Message Disposition Notification (MDN) alerts the sender that the message has been successfully received and decrypted by the recipient. This provides proof to the sender that their message was delivered without being tampered in transit. Use of AS2 is prevalent in workflows operating in retail, e-commerce, payments, supply chain for interacting with business partners who are also able to use AS2 to transact messages so that it is securely transmitted and delivered. AS2 provides you with options to ensure identity of the sender and receiver, integrity of the message, and confirm whether the message was successfully delivered and decrypted by the receiver.\n\nYes. Any existing file transfer client application will continue to work as long as you have enabled your endpoint for the chosen protocols. Examples of commonly used SFTP/FTPS/FTP clients include WinSCP, FileZilla, CyberDuck, lftp, and OpenSSH clients.\n\nYou can use AWS Transfer SFTP connectors to access files stored on external SFTP sites. Refer to\nSFTP connectors documentation\nto get started with SFTP connectors\n\nYou can use AWS Transfer Family’s fully managed SFTP/FTPS/AS2 capabilities to receive EDI documents that are generated from your trading partner’s business systems. EDI documents received using AWS Transfer Family’s connectivity capabilities are automatically uploaded to Amazon S3 where they can then be transformed into JSON and XML formatted outputs using AWS B2B Data Interchange. Alternatively, you can use any other any other EDI connectivity tool to upload EDI documents to S3.\n\n", "question": "Why is AS2 preferred in workflows for retail, e-commerce, payments, and supply chain?", "answer": "AS2 is preferred in workflows for retail, e-commerce, payments, and supply chain because it provides proof to the sender that their message was delivered without being tampered in transit through its built-in Message Disposition Notification (MDN) feature.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "aws-faq-3", "model": "mistral-7b", "chunk": "FTPS and SFTP can both be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer a secure tunnel for transmission of commands and data. SFTP is a newer protocol and uses a single channel for commands and data, requiring fewer port openings than FTPS.\n\nSFTP, FTPS, and AS2 can all be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer secure transmission of data. Aside from support for encrypted and signed messages, AS2’s built in mechanism for Message Disposition Notification (MDN) alerts the sender that the message has been successfully received and decrypted by the recipient. This provides proof to the sender that their message was delivered without being tampered in transit. Use of AS2 is prevalent in workflows operating in retail, e-commerce, payments, supply chain for interacting with business partners who are also able to use AS2 to transact messages so that it is securely transmitted and delivered. AS2 provides you with options to ensure identity of the sender and receiver, integrity of the message, and confirm whether the message was successfully delivered and decrypted by the receiver.\n\nYes. Any existing file transfer client application will continue to work as long as you have enabled your endpoint for the chosen protocols. Examples of commonly used SFTP/FTPS/FTP clients include WinSCP, FileZilla, CyberDuck, lftp, and OpenSSH clients.\n\nYou can use AWS Transfer SFTP connectors to access files stored on external SFTP sites. Refer to\nSFTP connectors documentation\nto get started with SFTP connectors\n\nYou can use AWS Transfer Family’s fully managed SFTP/FTPS/AS2 capabilities to receive EDI documents that are generated from your trading partner’s business systems. EDI documents received using AWS Transfer Family’s connectivity capabilities are automatically uploaded to Amazon S3 where they can then be transformed into JSON and XML formatted outputs using AWS B2B Data Interchange. Alternatively, you can use any other any other EDI connectivity tool to upload EDI documents to S3.\n\n", "question": "What protocols can be used with AWS Transfer Family for secure file transfers?", "answer": "AWS Transfer Family supports SFTP, FTPS, and AS2 protocols for secure file transfers.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "backup-faq-2", "model": "mistral-7b", "chunk": "Yes. Backups created using services with existing backup capabilities, such as EBS Snapshots, can be accessed using AWS Backup. Similarly, backups created by AWS Backup can be accessed using the source service.\n\nAWS services offer backup features to protect your data, such as Amazon S3 Replication, Amazon EBS Snapshots, Amazon RDS snapshots, Amazon FSx backups, Amazon DynamoDB backups, and AWS Storage Gateway snapshots. All existing per-service backup capabilities remain unchanged. AWS Backup provides a common way to manage backups across AWS services both on AWS and on premises. AWS Backup is a centralized service that offers backup scheduling, retention management, and backup monitoring. AWS Backup supports existing backup functionality provided by S3, EBS, RDS, Amazon FSx, DynamoDB, and Storage Gateway. For AWS services with backup functionality built on AWS Backup, such as Amazon EFS and DynamoDB, AWS Backup provides backup management capabilities. Additional features include lifecycle policies to transition backups to a low-cost storage tier, backup storage and encryption independent from its source data, and backup access policies.\n\nAmazon Data Lifecycle Manager policies and backup plans created in AWS Backup work independently from each other and provide two ways to manage EBS snapshots. Amazon Data Lifecycle Manager provides a streamlined way to manage the lifecycle of EBS resources, such as volume snapshots. Use Amazon Data Lifecycle Manager when you want to automate the creation, retention, and deletion of EBS snapshots. Use AWS Backup to manage and monitor backups across the AWS services you use, including EBS volumes, from a single place.\n\n", "question": "Which AWS services offer backup features?", "answer": "Amazon S3 Replication, Amazon EBS Snapshots, Amazon RDS snapshots, Amazon FSx backups, Amazon DynamoDB backups, and AWS Storage Gateway snapshots are some AWS services that offer backup features.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "backup-faq-2", "model": "mistral-7b", "chunk": "Yes. Backups created using services with existing backup capabilities, such as EBS Snapshots, can be accessed using AWS Backup. Similarly, backups created by AWS Backup can be accessed using the source service.\n\nAWS services offer backup features to protect your data, such as Amazon S3 Replication, Amazon EBS Snapshots, Amazon RDS snapshots, Amazon FSx backups, Amazon DynamoDB backups, and AWS Storage Gateway snapshots. All existing per-service backup capabilities remain unchanged. AWS Backup provides a common way to manage backups across AWS services both on AWS and on premises. AWS Backup is a centralized service that offers backup scheduling, retention management, and backup monitoring. AWS Backup supports existing backup functionality provided by S3, EBS, RDS, Amazon FSx, DynamoDB, and Storage Gateway. For AWS services with backup functionality built on AWS Backup, such as Amazon EFS and DynamoDB, AWS Backup provides backup management capabilities. Additional features include lifecycle policies to transition backups to a low-cost storage tier, backup storage and encryption independent from its source data, and backup access policies.\n\nAmazon Data Lifecycle Manager policies and backup plans created in AWS Backup work independently from each other and provide two ways to manage EBS snapshots. Amazon Data Lifecycle Manager provides a streamlined way to manage the lifecycle of EBS resources, such as volume snapshots. Use Amazon Data Lifecycle Manager when you want to automate the creation, retention, and deletion of EBS snapshots. Use AWS Backup to manage and monitor backups across the AWS services you use, including EBS volumes, from a single place.\n\n", "question": "How does AWS Backup manage backups across services?", "answer": "AWS Backup is a centralized service that offers backup scheduling, retention management, and backup monitoring. It allows users to manage backups across AWS services both on AWS and on-premises.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "backup-faq-2", "model": "mistral-7b", "chunk": "Yes. Backups created using services with existing backup capabilities, such as EBS Snapshots, can be accessed using AWS Backup. Similarly, backups created by AWS Backup can be accessed using the source service.\n\nAWS services offer backup features to protect your data, such as Amazon S3 Replication, Amazon EBS Snapshots, Amazon RDS snapshots, Amazon FSx backups, Amazon DynamoDB backups, and AWS Storage Gateway snapshots. All existing per-service backup capabilities remain unchanged. AWS Backup provides a common way to manage backups across AWS services both on AWS and on premises. AWS Backup is a centralized service that offers backup scheduling, retention management, and backup monitoring. AWS Backup supports existing backup functionality provided by S3, EBS, RDS, Amazon FSx, DynamoDB, and Storage Gateway. For AWS services with backup functionality built on AWS Backup, such as Amazon EFS and DynamoDB, AWS Backup provides backup management capabilities. Additional features include lifecycle policies to transition backups to a low-cost storage tier, backup storage and encryption independent from its source data, and backup access policies.\n\nAmazon Data Lifecycle Manager policies and backup plans created in AWS Backup work independently from each other and provide two ways to manage EBS snapshots. Amazon Data Lifecycle Manager provides a streamlined way to manage the lifecycle of EBS resources, such as volume snapshots. Use Amazon Data Lifecycle Manager when you want to automate the creation, retention, and deletion of EBS snapshots. Use AWS Backup to manage and monitor backups across the AWS services you use, including EBS volumes, from a single place.\n\n", "question": "What is the difference between using Amazon Data Lifecycle Manager and AWS Backup for managing EBS snapshots?", "answer": "Amazon Data Lifecycle Manager provides a streamlined way to manage the lifecycle of EBS resources, such as volume snapshots. It automates the creation, retention, and deletion of EBS snapshots. AWS Backup, on the other hand, is used to manage and monitor backups across the AWS services you use, including EBS volumes, from a single place.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "batch-faq-2", "model": "mistral-7b", "chunk": "Yes. You can set Fargate CE’s to have a max vCPU, which is the total amount of vCPU of all the jobs currently running in that CE. When your vCPU count hits max vCPU in a CE, Batch will begin scheduling jobs on the next Fargate CE in order attached to the queue, if there is one. This is useful if, for example, you want to set a Fargate CE to some minimum business requirement, then run the rest of your workload on Fargate Spot.\nWhile setting a Fargate Spot CE as first, followed by a Fargate CE, Batch will only spill over into Fargate when the vCPU used by your jobs is greater than max vCPU for that CE. In the event that Fargate Spot is reclaimed, max vCPU will not be met and Batch will not request Fargate resources in the subsequent CE to run your jobs.\nConnecting an AWS Batch job queue to Fargate/Fargate Spot CE and an EC2 or Spot CE is not allowed.\n\nAWS Batch handles job execution and compute resource management, allowing you to focus on developing applications or analyzing results instead of setting up and managing infrastructure. If you are considering running or moving batch workloads to AWS, you should consider using AWS Batch.\n\nAWS Batch is optimized for batch computing and applications that scale through the execution of multiple jobs in parallel. Deep learning, genomics analysis, financial risk models, Monte Carlo simulations, animation rendering, media transcoding, image processing, and engineering simulations are all excellent examples of batch computing applications.\n\n", "question": "What is the maximum vCPU limit for a Fargate CE in AWS Batch?", "answer": "The maximum vCPU limit for a Fargate CE in AWS Batch is the total amount of vCPU of all the jobs currently running in that CE.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "batch-faq-2", "model": "mistral-7b", "chunk": "Yes. You can set Fargate CE’s to have a max vCPU, which is the total amount of vCPU of all the jobs currently running in that CE. When your vCPU count hits max vCPU in a CE, Batch will begin scheduling jobs on the next Fargate CE in order attached to the queue, if there is one. This is useful if, for example, you want to set a Fargate CE to some minimum business requirement, then run the rest of your workload on Fargate Spot.\nWhile setting a Fargate Spot CE as first, followed by a Fargate CE, Batch will only spill over into Fargate when the vCPU used by your jobs is greater than max vCPU for that CE. In the event that Fargate Spot is reclaimed, max vCPU will not be met and Batch will not request Fargate resources in the subsequent CE to run your jobs.\nConnecting an AWS Batch job queue to Fargate/Fargate Spot CE and an EC2 or Spot CE is not allowed.\n\nAWS Batch handles job execution and compute resource management, allowing you to focus on developing applications or analyzing results instead of setting up and managing infrastructure. If you are considering running or moving batch workloads to AWS, you should consider using AWS Batch.\n\nAWS Batch is optimized for batch computing and applications that scale through the execution of multiple jobs in parallel. Deep learning, genomics analysis, financial risk models, Monte Carlo simulations, animation rendering, media transcoding, image processing, and engineering simulations are all excellent examples of batch computing applications.\n\n", "question": "Why does AWS Batch use multiple Fargate CEs for job scheduling?", "answer": "AWS Batch uses multiple Fargate CEs for job scheduling in order to meet the vCPU requirement for running jobs when the vCPU limit is exceeded in the current CE. This allows users to set a minimum business requirement for a Fargate CE and run the rest of their workload on Fargate Spot.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "batch-faq-2", "model": "mistral-7b", "chunk": "Yes. You can set Fargate CE’s to have a max vCPU, which is the total amount of vCPU of all the jobs currently running in that CE. When your vCPU count hits max vCPU in a CE, Batch will begin scheduling jobs on the next Fargate CE in order attached to the queue, if there is one. This is useful if, for example, you want to set a Fargate CE to some minimum business requirement, then run the rest of your workload on Fargate Spot.\nWhile setting a Fargate Spot CE as first, followed by a Fargate CE, Batch will only spill over into Fargate when the vCPU used by your jobs is greater than max vCPU for that CE. In the event that Fargate Spot is reclaimed, max vCPU will not be met and Batch will not request Fargate resources in the subsequent CE to run your jobs.\nConnecting an AWS Batch job queue to Fargate/Fargate Spot CE and an EC2 or Spot CE is not allowed.\n\nAWS Batch handles job execution and compute resource management, allowing you to focus on developing applications or analyzing results instead of setting up and managing infrastructure. If you are considering running or moving batch workloads to AWS, you should consider using AWS Batch.\n\nAWS Batch is optimized for batch computing and applications that scale through the execution of multiple jobs in parallel. Deep learning, genomics analysis, financial risk models, Monte Carlo simulations, animation rendering, media transcoding, image processing, and engineering simulations are all excellent examples of batch computing applications.\n\n", "question": "How does Fargate CE scheduling compare between on-demand and spot instances in AWS Batch?", "answer": "Fargate CE scheduling spills over to Fargate instances only when the vCPU used by jobs is greater than the max vCPU limit for the current CE with on-demand instances. However, with Fargate Spot instances, the max vCPU limit is not met in the event that the spot instances are reclaimed, and Batch will not request Fargate resources in the subsequent CE to run jobs.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "bedrock-faq-2", "model": "mistral-7b", "chunk": "Data security and compliance certifications: Amazon Bedrock offers several capabilities to support security and privacy requirements. Amazon Bedrock is in scope for common compliance standards such as Service and Organization Control (SOC), International Organization for Standardization (ISO), is Health Insurance Portability and Accountability Act (HIPAA) eligible, and customers can use Amazon Bedrock in compliance with the General Data Protection Regulation (GDPR). Amazon Bedrock is CSA Security Trust Assurance and Risk (STAR) Level 2 certified, which validates the use of best practices and the security posture of AWS cloud offerings. With Amazon Bedrock, your content is not used to improve the base models and is not shared with any model providers. Your data in Amazon Bedrock is always encrypted in transit and at rest, and you can optionally encrypt the data using your own keys. You can use AWS PrivateLink with Amazon Bedrock to establish private connectivity between your FMs and your Amazon Virtual Private Cloud (Amazon VPC) without exposing your traffic to the Internet.\n\n", "question": "Which compliance standards is Amazon Bedrock in scope for?", "answer": "Amazon Bedrock is in scope for common compliance standards such as Service and Organization Control (SOC), International Organization for Standardization (ISO), is Health Insurance Portability and Accountability Act (HIPAA) eligible, and can be used in compliance with the General Data Protection Regulation (GDPR).", "question_type": "", "original_pair_id": ""}
{"chunk_id": "bedrock-faq-2", "model": "mistral-7b", "chunk": "Data security and compliance certifications: Amazon Bedrock offers several capabilities to support security and privacy requirements. Amazon Bedrock is in scope for common compliance standards such as Service and Organization Control (SOC), International Organization for Standardization (ISO), is Health Insurance Portability and Accountability Act (HIPAA) eligible, and customers can use Amazon Bedrock in compliance with the General Data Protection Regulation (GDPR). Amazon Bedrock is CSA Security Trust Assurance and Risk (STAR) Level 2 certified, which validates the use of best practices and the security posture of AWS cloud offerings. With Amazon Bedrock, your content is not used to improve the base models and is not shared with any model providers. Your data in Amazon Bedrock is always encrypted in transit and at rest, and you can optionally encrypt the data using your own keys. You can use AWS PrivateLink with Amazon Bedrock to establish private connectivity between your FMs and your Amazon Virtual Private Cloud (Amazon VPC) without exposing your traffic to the Internet.\n\n", "question": "How does Amazon Bedrock help with data security and privacy?", "answer": "Amazon Bedrock helps with data security and privacy by offering several capabilities, such as being in scope for various compliance standards, encrypting data in transit and at rest, allowing the use of your own encryption keys, and providing private connectivity using AWS PrivateLink.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "bedrock-faq-2", "model": "mistral-7b", "chunk": "Data security and compliance certifications: Amazon Bedrock offers several capabilities to support security and privacy requirements. Amazon Bedrock is in scope for common compliance standards such as Service and Organization Control (SOC), International Organization for Standardization (ISO), is Health Insurance Portability and Accountability Act (HIPAA) eligible, and customers can use Amazon Bedrock in compliance with the General Data Protection Regulation (GDPR). Amazon Bedrock is CSA Security Trust Assurance and Risk (STAR) Level 2 certified, which validates the use of best practices and the security posture of AWS cloud offerings. With Amazon Bedrock, your content is not used to improve the base models and is not shared with any model providers. Your data in Amazon Bedrock is always encrypted in transit and at rest, and you can optionally encrypt the data using your own keys. You can use AWS PrivateLink with Amazon Bedrock to establish private connectivity between your FMs and your Amazon Virtual Private Cloud (Amazon VPC) without exposing your traffic to the Internet.\n\n", "question": "How does the security of Amazon Bedrock compare to other AWS offerings?", "answer": "Amazon Bedrock is CSA Security Trust Assurance and Risk (STAR) Level 2 certified, which validates the use of best practices and the security posture of AWS cloud offerings. However, the text passage doesn't provide enough information to compare Amazon Bedrock specifically to other AWS offerings.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "certificate-faq-3", "model": "mistral-7b", "chunk": "ACM enables you to manage the lifecycle of your public and private certificates. ACM’s capabilities depend on whether the certificate is public or private, how you obtain the certificate, and where you deploy it.\nPublic certificates\n- You can request Amazon-issued public certificates in ACM. ACM manages the renewal and deployment of public certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway.\nExportable public certificates\n- ACM manages the renewal of exported public certificates that you may deploy with AWS services and/or in your on-premises environments.\nPrivate certificates\n– You can choose to delegate private certificate management to ACM. When used in this way, ACM can automatically renew and deploy private certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway. You can easily deploy these private certificates using the AWS Management console, APIs, and command-line interface (CLI). You can export private certificates from ACM and use them with EC2 instances, containers, on-premises servers, and IoT devices. AWS Private CA automatically renews these certificates and sends an Amazon CloudWatch notification when the renewal is completed. You can write client-side code to download renewed certificates and private keys and deploy them with your application.\nImported certificates\n– If you want to use a third-party certificate with Amazon CloudFront, Elastic Load Balancing, or Amazon API Gateway, you may import it into ACM using the AWS Management Console, AWS CLI, or ACM APIs. ACM can not renew imported certificates, but it can help you manage the renewal process. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire. You can use\nACM CloudWatch metrics\nto monitor the expiration dates of an imported certificates and import a new third-party certificate to replace an expiring one.\n\n", "question": "What services can you use Amazon Certificate Manager (ACM) public certificates with?", "answer": "Amazon Certificate Manager (ACM) public certificates can be used with Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "certificate-faq-3", "model": "mistral-7b", "chunk": "ACM enables you to manage the lifecycle of your public and private certificates. ACM’s capabilities depend on whether the certificate is public or private, how you obtain the certificate, and where you deploy it.\nPublic certificates\n- You can request Amazon-issued public certificates in ACM. ACM manages the renewal and deployment of public certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway.\nExportable public certificates\n- ACM manages the renewal of exported public certificates that you may deploy with AWS services and/or in your on-premises environments.\nPrivate certificates\n– You can choose to delegate private certificate management to ACM. When used in this way, ACM can automatically renew and deploy private certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway. You can easily deploy these private certificates using the AWS Management console, APIs, and command-line interface (CLI). You can export private certificates from ACM and use them with EC2 instances, containers, on-premises servers, and IoT devices. AWS Private CA automatically renews these certificates and sends an Amazon CloudWatch notification when the renewal is completed. You can write client-side code to download renewed certificates and private keys and deploy them with your application.\nImported certificates\n– If you want to use a third-party certificate with Amazon CloudFront, Elastic Load Balancing, or Amazon API Gateway, you may import it into ACM using the AWS Management Console, AWS CLI, or ACM APIs. ACM can not renew imported certificates, but it can help you manage the renewal process. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire. You can use\nACM CloudWatch metrics\nto monitor the expiration dates of an imported certificates and import a new third-party certificate to replace an expiring one.\n\n", "question": "How does ACM manage public and private certificates?", "answer": "ACM manages the renewal and deployment of public certificates used with ACM-integrated services, and allows you to manage the renewal and deployment of private certificates used with the same services. For exported public certificates and imported certificates, ACM assists in managing the renewal process.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "certificate-faq-3", "model": "mistral-7b", "chunk": "ACM enables you to manage the lifecycle of your public and private certificates. ACM’s capabilities depend on whether the certificate is public or private, how you obtain the certificate, and where you deploy it.\nPublic certificates\n- You can request Amazon-issued public certificates in ACM. ACM manages the renewal and deployment of public certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway.\nExportable public certificates\n- ACM manages the renewal of exported public certificates that you may deploy with AWS services and/or in your on-premises environments.\nPrivate certificates\n– You can choose to delegate private certificate management to ACM. When used in this way, ACM can automatically renew and deploy private certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway. You can easily deploy these private certificates using the AWS Management console, APIs, and command-line interface (CLI). You can export private certificates from ACM and use them with EC2 instances, containers, on-premises servers, and IoT devices. AWS Private CA automatically renews these certificates and sends an Amazon CloudWatch notification when the renewal is completed. You can write client-side code to download renewed certificates and private keys and deploy them with your application.\nImported certificates\n– If you want to use a third-party certificate with Amazon CloudFront, Elastic Load Balancing, or Amazon API Gateway, you may import it into ACM using the AWS Management Console, AWS CLI, or ACM APIs. ACM can not renew imported certificates, but it can help you manage the renewal process. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire. You can use\nACM CloudWatch metrics\nto monitor the expiration dates of an imported certificates and import a new third-party certificate to replace an expiring one.\n\n", "question": "What's the difference in certificate management between imported and Amazon-issued certificates in ACM?", "answer": "Amazon-issued certificates are managed by ACM for renewal and deployment with ACM-integrated services. For imported certificates, ACM assists in the renewal process but does not manage certificate renewal or deployment.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "chatbot-faq-2", "model": "mistral-7b", "chunk": "You can provision Microsoft Teams and Slack channel configurations using AWS CLI, AWS CloudFormation, AWS Cloud Control APIs, and SDKs. Terraform users can use AWS provider to manage Chatbot channel configurations.\n\nAWS Chatbot is a global service and can be used in all commercial AWS Regions. You can combine Amazon SNS topics from multiple Regions in a single AWS Chatbot configuration. Visit the\nAWS Regional Product and Services table\nfor details about availability.\n\nAWS Chatbot is a global service and we may store or process customer information, such as Chatbot configurations and permissions, Microsoft Teams team identifiers, Slack workspace identifiers and channel names, notifications, user inputs, and AWS Chatbot generated responses and images, in any of the commercial AWS Regions.\nYou can request deletion of data used for analytics and to improve the quality of service associated with your account by contacting AWS Support. Your trust, your privacy, and the security of your data are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your data and ensure that our use complies with our commitments to you. See\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\nWhen you opt out of use of your data to improve and develop the quality of AWS Chatbot and other Amazon machine-learning/artificial-intelligence technologies, your data will be removed from all AWS Regions. For information about how to opt out, contact AWS Support.\n\nYou can customize AWS Chatbot to suit your ChatOps use cases. You can designate different channels to monitor and operate different aspects of your cloud applications. You can operate resources across multiple accounts and regions from a channel. With IAM-based permissions, guarails, and Service Control Policies (SCPs), you can decide the type of the actions channel members can take from chat channels.\nYou can also send custom notifications so that you are informed about the state of your resources and applications. You can also customize action buttons on notifications and configure command aliases so that you can quickly run commands to dignose and remediate issues.\n\n", "question": "Which services can be used to provision Microsoft Teams and Slack channel configurations in AWS?", "answer": "You can use AWS CLI, AWS CloudFormation, AWS Cloud Control APIs, and SDKs to provision Microsoft Teams and Slack channel configurations in AWS.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "chatbot-faq-2", "model": "mistral-7b", "chunk": "You can provision Microsoft Teams and Slack channel configurations using AWS CLI, AWS CloudFormation, AWS Cloud Control APIs, and SDKs. Terraform users can use AWS provider to manage Chatbot channel configurations.\n\nAWS Chatbot is a global service and can be used in all commercial AWS Regions. You can combine Amazon SNS topics from multiple Regions in a single AWS Chatbot configuration. Visit the\nAWS Regional Product and Services table\nfor details about availability.\n\nAWS Chatbot is a global service and we may store or process customer information, such as Chatbot configurations and permissions, Microsoft Teams team identifiers, Slack workspace identifiers and channel names, notifications, user inputs, and AWS Chatbot generated responses and images, in any of the commercial AWS Regions.\nYou can request deletion of data used for analytics and to improve the quality of service associated with your account by contacting AWS Support. Your trust, your privacy, and the security of your data are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your data and ensure that our use complies with our commitments to you. See\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\nWhen you opt out of use of your data to improve and develop the quality of AWS Chatbot and other Amazon machine-learning/artificial-intelligence technologies, your data will be removed from all AWS Regions. For information about how to opt out, contact AWS Support.\n\nYou can customize AWS Chatbot to suit your ChatOps use cases. You can designate different channels to monitor and operate different aspects of your cloud applications. You can operate resources across multiple accounts and regions from a channel. With IAM-based permissions, guarails, and Service Control Policies (SCPs), you can decide the type of the actions channel members can take from chat channels.\nYou can also send custom notifications so that you are informed about the state of your resources and applications. You can also customize action buttons on notifications and configure command aliases so that you can quickly run commands to dignose and remediate issues.\n\n", "question": "How can AWS Chatbot be used across multiple regions and accounts?", "answer": "AWS Chatbot allows you to operate resources across multiple accounts and regions from a single channel. You can also designate different channels to monitor and operate different aspects of your cloud applications.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "chatbot-faq-2", "model": "mistral-7b", "chunk": "You can provision Microsoft Teams and Slack channel configurations using AWS CLI, AWS CloudFormation, AWS Cloud Control APIs, and SDKs. Terraform users can use AWS provider to manage Chatbot channel configurations.\n\nAWS Chatbot is a global service and can be used in all commercial AWS Regions. You can combine Amazon SNS topics from multiple Regions in a single AWS Chatbot configuration. Visit the\nAWS Regional Product and Services table\nfor details about availability.\n\nAWS Chatbot is a global service and we may store or process customer information, such as Chatbot configurations and permissions, Microsoft Teams team identifiers, Slack workspace identifiers and channel names, notifications, user inputs, and AWS Chatbot generated responses and images, in any of the commercial AWS Regions.\nYou can request deletion of data used for analytics and to improve the quality of service associated with your account by contacting AWS Support. Your trust, your privacy, and the security of your data are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your data and ensure that our use complies with our commitments to you. See\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\nWhen you opt out of use of your data to improve and develop the quality of AWS Chatbot and other Amazon machine-learning/artificial-intelligence technologies, your data will be removed from all AWS Regions. For information about how to opt out, contact AWS Support.\n\nYou can customize AWS Chatbot to suit your ChatOps use cases. You can designate different channels to monitor and operate different aspects of your cloud applications. You can operate resources across multiple accounts and regions from a channel. With IAM-based permissions, guarails, and Service Control Policies (SCPs), you can decide the type of the actions channel members can take from chat channels.\nYou can also send custom notifications so that you are informed about the state of your resources and applications. You can also customize action buttons on notifications and configure command aliases so that you can quickly run commands to dignose and remediate issues.\n\n", "question": "What are the differences between managing Chatbot channel configurations using AWS CLI and Terraform?", "answer": "AWS CLI and Terraform are two different tools for managing AWS resources. AWS CLI is a command-line tool for interacting with AWS services, while Terraform is a tool for infrastructure provisioning and management.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "cloud-faq-2", "model": "mistral-7b", "chunk": "We have coverage for many common AWS services and features with rich, high-level constructs, and complete coverage of the lower-level CloudFormation resources, including new resources shortly after they are available. We keep the AWS Construct Library up to date by autogenerating the resource-level APIs every time the CloudFormation specification changes. On top of these autogenerated APIs, we provide handcrafted, higher-level abstractions that make it even easier to work with each service. We do not have this high-level, convenient abstraction for every AWS service yet, but are adding new features all the time with a focus on services most used in modern cloud applications. Visit the\nAWS Construct Library API reference\nto learn more.\n\nWe keep the AWS Construct Library in sync with the CloudFormation resource specification by adding new CloudFormation features to AWS CDK shortly after their publication.\n\nAWS CDK construct libraries are just like any other library. They are consumed through the package manager of the programming language you use; keeping those libraries up to date is part of your normal workflow. All packages support semantic versioning, allowing you to make conscious choices about when to migrate to new infrastructure models.\n\nThink of the AWS CDK as a developer-centric toolkit leveraging the full power of modern programming languages to define your AWS infrastructure as code. When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection.\n\n", "question": "What does the AWS Construct Library provide for new CloudFormation resources?", "answer": "The AWS Construct Library provides autogenerated APIs for new CloudFormation resources and handcrafted, higher-level abstractions to make it easier to work with each service.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "cloud-faq-2", "model": "mistral-7b", "chunk": "We have coverage for many common AWS services and features with rich, high-level constructs, and complete coverage of the lower-level CloudFormation resources, including new resources shortly after they are available. We keep the AWS Construct Library up to date by autogenerating the resource-level APIs every time the CloudFormation specification changes. On top of these autogenerated APIs, we provide handcrafted, higher-level abstractions that make it even easier to work with each service. We do not have this high-level, convenient abstraction for every AWS service yet, but are adding new features all the time with a focus on services most used in modern cloud applications. Visit the\nAWS Construct Library API reference\nto learn more.\n\nWe keep the AWS Construct Library in sync with the CloudFormation resource specification by adding new CloudFormation features to AWS CDK shortly after their publication.\n\nAWS CDK construct libraries are just like any other library. They are consumed through the package manager of the programming language you use; keeping those libraries up to date is part of your normal workflow. All packages support semantic versioning, allowing you to make conscious choices about when to migrate to new infrastructure models.\n\nThink of the AWS CDK as a developer-centric toolkit leveraging the full power of modern programming languages to define your AWS infrastructure as code. When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection.\n\n", "question": "Why does the AWS Construct Library provide higher-level abstractions for AWS services?", "answer": "The AWS Construct Library provides higher-level abstractions for AWS services to make it even easier to work with each service.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "cloud-faq-2", "model": "mistral-7b", "chunk": "We have coverage for many common AWS services and features with rich, high-level constructs, and complete coverage of the lower-level CloudFormation resources, including new resources shortly after they are available. We keep the AWS Construct Library up to date by autogenerating the resource-level APIs every time the CloudFormation specification changes. On top of these autogenerated APIs, we provide handcrafted, higher-level abstractions that make it even easier to work with each service. We do not have this high-level, convenient abstraction for every AWS service yet, but are adding new features all the time with a focus on services most used in modern cloud applications. Visit the\nAWS Construct Library API reference\nto learn more.\n\nWe keep the AWS Construct Library in sync with the CloudFormation resource specification by adding new CloudFormation features to AWS CDK shortly after their publication.\n\nAWS CDK construct libraries are just like any other library. They are consumed through the package manager of the programming language you use; keeping those libraries up to date is part of your normal workflow. All packages support semantic versioning, allowing you to make conscious choices about when to migrate to new infrastructure models.\n\nThink of the AWS CDK as a developer-centric toolkit leveraging the full power of modern programming languages to define your AWS infrastructure as code. When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection.\n\n", "question": "How does the AWS Construct Library compare to using CloudFormation directly?", "answer": "The AWS Construct Library allows you to define your AWS infrastructure as code using modern programming languages and compiles down to CloudFormation JSON/YAML templates for provisioning. It provides autogenerated APIs and higher-level abstractions for easier usage.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "cloudformation-faq-1", "model": "mistral-7b", "chunk": "These services are designed to complement each other.\nAWS Elastic Beanstalk\nprovides an environment where you can easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for managing application lifecycle. If your application workloads can be managed as Elastic Beanstalk workloads, you can enjoy a more turn-key experience in creating and updating applications. Behind the scenes, Elastic Beanstalk uses CloudFormation to create and maintain resources. If your application requirements dictate more custom control, the additional functionality of CloudFormation gives you more options to control your workloads.\nAWS CloudFormation is a convenient provisioning mechanism for a broad range of\nAWS\nand third-party resources. It supports the infrastructure needs of many different types of applications such as existing enterprise applications, legacy applications, applications built using a variety of AWS resources, and container-based solutions (including those built using AWS Elastic Beanstalk).\nAWS CloudFormation supports Elastic Beanstalk application environments as one of the AWS resource types. This allows you, for example, to create and manage an AWS Elastic Beanstalk–hosted application along with an RDS database to store the application data. Any other supported AWS resource can be added to the group as well.\n\nCloudFormation introduces four concepts: A template is a JSON or YAML declarative code file that describes the intended state of all the resources you need to deploy your application. A stack implements and manages the group of resources outlined in your template, and allows the state and dependencies of those resources to be managed together. A change set is a preview of changes that will be executed by stack operations to create, update, or remove resources. A stack set is a group of stacks you manage together that can replicate a group.\n\nTo see a complete list of supported\nAWS resources\nand their features, visit the Supported AWS Services page in the Release History of the documentation.\nThe\nAWS CloudFormation Registry\nand AWS CloudFormation\ncustom resources\nenable management of additional AWS and third party resources.\n\n", "question": "What is the role of AWS Elastic Beanstalk in the context of AWS services?", "answer": "AWS Elastic Beanstalk is a service that provides an environment for deploying and running applications in the cloud, with a one-stop experience for managing application lifecycles. It uses CloudFormation behind the scenes for creating and maintaining resources.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "cloudformation-faq-1", "model": "mistral-7b", "chunk": "These services are designed to complement each other.\nAWS Elastic Beanstalk\nprovides an environment where you can easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for managing application lifecycle. If your application workloads can be managed as Elastic Beanstalk workloads, you can enjoy a more turn-key experience in creating and updating applications. Behind the scenes, Elastic Beanstalk uses CloudFormation to create and maintain resources. If your application requirements dictate more custom control, the additional functionality of CloudFormation gives you more options to control your workloads.\nAWS CloudFormation is a convenient provisioning mechanism for a broad range of\nAWS\nand third-party resources. It supports the infrastructure needs of many different types of applications such as existing enterprise applications, legacy applications, applications built using a variety of AWS resources, and container-based solutions (including those built using AWS Elastic Beanstalk).\nAWS CloudFormation supports Elastic Beanstalk application environments as one of the AWS resource types. This allows you, for example, to create and manage an AWS Elastic Beanstalk–hosted application along with an RDS database to store the application data. Any other supported AWS resource can be added to the group as well.\n\nCloudFormation introduces four concepts: A template is a JSON or YAML declarative code file that describes the intended state of all the resources you need to deploy your application. A stack implements and manages the group of resources outlined in your template, and allows the state and dependencies of those resources to be managed together. A change set is a preview of changes that will be executed by stack operations to create, update, or remove resources. A stack set is a group of stacks you manage together that can replicate a group.\n\nTo see a complete list of supported\nAWS resources\nand their features, visit the Supported AWS Services page in the Release History of the documentation.\nThe\nAWS CloudFormation Registry\nand AWS CloudFormation\ncustom resources\nenable management of additional AWS and third party resources.\n\n", "question": "How does AWS CloudFormation support AWS Elastic Beanstalk?", "answer": "AWS CloudFormation supports AWS Elastic Beanstalk by allowing users to create and manage Elastic Beanstalk application environments as one of the AWS resource types. This enables the creation and management of an Elastic Beanstalk-hosted application along with other resources.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "cloudformation-faq-1", "model": "mistral-7b", "chunk": "These services are designed to complement each other.\nAWS Elastic Beanstalk\nprovides an environment where you can easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for managing application lifecycle. If your application workloads can be managed as Elastic Beanstalk workloads, you can enjoy a more turn-key experience in creating and updating applications. Behind the scenes, Elastic Beanstalk uses CloudFormation to create and maintain resources. If your application requirements dictate more custom control, the additional functionality of CloudFormation gives you more options to control your workloads.\nAWS CloudFormation is a convenient provisioning mechanism for a broad range of\nAWS\nand third-party resources. It supports the infrastructure needs of many different types of applications such as existing enterprise applications, legacy applications, applications built using a variety of AWS resources, and container-based solutions (including those built using AWS Elastic Beanstalk).\nAWS CloudFormation supports Elastic Beanstalk application environments as one of the AWS resource types. This allows you, for example, to create and manage an AWS Elastic Beanstalk–hosted application along with an RDS database to store the application data. Any other supported AWS resource can be added to the group as well.\n\nCloudFormation introduces four concepts: A template is a JSON or YAML declarative code file that describes the intended state of all the resources you need to deploy your application. A stack implements and manages the group of resources outlined in your template, and allows the state and dependencies of those resources to be managed together. A change set is a preview of changes that will be executed by stack operations to create, update, or remove resources. A stack set is a group of stacks you manage together that can replicate a group.\n\nTo see a complete list of supported\nAWS resources\nand their features, visit the Supported AWS Services page in the Release History of the documentation.\nThe\nAWS CloudFormation Registry\nand AWS CloudFormation\ncustom resources\nenable management of additional AWS and third party resources.\n\n", "question": "What is the difference between AWS Elastic Beanstalk and AWS CloudFormation in terms of control and customization?", "answer": "AWS Elastic Beanstalk offers a more turn-key experience for creating and updating applications, while AWS CloudFormation provides more options for controlling and customizing workloads through the use of a template, stack, change set, and stack set. Elastic Beanstalk uses CloudFormation behind the scenes but does not offer the same level of control.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "amazon-faq-6", "model": "llama-3.1-8b", "chunk": "With Amazon MQ, you pay only for what you use. You are charged for the broker instance usage, storage usage, and standard data transfer fees.\nSee\nAmazon MQ pricing\nfor details.\nStarting July 15, 2025, new AWS customers will receive up to $200 in AWS Free Tier credits, which can be applied towards eligible AWS services, including Amazon MQ. At account sign-up, you can choose between a free plan and a paid plan. The free plan will be available for 6 months after account creation. If you upgrade to a paid plan, any remaining Free Tier credit balance will automatically apply to your AWS bills. All Free Tier credits must be used within 12 months of your account creation date. To learn more about the AWS Free Tier program, refer to\nAWS Free Tier website\nand\nAWS Free Tier documentation\n.\n\nNo, all inter-node data transfer is included with the service at no additional charge.\n\nNo, Amazon MQ does not offer reserved instance pricing.\n\nNo, each broker you provision includes boot volume storage managed by the Amazon MQ service.\n\nAmazon MQ is compliant or eligible for the following programs:\nHIPAA eligible\nPCI DSS compliant\nSOC 1,2,3 compliant\nISO 9001, 27001, 27017, and 27018 certified\nFor a complete list of AWS services and compliance programs, please see\nAWS Services in Scope by Compliance Program.\n\n", "question": "What are the charges for using Amazon MQ?", "answer": "You are charged for the broker instance usage, storage usage, and standard data transfer fees.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "amazon-faq-6", "model": "llama-3.1-8b", "chunk": "With Amazon MQ, you pay only for what you use. You are charged for the broker instance usage, storage usage, and standard data transfer fees.\nSee\nAmazon MQ pricing\nfor details.\nStarting July 15, 2025, new AWS customers will receive up to $200 in AWS Free Tier credits, which can be applied towards eligible AWS services, including Amazon MQ. At account sign-up, you can choose between a free plan and a paid plan. The free plan will be available for 6 months after account creation. If you upgrade to a paid plan, any remaining Free Tier credit balance will automatically apply to your AWS bills. All Free Tier credits must be used within 12 months of your account creation date. To learn more about the AWS Free Tier program, refer to\nAWS Free Tier website\nand\nAWS Free Tier documentation\n.\n\nNo, all inter-node data transfer is included with the service at no additional charge.\n\nNo, Amazon MQ does not offer reserved instance pricing.\n\nNo, each broker you provision includes boot volume storage managed by the Amazon MQ service.\n\nAmazon MQ is compliant or eligible for the following programs:\nHIPAA eligible\nPCI DSS compliant\nSOC 1,2,3 compliant\nISO 9001, 27001, 27017, and 27018 certified\nFor a complete list of AWS services and compliance programs, please see\nAWS Services in Scope by Compliance Program.\n\n", "question": "How do new AWS customers receive AWS Free Tier credits?", "answer": "New AWS customers will receive up to $200 in AWS Free Tier credits, which can be applied towards eligible AWS services, including Amazon MQ.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "amazon-faq-6", "model": "llama-3.1-8b", "chunk": "With Amazon MQ, you pay only for what you use. You are charged for the broker instance usage, storage usage, and standard data transfer fees.\nSee\nAmazon MQ pricing\nfor details.\nStarting July 15, 2025, new AWS customers will receive up to $200 in AWS Free Tier credits, which can be applied towards eligible AWS services, including Amazon MQ. At account sign-up, you can choose between a free plan and a paid plan. The free plan will be available for 6 months after account creation. If you upgrade to a paid plan, any remaining Free Tier credit balance will automatically apply to your AWS bills. All Free Tier credits must be used within 12 months of your account creation date. To learn more about the AWS Free Tier program, refer to\nAWS Free Tier website\nand\nAWS Free Tier documentation\n.\n\nNo, all inter-node data transfer is included with the service at no additional charge.\n\nNo, Amazon MQ does not offer reserved instance pricing.\n\nNo, each broker you provision includes boot volume storage managed by the Amazon MQ service.\n\nAmazon MQ is compliant or eligible for the following programs:\nHIPAA eligible\nPCI DSS compliant\nSOC 1,2,3 compliant\nISO 9001, 27001, 27017, and 27018 certified\nFor a complete list of AWS services and compliance programs, please see\nAWS Services in Scope by Compliance Program.\n\n", "question": "What is the difference between the free plan and paid plan for Amazon MQ?", "answer": "The free plan will be available for 6 months after account creation, while the paid plan will allow you to use any remaining Free Tier credit balance to your AWS bills.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "amplify-faq-7", "model": "llama-3.1-8b", "chunk": "Prices are the same across all regions.\n\n", "question": "Are prices uniform across all AWS regions?", "answer": "Yes, prices are the same across all regions.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "amplify-faq-7", "model": "llama-3.1-8b", "chunk": "Prices are the same across all regions.\n\n", "question": "How does Amazon ensure uniform pricing across regions?", "answer": "Unfortunately, the provided text does not address how Amazon ensures uniform pricing across regions.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "amplify-faq-7", "model": "llama-3.1-8b", "chunk": "Prices are the same across all regions.\n\n", "question": "Do Amazon S3 or Amazon Glacier storage prices differ across regions?", "answer": "There is no mention of specific storage services like Amazon S3 or Amazon Glacier in the provided text, so we cannot determine if their prices differ across regions.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "api-faq-3", "model": "llama-3.1-8b", "chunk": "Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs.\nHTTP API\n: HTTP APIs are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads. They do not currently offer API management functionality.\nREST API\n: REST APIs offer API proxy functionality and API management features in a single solution. REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs.\nWebSocket API:\nWebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. With WebSocket APIs in API Gateway, you can define backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint to be invoked when messages are received from the connected clients.\n\nTo get started with HTTP APIs, you can use the Amazon API Gateway console, the AWS CLI, AWS SDKs, or AWS CloudFormation. To learn more about getting started with HTTP APIs, visit our\ndocumentation\n.\n\nTo get started with REST APIs, you can use the Amazon API Gateway console, the AWS CLI, or AWS SDKs. To learn more about getting started with REST APIs, visit our\ndocumentation\n.\n\n", "question": "What are the two options to create RESTful APIs in Amazon API Gateway?", "answer": "HTTP APIs and REST APIs", "question_type": "", "original_pair_id": ""}
{"chunk_id": "api-faq-3", "model": "llama-3.1-8b", "chunk": "Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs.\nHTTP API\n: HTTP APIs are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads. They do not currently offer API management functionality.\nREST API\n: REST APIs offer API proxy functionality and API management features in a single solution. REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs.\nWebSocket API:\nWebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. With WebSocket APIs in API Gateway, you can define backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint to be invoked when messages are received from the connected clients.\n\nTo get started with HTTP APIs, you can use the Amazon API Gateway console, the AWS CLI, AWS SDKs, or AWS CloudFormation. To learn more about getting started with HTTP APIs, visit our\ndocumentation\n.\n\nTo get started with REST APIs, you can use the Amazon API Gateway console, the AWS CLI, or AWS SDKs. To learn more about getting started with REST APIs, visit our\ndocumentation\n.\n\n", "question": "How does Amazon API Gateway support real-time message communication?", "answer": "By maintaining a persistent connection between connected clients via WebSocket APIs", "question_type": "", "original_pair_id": ""}
{"chunk_id": "api-faq-3", "model": "llama-3.1-8b", "chunk": "Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs.\nHTTP API\n: HTTP APIs are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads. They do not currently offer API management functionality.\nREST API\n: REST APIs offer API proxy functionality and API management features in a single solution. REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs.\nWebSocket API:\nWebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. With WebSocket APIs in API Gateway, you can define backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint to be invoked when messages are received from the connected clients.\n\nTo get started with HTTP APIs, you can use the Amazon API Gateway console, the AWS CLI, AWS SDKs, or AWS CloudFormation. To learn more about getting started with HTTP APIs, visit our\ndocumentation\n.\n\nTo get started with REST APIs, you can use the Amazon API Gateway console, the AWS CLI, or AWS SDKs. To learn more about getting started with REST APIs, visit our\ndocumentation\n.\n\n", "question": "What is the primary difference between HTTP APIs and REST APIs?", "answer": "HTTP APIs are optimized for serverless workloads and do not offer API management functionality, whereas REST APIs offer API proxy functionality and API management features", "question_type": "", "original_pair_id": ""}
{"chunk_id": "appflow-faq-0", "model": "llama-3.1-8b", "chunk": "Analytics\n›\nAmazon Appflow\n›\nFAQs\n\nGeneral\n7\nSaaS Integrations\n4\nPrivacy and Security\n2\nAWS Services\n5\n\nPrivacy and Security\n\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. With AppFlow, you can run data flows at nearly any scale at the frequency you choose - on a schedule, in response to a business event, or on demand. You can configure powerful data transformation capabilities like filtering and validation to generate rich, ready-to-use data as part of the flow itself, without additional steps. AppFlow automatically encrypts data in motion, and allows users to restrict data from flowing over the public Internet for SaaS applications that are integrated with AWS PrivateLink, reducing exposure to security threats.\n\n", "question": "What is Amazon AppFlow?", "answer": "Amazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications and AWS services.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "appflow-faq-0", "model": "llama-3.1-8b", "chunk": "Analytics\n›\nAmazon Appflow\n›\nFAQs\n\nGeneral\n7\nSaaS Integrations\n4\nPrivacy and Security\n2\nAWS Services\n5\n\nPrivacy and Security\n\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. With AppFlow, you can run data flows at nearly any scale at the frequency you choose - on a schedule, in response to a business event, or on demand. You can configure powerful data transformation capabilities like filtering and validation to generate rich, ready-to-use data as part of the flow itself, without additional steps. AppFlow automatically encrypts data in motion, and allows users to restrict data from flowing over the public Internet for SaaS applications that are integrated with AWS PrivateLink, reducing exposure to security threats.\n\n", "question": "How does Amazon AppFlow handle data encryption?", "answer": "Amazon AppFlow automatically encrypts data in motion.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "appflow-faq-0", "model": "llama-3.1-8b", "chunk": "Analytics\n›\nAmazon Appflow\n›\nFAQs\n\nGeneral\n7\nSaaS Integrations\n4\nPrivacy and Security\n2\nAWS Services\n5\n\nPrivacy and Security\n\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. With AppFlow, you can run data flows at nearly any scale at the frequency you choose - on a schedule, in response to a business event, or on demand. You can configure powerful data transformation capabilities like filtering and validation to generate rich, ready-to-use data as part of the flow itself, without additional steps. AppFlow automatically encrypts data in motion, and allows users to restrict data from flowing over the public Internet for SaaS applications that are integrated with AWS PrivateLink, reducing exposure to security threats.\n\n", "question": "What is the difference between AppFlow's encryption of data in motion and data encryption for SaaS applications integrated with AWS PrivateLink?", "answer": "AppFlow automatically encrypts data in motion, whereas for SaaS applications integrated with AWS PrivateLink, AppFlow allows users to restrict data from flowing over the public Internet.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "application-faq-3", "model": "llama-3.1-8b", "chunk": "Please refer to the\nAWS Regional Services List\nfor the most up-to-date information.\n\n", "question": "What is the recommended resource for the most up-to-date information on AWS Regional Services?", "answer": "The AWS Regional Services List", "question_type": "", "original_pair_id": ""}
{"chunk_id": "application-faq-3", "model": "llama-3.1-8b", "chunk": "Please refer to the\nAWS Regional Services List\nfor the most up-to-date information.\n\n", "question": "Why would you refer to the AWS Regional Services List for information?", "answer": "For the most up-to-date information", "question_type": "", "original_pair_id": ""}
{"chunk_id": "application-faq-3", "model": "llama-3.1-8b", "chunk": "Please refer to the\nAWS Regional Services List\nfor the most up-to-date information.\n\n", "question": "What information is provided by the AWS Regional Services List?", "answer": "Information on AWS Regional Services", "question_type": "", "original_pair_id": ""}
{"chunk_id": "appsync-faq-3", "model": "llama-3.1-8b", "chunk": "AWS AppSync enables customers to use custom domain names with their AWS AppSync API to access their GraphQl endpoint and real-time endpoint. To create a custom domain name in AppSync, you simply provide a domain name you own and indicate a valid AWS Certificate Manager (ACM) certificate that covers your domain. Once the custom domain name is created, you can associate the domain name with any available AppSync API in your account. After you have updated your DNS record to map to to the AppSync-provided domain name, you can configure your applications to use the new GraphQL and real-time endpoints. You can change the API association on your custom domain at any time without having to update your applications. When AppSync receives a request on the custom domain endpoint, it routes it to the associated API for handling.\n\nYes, AWS AppSync supports private APIs. With private APIs, you can create GraphQL APIs that can only be accessed from your Amazon Virtual Private Cloud (VPC).\n\nAn \"event\" is a data record expressing an occurrence and its context. Events are routed from an event producer (the source) to interested event consumers. AppSync Events gives developers the ability to publish events using the HTTP protocol and JSON format. Subscribing is done over WebSocket channels.\n\nA channel is a routing concept. It is the logical destination of an event. It allows a publisher to say: “this is where my event should be routed.” It allows a subscriber to say: “this is the route that I want to monitor for arriving events”.\n\nA namespace is a logical construct that is used to define capabilities shared by channels that live within it. For example, you can define multiple authorization modes for your Event API, then attach specific modes on your namespaces (e.g.: only allow IAM_AUTH mode on namespace /chat).\n\nYes. Simply attach an Event Handler that specifies an onPublish function. In that handler, implement your transformation logic.\n\n", "question": "What is required to create a custom domain name in AppSync?", "answer": "You simply provide a domain name you own and indicate a valid AWS Certificate Manager (ACM) certificate that covers your domain.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "appsync-faq-3", "model": "llama-3.1-8b", "chunk": "AWS AppSync enables customers to use custom domain names with their AWS AppSync API to access their GraphQl endpoint and real-time endpoint. To create a custom domain name in AppSync, you simply provide a domain name you own and indicate a valid AWS Certificate Manager (ACM) certificate that covers your domain. Once the custom domain name is created, you can associate the domain name with any available AppSync API in your account. After you have updated your DNS record to map to to the AppSync-provided domain name, you can configure your applications to use the new GraphQL and real-time endpoints. You can change the API association on your custom domain at any time without having to update your applications. When AppSync receives a request on the custom domain endpoint, it routes it to the associated API for handling.\n\nYes, AWS AppSync supports private APIs. With private APIs, you can create GraphQL APIs that can only be accessed from your Amazon Virtual Private Cloud (VPC).\n\nAn \"event\" is a data record expressing an occurrence and its context. Events are routed from an event producer (the source) to interested event consumers. AppSync Events gives developers the ability to publish events using the HTTP protocol and JSON format. Subscribing is done over WebSocket channels.\n\nA channel is a routing concept. It is the logical destination of an event. It allows a publisher to say: “this is where my event should be routed.” It allows a subscriber to say: “this is the route that I want to monitor for arriving events”.\n\nA namespace is a logical construct that is used to define capabilities shared by channels that live within it. For example, you can define multiple authorization modes for your Event API, then attach specific modes on your namespaces (e.g.: only allow IAM_AUTH mode on namespace /chat).\n\nYes. Simply attach an Event Handler that specifies an onPublish function. In that handler, implement your transformation logic.\n\n", "question": "How does AppSync route requests on a custom domain endpoint?", "answer": "AppSync routes it to the associated API for handling.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "appsync-faq-3", "model": "llama-3.1-8b", "chunk": "AWS AppSync enables customers to use custom domain names with their AWS AppSync API to access their GraphQl endpoint and real-time endpoint. To create a custom domain name in AppSync, you simply provide a domain name you own and indicate a valid AWS Certificate Manager (ACM) certificate that covers your domain. Once the custom domain name is created, you can associate the domain name with any available AppSync API in your account. After you have updated your DNS record to map to to the AppSync-provided domain name, you can configure your applications to use the new GraphQL and real-time endpoints. You can change the API association on your custom domain at any time without having to update your applications. When AppSync receives a request on the custom domain endpoint, it routes it to the associated API for handling.\n\nYes, AWS AppSync supports private APIs. With private APIs, you can create GraphQL APIs that can only be accessed from your Amazon Virtual Private Cloud (VPC).\n\nAn \"event\" is a data record expressing an occurrence and its context. Events are routed from an event producer (the source) to interested event consumers. AppSync Events gives developers the ability to publish events using the HTTP protocol and JSON format. Subscribing is done over WebSocket channels.\n\nA channel is a routing concept. It is the logical destination of an event. It allows a publisher to say: “this is where my event should be routed.” It allows a subscriber to say: “this is the route that I want to monitor for arriving events”.\n\nA namespace is a logical construct that is used to define capabilities shared by channels that live within it. For example, you can define multiple authorization modes for your Event API, then attach specific modes on your namespaces (e.g.: only allow IAM_AUTH mode on namespace /chat).\n\nYes. Simply attach an Event Handler that specifies an onPublish function. In that handler, implement your transformation logic.\n\n", "question": "What is the difference between a channel and a namespace in AppSync Events?", "answer": "A channel is the logical destination of an event, while a namespace is a logical construct that defines capabilities shared by channels that live within it.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "artifact-faq-0", "model": "llama-3.1-8b", "chunk": "Security, Identity and Compliance\n›\nAWS Artifact\n›\nAWS Artifact FAQs\n\nGeneral\n8\nAWS Reports\n3\nThird-Party Reports\n4\nAgreements\n20\nBusiness Associate Addendum (BAA)\n18\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n9\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n9\nTroubleshooting\n2\nNotifications\n8\n\nBusiness Associate Addendum (BAA)\n\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n\nAWS Artifact, available in the console, is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS’ compliance documentation and AWS agreements.\nYou can use AWS Artifact Reports to download AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports.\nYou can use AWS Artifact Agreements to review, accept, and track the status of AWS agreements such as the Business Associate Addendum (BAA).\n\nAll AWS Accounts have access to AWS Artifact. Root users and IAM users with admin permissions can download all audit artifacts available to their account by agreeing to the associated terms and conditions.\nYou will need to grant IAM users with non-admin permissions access to AWS Artifact using IAM permissions. This allows you to grant a user access to AWS Artifact, while restricting access to other services and resources within your AWS Account. For information on how to grant access using IAM, refer to this help topic in the\nAWS Artifact documentation\n.\n\nAn Agreement can have two states:\nInactive:\nAn Agreement is in Inactive state if the Agreement has not been accepted by the user or a previously accepted Agreement has been terminated by the user\nActive:\nAn Agreement is in Active state if the Agreement has been accepted by the user\n\n", "question": "What is AWS Artifact?", "answer": "AWS Artifact is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS' compliance documentation and AWS agreements.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "artifact-faq-0", "model": "llama-3.1-8b", "chunk": "Security, Identity and Compliance\n›\nAWS Artifact\n›\nAWS Artifact FAQs\n\nGeneral\n8\nAWS Reports\n3\nThird-Party Reports\n4\nAgreements\n20\nBusiness Associate Addendum (BAA)\n18\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n9\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n9\nTroubleshooting\n2\nNotifications\n8\n\nBusiness Associate Addendum (BAA)\n\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n\nAWS Artifact, available in the console, is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS’ compliance documentation and AWS agreements.\nYou can use AWS Artifact Reports to download AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports.\nYou can use AWS Artifact Agreements to review, accept, and track the status of AWS agreements such as the Business Associate Addendum (BAA).\n\nAll AWS Accounts have access to AWS Artifact. Root users and IAM users with admin permissions can download all audit artifacts available to their account by agreeing to the associated terms and conditions.\nYou will need to grant IAM users with non-admin permissions access to AWS Artifact using IAM permissions. This allows you to grant a user access to AWS Artifact, while restricting access to other services and resources within your AWS Account. For information on how to grant access using IAM, refer to this help topic in the\nAWS Artifact documentation\n.\n\nAn Agreement can have two states:\nInactive:\nAn Agreement is in Inactive state if the Agreement has not been accepted by the user or a previously accepted Agreement has been terminated by the user\nActive:\nAn Agreement is in Active state if the Agreement has been accepted by the user\n\n", "question": "How does an Agreement in AWS Artifact change state?", "answer": "An Agreement in AWS Artifact can change to either Inactive or Active state. It is in Inactive state if the Agreement has not been accepted by the user or a previously accepted Agreement has been terminated by the user, and it is in Active state if the Agreement has been accepted by the user.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "artifact-faq-0", "model": "llama-3.1-8b", "chunk": "Security, Identity and Compliance\n›\nAWS Artifact\n›\nAWS Artifact FAQs\n\nGeneral\n8\nAWS Reports\n3\nThird-Party Reports\n4\nAgreements\n20\nBusiness Associate Addendum (BAA)\n18\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n9\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n9\nTroubleshooting\n2\nNotifications\n8\n\nBusiness Associate Addendum (BAA)\n\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n\nAWS Artifact, available in the console, is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS’ compliance documentation and AWS agreements.\nYou can use AWS Artifact Reports to download AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports.\nYou can use AWS Artifact Agreements to review, accept, and track the status of AWS agreements such as the Business Associate Addendum (BAA).\n\nAll AWS Accounts have access to AWS Artifact. Root users and IAM users with admin permissions can download all audit artifacts available to their account by agreeing to the associated terms and conditions.\nYou will need to grant IAM users with non-admin permissions access to AWS Artifact using IAM permissions. This allows you to grant a user access to AWS Artifact, while restricting access to other services and resources within your AWS Account. For information on how to grant access using IAM, refer to this help topic in the\nAWS Artifact documentation\n.\n\nAn Agreement can have two states:\nInactive:\nAn Agreement is in Inactive state if the Agreement has not been accepted by the user or a previously accepted Agreement has been terminated by the user\nActive:\nAn Agreement is in Active state if the Agreement has been accepted by the user\n\n", "question": "What is the difference between an Inactive and an Active Agreement in AWS Artifact?", "answer": "An Inactive Agreement has not been accepted by the user or a previously accepted Agreement has been terminated by the user, whereas an Active Agreement has been accepted by the user.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "athena-faq-3", "model": "llama-3.1-8b", "chunk": "AWS Glue\nis a fully managed extract, transform, and load (ETL) service. AWS Glue has three main components: 1) a crawler that automatically scans your data sources, identifies data formats, and infers schemas, 2) a fully managed ETL service that allows you to transform and move data to various destinations, and 3) a Data Catalog that stores metadata information about databases and tables either stored in S3 or an\nODBC\n- or\nJDBC\n-compliant data store. To use the benefits of AWS Glue, you must upgrade from using Athena’s internal Data Catalog to the Glue Data Catalog.\nBenefits of upgrading to the Data Catalog include the following:\nUnified metadata repository: AWS Glue is integrated across various AWS services. AWS Glue supports data stored in Amazon Aurora, Amazon Relational Database Service (RDS) for MySQL, Amazon RDS for PostgreSQL, Amazon Redshift, and S3, as well as MySQL and PostgreSQL databases in your Amazon Virtual Private Cloud (VPC) running on Amazon Elastic Compute Cloud (EC2). AWS Glue provides out-of-the-box integration with Athena, Amazon EMR, Amazon Redshift Spectrum, and applications compatible with Apache Hive metastore.\nAutomatic schema and partition recognition: AWS Glue automatically crawls your data sources, identifies data formats, and suggests schemas and transformations. Crawlers can help automate table creation and automatic loading of partitions.\nTo learn more about the Data Catalog, review the\nAWS Glue\nwebpage.\n\nYes. For a step-by-step process, review the Amazon Athena User Guide:\nIntegration with AWS Glue\n.\n\nFor details of Athena service availability by Region, review the\nAWS Regional Services List\n.\n\nTo learn more about service limits, review the Amazon Athena User Guide: Service Quotas.\n\n", "question": "What are the three main components of AWS Glue?", "answer": "AWS Glue has three main components: 1) a crawler that automatically scans your data sources, identifies data formats, and infers schemas, 2) a fully managed ETL service that allows you to transform and move data to various destinations, and 3) a Data Catalog that stores metadata information about databases and tables either stored in S3 or an ODBC- or JDBC-compliant data store.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "athena-faq-3", "model": "llama-3.1-8b", "chunk": "AWS Glue\nis a fully managed extract, transform, and load (ETL) service. AWS Glue has three main components: 1) a crawler that automatically scans your data sources, identifies data formats, and infers schemas, 2) a fully managed ETL service that allows you to transform and move data to various destinations, and 3) a Data Catalog that stores metadata information about databases and tables either stored in S3 or an\nODBC\n- or\nJDBC\n-compliant data store. To use the benefits of AWS Glue, you must upgrade from using Athena’s internal Data Catalog to the Glue Data Catalog.\nBenefits of upgrading to the Data Catalog include the following:\nUnified metadata repository: AWS Glue is integrated across various AWS services. AWS Glue supports data stored in Amazon Aurora, Amazon Relational Database Service (RDS) for MySQL, Amazon RDS for PostgreSQL, Amazon Redshift, and S3, as well as MySQL and PostgreSQL databases in your Amazon Virtual Private Cloud (VPC) running on Amazon Elastic Compute Cloud (EC2). AWS Glue provides out-of-the-box integration with Athena, Amazon EMR, Amazon Redshift Spectrum, and applications compatible with Apache Hive metastore.\nAutomatic schema and partition recognition: AWS Glue automatically crawls your data sources, identifies data formats, and suggests schemas and transformations. Crawlers can help automate table creation and automatic loading of partitions.\nTo learn more about the Data Catalog, review the\nAWS Glue\nwebpage.\n\nYes. For a step-by-step process, review the Amazon Athena User Guide:\nIntegration with AWS Glue\n.\n\nFor details of Athena service availability by Region, review the\nAWS Regional Services List\n.\n\nTo learn more about service limits, review the Amazon Athena User Guide: Service Quotas.\n\n", "question": "Why would you use the Glue Data Catalog over Athena's internal Data Catalog?", "answer": "Benefits of upgrading to the Data Catalog include a unified metadata repository and integration across various AWS services.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "athena-faq-3", "model": "llama-3.1-8b", "chunk": "AWS Glue\nis a fully managed extract, transform, and load (ETL) service. AWS Glue has three main components: 1) a crawler that automatically scans your data sources, identifies data formats, and infers schemas, 2) a fully managed ETL service that allows you to transform and move data to various destinations, and 3) a Data Catalog that stores metadata information about databases and tables either stored in S3 or an\nODBC\n- or\nJDBC\n-compliant data store. To use the benefits of AWS Glue, you must upgrade from using Athena’s internal Data Catalog to the Glue Data Catalog.\nBenefits of upgrading to the Data Catalog include the following:\nUnified metadata repository: AWS Glue is integrated across various AWS services. AWS Glue supports data stored in Amazon Aurora, Amazon Relational Database Service (RDS) for MySQL, Amazon RDS for PostgreSQL, Amazon Redshift, and S3, as well as MySQL and PostgreSQL databases in your Amazon Virtual Private Cloud (VPC) running on Amazon Elastic Compute Cloud (EC2). AWS Glue provides out-of-the-box integration with Athena, Amazon EMR, Amazon Redshift Spectrum, and applications compatible with Apache Hive metastore.\nAutomatic schema and partition recognition: AWS Glue automatically crawls your data sources, identifies data formats, and suggests schemas and transformations. Crawlers can help automate table creation and automatic loading of partitions.\nTo learn more about the Data Catalog, review the\nAWS Glue\nwebpage.\n\nYes. For a step-by-step process, review the Amazon Athena User Guide:\nIntegration with AWS Glue\n.\n\nFor details of Athena service availability by Region, review the\nAWS Regional Services List\n.\n\nTo learn more about service limits, review the Amazon Athena User Guide: Service Quotas.\n\n", "question": "How does the crawler component of AWS Glue compare to the process of loading partitions in Amazon Athena?", "answer": "AWS Glue crawlers can help automate table creation and automatic loading of partitions, suggesting schemas and transformations.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "audit-faq-3", "model": "llama-3.1-8b", "chunk": "A control is a prescriptive description that explains how to implement a procedure to conform to a given rule, such as a compliance requirement. It provides a reasonable assurance that the resources used by your organization operate as intended, that data is reliable, and that your organization is in compliance with applicable laws and regulations.\n\nAWS Audit Manager enables you to define your own controls to collect evidence from specific data sources to help you meet unique compliance requirements.\nA common control collects evidence that can support a range of overlapping compliance obligations. Each common control consists of one or more core controls that collects evidence from a predefined group of AWS managed data sources. AWS updates these underlying data sources for you when regulations and standards change and new data sources are identified.\n\nAn AWS Audit Manager assessment is an implementation of an AWS Audit Manager framework. Using a framework as a starting point, you can create an assessment and define the AWS accounts that you want to include in the scope of your audit. After your assessment is created, AWS Audit Manager begins to automatically assess resources in your AWS accounts and services based on the controls deﬁned in the framework. Next, it collects the relevant evidence and converts it into an auditor-friendly format, and then attaches it to the controls in your assessment.\n\nA resource assessment is a process that collects, stores, and manages evidence, which you can use to assess risk and compliance with industry standards and regulations. When you define and launch an assessment based on an assessment framework, Audit Manager will execute a resource assessment for each individual resource, such as your Amazon EC2 instances, Amazon RDS instances, Amazon S3 buckets, or Amazon VPC subnets.\n\nEvidence is a record that contains the information needed to demonstrate compliance with the requirements specified by a control. Examples of evidence could be a change activity triggered by a user, or a system configuration snapshot.\n\nAn assessment report is a finalized document generated from an AWS Audit Manager assessment. The report summarizes the relevant evidence collected for your audit. The report links to the relevant evidence folders, which are named and organized according to the controls that are specified in your assessment.\n\n", "question": "What is a control in the context of AWS Audit Manager?", "answer": "A control is a prescriptive description that explains how to implement a procedure to conform to a given rule, such as a compliance requirement. It provides a reasonable assurance that the resources used by your organization operate as intended, that data is reliable, and that your organization is in compliance with applicable laws and regulations.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "audit-faq-3", "model": "llama-3.1-8b", "chunk": "A control is a prescriptive description that explains how to implement a procedure to conform to a given rule, such as a compliance requirement. It provides a reasonable assurance that the resources used by your organization operate as intended, that data is reliable, and that your organization is in compliance with applicable laws and regulations.\n\nAWS Audit Manager enables you to define your own controls to collect evidence from specific data sources to help you meet unique compliance requirements.\nA common control collects evidence that can support a range of overlapping compliance obligations. Each common control consists of one or more core controls that collects evidence from a predefined group of AWS managed data sources. AWS updates these underlying data sources for you when regulations and standards change and new data sources are identified.\n\nAn AWS Audit Manager assessment is an implementation of an AWS Audit Manager framework. Using a framework as a starting point, you can create an assessment and define the AWS accounts that you want to include in the scope of your audit. After your assessment is created, AWS Audit Manager begins to automatically assess resources in your AWS accounts and services based on the controls deﬁned in the framework. Next, it collects the relevant evidence and converts it into an auditor-friendly format, and then attaches it to the controls in your assessment.\n\nA resource assessment is a process that collects, stores, and manages evidence, which you can use to assess risk and compliance with industry standards and regulations. When you define and launch an assessment based on an assessment framework, Audit Manager will execute a resource assessment for each individual resource, such as your Amazon EC2 instances, Amazon RDS instances, Amazon S3 buckets, or Amazon VPC subnets.\n\nEvidence is a record that contains the information needed to demonstrate compliance with the requirements specified by a control. Examples of evidence could be a change activity triggered by a user, or a system configuration snapshot.\n\nAn assessment report is a finalized document generated from an AWS Audit Manager assessment. The report summarizes the relevant evidence collected for your audit. The report links to the relevant evidence folders, which are named and organized according to the controls that are specified in your assessment.\n\n", "question": "How does AWS Audit Manager use controls to support compliance requirements?", "answer": "AWS Audit Manager enables you to define your own controls to collect evidence from specific data sources to help you meet unique compliance requirements. A common control collects evidence that can support a range of overlapping compliance obligations.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "audit-faq-3", "model": "llama-3.1-8b", "chunk": "A control is a prescriptive description that explains how to implement a procedure to conform to a given rule, such as a compliance requirement. It provides a reasonable assurance that the resources used by your organization operate as intended, that data is reliable, and that your organization is in compliance with applicable laws and regulations.\n\nAWS Audit Manager enables you to define your own controls to collect evidence from specific data sources to help you meet unique compliance requirements.\nA common control collects evidence that can support a range of overlapping compliance obligations. Each common control consists of one or more core controls that collects evidence from a predefined group of AWS managed data sources. AWS updates these underlying data sources for you when regulations and standards change and new data sources are identified.\n\nAn AWS Audit Manager assessment is an implementation of an AWS Audit Manager framework. Using a framework as a starting point, you can create an assessment and define the AWS accounts that you want to include in the scope of your audit. After your assessment is created, AWS Audit Manager begins to automatically assess resources in your AWS accounts and services based on the controls deﬁned in the framework. Next, it collects the relevant evidence and converts it into an auditor-friendly format, and then attaches it to the controls in your assessment.\n\nA resource assessment is a process that collects, stores, and manages evidence, which you can use to assess risk and compliance with industry standards and regulations. When you define and launch an assessment based on an assessment framework, Audit Manager will execute a resource assessment for each individual resource, such as your Amazon EC2 instances, Amazon RDS instances, Amazon S3 buckets, or Amazon VPC subnets.\n\nEvidence is a record that contains the information needed to demonstrate compliance with the requirements specified by a control. Examples of evidence could be a change activity triggered by a user, or a system configuration snapshot.\n\nAn assessment report is a finalized document generated from an AWS Audit Manager assessment. The report summarizes the relevant evidence collected for your audit. The report links to the relevant evidence folders, which are named and organized according to the controls that are specified in your assessment.\n\n", "question": "What is the difference between a resource assessment and an assessment report in AWS Audit Manager?", "answer": "A resource assessment is a process that collects, stores, and manages evidence, which you can use to assess risk and compliance with industry standards and regulations. An assessment report is a finalized document generated from an AWS Audit Manager assessment. The report summarizes the relevant evidence collected for your audit and links to the relevant evidence folders.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "augmented-faq-1", "model": "llama-3.1-8b", "chunk": "Amazon A2I provides a managed experience where you can set up an entire human review workflow in a few easy steps. To get started with Amazon A2I, sign in to your AWS Console, and navigate to the Amazon SageMaker console. From there, select Human review workflows under Augmented AI. First, as a part of the human review workflow, you provide a pointer to the S3 bucket where the review results should be stored. Next, you select the appropriate task type and define conditions when a human review should be triggered. Amazon A2I provides pre-built workflows where you only need to enter a few choices and provide instructions on how your objects should be reviewed by humans. Alternatively, you can create your own custom workflow and use your own custom review templates. Once created, the workflow can be used directly in your applications using a generated unique identifier for this workflow.\n\nWith A2I, you can define what is an acceptable prediction confidence for your business problem. You can define business rules for the machine learning predictions, based on which a human review is triggered. For Amazon Rekognition image moderation tasks, you can use the confidence score that Amazon Rekognition provides for each label it outputs to trigger human review. For Amazon Textract tasks, you can trigger a human review when specific form keys are missing or when form key detection confidence is low. You can also trigger a human review if, after evaluating all form keys in the text, confidence is lower than your required threshold for any form key. For your own custom workflow, you can write the code for business conditions in AWS Lambda or directly in your client application.\n\nWith Amazon A2I, you can choose from three workforce options: (1) Amazon Mechanical Turk; (2) Third party data labeling service providers available through the AWS Marketplace; and (3) Your own employees. See the\nAmazon A2I developer guide\nfor more information.\n\nPlease see the\nAmazon A2I pricing page\nfor the current pricing information.\n\nThe\nAWS Region Table\nlists all the AWS regions where Amazon A2I is currently available.\n\n", "question": "Where do you sign in to get started with Amazon A2I?", "answer": "The AWS Console", "question_type": "", "original_pair_id": ""}
{"chunk_id": "augmented-faq-1", "model": "llama-3.1-8b", "chunk": "Amazon A2I provides a managed experience where you can set up an entire human review workflow in a few easy steps. To get started with Amazon A2I, sign in to your AWS Console, and navigate to the Amazon SageMaker console. From there, select Human review workflows under Augmented AI. First, as a part of the human review workflow, you provide a pointer to the S3 bucket where the review results should be stored. Next, you select the appropriate task type and define conditions when a human review should be triggered. Amazon A2I provides pre-built workflows where you only need to enter a few choices and provide instructions on how your objects should be reviewed by humans. Alternatively, you can create your own custom workflow and use your own custom review templates. Once created, the workflow can be used directly in your applications using a generated unique identifier for this workflow.\n\nWith A2I, you can define what is an acceptable prediction confidence for your business problem. You can define business rules for the machine learning predictions, based on which a human review is triggered. For Amazon Rekognition image moderation tasks, you can use the confidence score that Amazon Rekognition provides for each label it outputs to trigger human review. For Amazon Textract tasks, you can trigger a human review when specific form keys are missing or when form key detection confidence is low. You can also trigger a human review if, after evaluating all form keys in the text, confidence is lower than your required threshold for any form key. For your own custom workflow, you can write the code for business conditions in AWS Lambda or directly in your client application.\n\nWith Amazon A2I, you can choose from three workforce options: (1) Amazon Mechanical Turk; (2) Third party data labeling service providers available through the AWS Marketplace; and (3) Your own employees. See the\nAmazon A2I developer guide\nfor more information.\n\nPlease see the\nAmazon A2I pricing page\nfor the current pricing information.\n\nThe\nAWS Region Table\nlists all the AWS regions where Amazon A2I is currently available.\n\n", "question": "How does Amazon A2I help with machine learning predictions?", "answer": "Amazon A2I provides a way to define business rules for machine learning predictions, based on which a human review is triggered, and also allows you to choose the confidence score threshold for triggering human review.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "augmented-faq-1", "model": "llama-3.1-8b", "chunk": "Amazon A2I provides a managed experience where you can set up an entire human review workflow in a few easy steps. To get started with Amazon A2I, sign in to your AWS Console, and navigate to the Amazon SageMaker console. From there, select Human review workflows under Augmented AI. First, as a part of the human review workflow, you provide a pointer to the S3 bucket where the review results should be stored. Next, you select the appropriate task type and define conditions when a human review should be triggered. Amazon A2I provides pre-built workflows where you only need to enter a few choices and provide instructions on how your objects should be reviewed by humans. Alternatively, you can create your own custom workflow and use your own custom review templates. Once created, the workflow can be used directly in your applications using a generated unique identifier for this workflow.\n\nWith A2I, you can define what is an acceptable prediction confidence for your business problem. You can define business rules for the machine learning predictions, based on which a human review is triggered. For Amazon Rekognition image moderation tasks, you can use the confidence score that Amazon Rekognition provides for each label it outputs to trigger human review. For Amazon Textract tasks, you can trigger a human review when specific form keys are missing or when form key detection confidence is low. You can also trigger a human review if, after evaluating all form keys in the text, confidence is lower than your required threshold for any form key. For your own custom workflow, you can write the code for business conditions in AWS Lambda or directly in your client application.\n\nWith Amazon A2I, you can choose from three workforce options: (1) Amazon Mechanical Turk; (2) Third party data labeling service providers available through the AWS Marketplace; and (3) Your own employees. See the\nAmazon A2I developer guide\nfor more information.\n\nPlease see the\nAmazon A2I pricing page\nfor the current pricing information.\n\nThe\nAWS Region Table\nlists all the AWS regions where Amazon A2I is currently available.\n\n", "question": "What are the three workforce options available with Amazon A2I?", "answer": "You can choose from three workforce options: Amazon Mechanical Turk, Third party data labeling service providers available through the AWS Marketplace, and your own employees.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "autoscaling-faq-3", "model": "llama-3.1-8b", "chunk": "You should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling lets you define dynamic scaling policies for multiple EC2 Auto Scaling groups or other resources using predefined scaling strategies. Using AWS Auto Scaling to configure scaling policies for all of the scalable resources in your application is faster than managing scaling policies for each resource via its individual service console. It’s also easier, as AWS Auto Scaling includes predefined scaling strategies that simplify the setup of scaling policies. You should also use AWS Auto Scaling if you want to create predictive scaling for EC2 resources.\nYou should use EC2 Auto Scaling if you only need to scale Amazon EC2 Auto Scaling groups, or if you are only interested in maintaining the health of your EC2 fleet. You should also use EC2 Auto Scaling if you need to create or configure Amazon EC2 Auto Scaling groups, or if you need to set up scheduled or step scaling policies (as AWS Auto Scaling supports only target tracking scaling policies).\nEC2 Auto Scaling groups must be created and configured outside of AWS Auto Scaling, such as through the EC2 console, Auto Scaling API or via CloudFormation. AWS Auto Scaling can help you configure dynamic scaling policies for your existing EC2 Auto Scaling groups.\n\nYou should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling enables unified scaling for multiple resources, and has predefined guidance that helps make it easier and faster to configure scaling. If you prefer, you can instead choose to use the individual service consoles, Auto Scaling API, or Application Auto Scaling API to scale individual AWS services. You should also use the individual consoles or API if you want to setup step scaling policies or scheduled scaling, as AWS Auto Scaling creates target tracking scaling policies only.\n\nPredictive Scaling is a feature of AWS Auto Scaling that looks at historic traffic patterns and forecasts them into the future to schedule changes in the number of EC2 instances at the appropriate times going forward. Predictive Scaling uses machine learning models to forecast daily and weekly patterns.\nAuto Scaling enhanced with Predictive Scaling delivers faster, simpler, and more accurate capacity provisioning resulting in lower cost and more responsive applications. By predicting traffic changes, Predictive Scaling provisions EC2 instances in advance of changing traffic, making Auto Scaling faster and more accurate.\n\n", "question": "What does AWS Auto Scaling enable?", "answer": "unified scaling for multiple resources", "question_type": "", "original_pair_id": ""}
{"chunk_id": "autoscaling-faq-3", "model": "llama-3.1-8b", "chunk": "You should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling lets you define dynamic scaling policies for multiple EC2 Auto Scaling groups or other resources using predefined scaling strategies. Using AWS Auto Scaling to configure scaling policies for all of the scalable resources in your application is faster than managing scaling policies for each resource via its individual service console. It’s also easier, as AWS Auto Scaling includes predefined scaling strategies that simplify the setup of scaling policies. You should also use AWS Auto Scaling if you want to create predictive scaling for EC2 resources.\nYou should use EC2 Auto Scaling if you only need to scale Amazon EC2 Auto Scaling groups, or if you are only interested in maintaining the health of your EC2 fleet. You should also use EC2 Auto Scaling if you need to create or configure Amazon EC2 Auto Scaling groups, or if you need to set up scheduled or step scaling policies (as AWS Auto Scaling supports only target tracking scaling policies).\nEC2 Auto Scaling groups must be created and configured outside of AWS Auto Scaling, such as through the EC2 console, Auto Scaling API or via CloudFormation. AWS Auto Scaling can help you configure dynamic scaling policies for your existing EC2 Auto Scaling groups.\n\nYou should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling enables unified scaling for multiple resources, and has predefined guidance that helps make it easier and faster to configure scaling. If you prefer, you can instead choose to use the individual service consoles, Auto Scaling API, or Application Auto Scaling API to scale individual AWS services. You should also use the individual consoles or API if you want to setup step scaling policies or scheduled scaling, as AWS Auto Scaling creates target tracking scaling policies only.\n\nPredictive Scaling is a feature of AWS Auto Scaling that looks at historic traffic patterns and forecasts them into the future to schedule changes in the number of EC2 instances at the appropriate times going forward. Predictive Scaling uses machine learning models to forecast daily and weekly patterns.\nAuto Scaling enhanced with Predictive Scaling delivers faster, simpler, and more accurate capacity provisioning resulting in lower cost and more responsive applications. By predicting traffic changes, Predictive Scaling provisions EC2 instances in advance of changing traffic, making Auto Scaling faster and more accurate.\n\n", "question": "Why would you use AWS Auto Scaling over individual service consoles?", "answer": "It’s faster and easier to configure scaling policies", "question_type": "", "original_pair_id": ""}
{"chunk_id": "autoscaling-faq-3", "model": "llama-3.1-8b", "chunk": "You should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling lets you define dynamic scaling policies for multiple EC2 Auto Scaling groups or other resources using predefined scaling strategies. Using AWS Auto Scaling to configure scaling policies for all of the scalable resources in your application is faster than managing scaling policies for each resource via its individual service console. It’s also easier, as AWS Auto Scaling includes predefined scaling strategies that simplify the setup of scaling policies. You should also use AWS Auto Scaling if you want to create predictive scaling for EC2 resources.\nYou should use EC2 Auto Scaling if you only need to scale Amazon EC2 Auto Scaling groups, or if you are only interested in maintaining the health of your EC2 fleet. You should also use EC2 Auto Scaling if you need to create or configure Amazon EC2 Auto Scaling groups, or if you need to set up scheduled or step scaling policies (as AWS Auto Scaling supports only target tracking scaling policies).\nEC2 Auto Scaling groups must be created and configured outside of AWS Auto Scaling, such as through the EC2 console, Auto Scaling API or via CloudFormation. AWS Auto Scaling can help you configure dynamic scaling policies for your existing EC2 Auto Scaling groups.\n\nYou should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling enables unified scaling for multiple resources, and has predefined guidance that helps make it easier and faster to configure scaling. If you prefer, you can instead choose to use the individual service consoles, Auto Scaling API, or Application Auto Scaling API to scale individual AWS services. You should also use the individual consoles or API if you want to setup step scaling policies or scheduled scaling, as AWS Auto Scaling creates target tracking scaling policies only.\n\nPredictive Scaling is a feature of AWS Auto Scaling that looks at historic traffic patterns and forecasts them into the future to schedule changes in the number of EC2 instances at the appropriate times going forward. Predictive Scaling uses machine learning models to forecast daily and weekly patterns.\nAuto Scaling enhanced with Predictive Scaling delivers faster, simpler, and more accurate capacity provisioning resulting in lower cost and more responsive applications. By predicting traffic changes, Predictive Scaling provisions EC2 instances in advance of changing traffic, making Auto Scaling faster and more accurate.\n\n", "question": "How does AWS Auto Scaling compare to EC2 Auto Scaling in terms of supported scaling policies?", "answer": "AWS Auto Scaling supports only target tracking scaling policies, while EC2 Auto Scaling supports scheduled or step scaling policies", "question_type": "", "original_pair_id": ""}
{"chunk_id": "aws-faq-3", "model": "llama-3.1-8b", "chunk": "FTPS and SFTP can both be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer a secure tunnel for transmission of commands and data. SFTP is a newer protocol and uses a single channel for commands and data, requiring fewer port openings than FTPS.\n\nSFTP, FTPS, and AS2 can all be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer secure transmission of data. Aside from support for encrypted and signed messages, AS2’s built in mechanism for Message Disposition Notification (MDN) alerts the sender that the message has been successfully received and decrypted by the recipient. This provides proof to the sender that their message was delivered without being tampered in transit. Use of AS2 is prevalent in workflows operating in retail, e-commerce, payments, supply chain for interacting with business partners who are also able to use AS2 to transact messages so that it is securely transmitted and delivered. AS2 provides you with options to ensure identity of the sender and receiver, integrity of the message, and confirm whether the message was successfully delivered and decrypted by the receiver.\n\nYes. Any existing file transfer client application will continue to work as long as you have enabled your endpoint for the chosen protocols. Examples of commonly used SFTP/FTPS/FTP clients include WinSCP, FileZilla, CyberDuck, lftp, and OpenSSH clients.\n\nYou can use AWS Transfer SFTP connectors to access files stored on external SFTP sites. Refer to\nSFTP connectors documentation\nto get started with SFTP connectors\n\nYou can use AWS Transfer Family’s fully managed SFTP/FTPS/AS2 capabilities to receive EDI documents that are generated from your trading partner’s business systems. EDI documents received using AWS Transfer Family’s connectivity capabilities are automatically uploaded to Amazon S3 where they can then be transformed into JSON and XML formatted outputs using AWS B2B Data Interchange. Alternatively, you can use any other any other EDI connectivity tool to upload EDI documents to S3.\n\n", "question": "What is the primary difference between SFTP and FTPS?", "answer": "SFTP is a newer protocol and uses a single channel for commands and data, requiring fewer port openings than FTPS.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "aws-faq-3", "model": "llama-3.1-8b", "chunk": "FTPS and SFTP can both be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer a secure tunnel for transmission of commands and data. SFTP is a newer protocol and uses a single channel for commands and data, requiring fewer port openings than FTPS.\n\nSFTP, FTPS, and AS2 can all be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer secure transmission of data. Aside from support for encrypted and signed messages, AS2’s built in mechanism for Message Disposition Notification (MDN) alerts the sender that the message has been successfully received and decrypted by the recipient. This provides proof to the sender that their message was delivered without being tampered in transit. Use of AS2 is prevalent in workflows operating in retail, e-commerce, payments, supply chain for interacting with business partners who are also able to use AS2 to transact messages so that it is securely transmitted and delivered. AS2 provides you with options to ensure identity of the sender and receiver, integrity of the message, and confirm whether the message was successfully delivered and decrypted by the receiver.\n\nYes. Any existing file transfer client application will continue to work as long as you have enabled your endpoint for the chosen protocols. Examples of commonly used SFTP/FTPS/FTP clients include WinSCP, FileZilla, CyberDuck, lftp, and OpenSSH clients.\n\nYou can use AWS Transfer SFTP connectors to access files stored on external SFTP sites. Refer to\nSFTP connectors documentation\nto get started with SFTP connectors\n\nYou can use AWS Transfer Family’s fully managed SFTP/FTPS/AS2 capabilities to receive EDI documents that are generated from your trading partner’s business systems. EDI documents received using AWS Transfer Family’s connectivity capabilities are automatically uploaded to Amazon S3 where they can then be transformed into JSON and XML formatted outputs using AWS B2B Data Interchange. Alternatively, you can use any other any other EDI connectivity tool to upload EDI documents to S3.\n\n", "question": "How does AS2 provide secure file transfer?", "answer": "AS2 provides secure file transfer through its built-in mechanism for Message Disposition Notification (MDN) alerts, which notifies the sender that the message has been successfully received and decrypted by the recipient, and through its support for encrypted and signed messages.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "aws-faq-3", "model": "llama-3.1-8b", "chunk": "FTPS and SFTP can both be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer a secure tunnel for transmission of commands and data. SFTP is a newer protocol and uses a single channel for commands and data, requiring fewer port openings than FTPS.\n\nSFTP, FTPS, and AS2 can all be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer secure transmission of data. Aside from support for encrypted and signed messages, AS2’s built in mechanism for Message Disposition Notification (MDN) alerts the sender that the message has been successfully received and decrypted by the recipient. This provides proof to the sender that their message was delivered without being tampered in transit. Use of AS2 is prevalent in workflows operating in retail, e-commerce, payments, supply chain for interacting with business partners who are also able to use AS2 to transact messages so that it is securely transmitted and delivered. AS2 provides you with options to ensure identity of the sender and receiver, integrity of the message, and confirm whether the message was successfully delivered and decrypted by the receiver.\n\nYes. Any existing file transfer client application will continue to work as long as you have enabled your endpoint for the chosen protocols. Examples of commonly used SFTP/FTPS/FTP clients include WinSCP, FileZilla, CyberDuck, lftp, and OpenSSH clients.\n\nYou can use AWS Transfer SFTP connectors to access files stored on external SFTP sites. Refer to\nSFTP connectors documentation\nto get started with SFTP connectors\n\nYou can use AWS Transfer Family’s fully managed SFTP/FTPS/AS2 capabilities to receive EDI documents that are generated from your trading partner’s business systems. EDI documents received using AWS Transfer Family’s connectivity capabilities are automatically uploaded to Amazon S3 where they can then be transformed into JSON and XML formatted outputs using AWS B2B Data Interchange. Alternatively, you can use any other any other EDI connectivity tool to upload EDI documents to S3.\n\n", "question": "What benefits do users get when they use AWS Transfer Family’s fully managed SFTP/FTPS/AS2 capabilities?", "answer": "Users can receive EDI documents that are generated from their trading partner’s business systems and automatically upload them to Amazon S3, where they can be transformed into JSON and XML formatted outputs using AWS B2B Data Interchange.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "backup-faq-2", "model": "llama-3.1-8b", "chunk": "Yes. Backups created using services with existing backup capabilities, such as EBS Snapshots, can be accessed using AWS Backup. Similarly, backups created by AWS Backup can be accessed using the source service.\n\nAWS services offer backup features to protect your data, such as Amazon S3 Replication, Amazon EBS Snapshots, Amazon RDS snapshots, Amazon FSx backups, Amazon DynamoDB backups, and AWS Storage Gateway snapshots. All existing per-service backup capabilities remain unchanged. AWS Backup provides a common way to manage backups across AWS services both on AWS and on premises. AWS Backup is a centralized service that offers backup scheduling, retention management, and backup monitoring. AWS Backup supports existing backup functionality provided by S3, EBS, RDS, Amazon FSx, DynamoDB, and Storage Gateway. For AWS services with backup functionality built on AWS Backup, such as Amazon EFS and DynamoDB, AWS Backup provides backup management capabilities. Additional features include lifecycle policies to transition backups to a low-cost storage tier, backup storage and encryption independent from its source data, and backup access policies.\n\nAmazon Data Lifecycle Manager policies and backup plans created in AWS Backup work independently from each other and provide two ways to manage EBS snapshots. Amazon Data Lifecycle Manager provides a streamlined way to manage the lifecycle of EBS resources, such as volume snapshots. Use Amazon Data Lifecycle Manager when you want to automate the creation, retention, and deletion of EBS snapshots. Use AWS Backup to manage and monitor backups across the AWS services you use, including EBS volumes, from a single place.\n\n", "question": "What AWS services offer backup features to protect your data?", "answer": "Amazon S3 Replication, Amazon EBS Snapshots, Amazon RDS snapshots, Amazon FSx backups, Amazon DynamoDB backups, and AWS Storage Gateway snapshots", "question_type": "", "original_pair_id": ""}
{"chunk_id": "backup-faq-2", "model": "llama-3.1-8b", "chunk": "Yes. Backups created using services with existing backup capabilities, such as EBS Snapshots, can be accessed using AWS Backup. Similarly, backups created by AWS Backup can be accessed using the source service.\n\nAWS services offer backup features to protect your data, such as Amazon S3 Replication, Amazon EBS Snapshots, Amazon RDS snapshots, Amazon FSx backups, Amazon DynamoDB backups, and AWS Storage Gateway snapshots. All existing per-service backup capabilities remain unchanged. AWS Backup provides a common way to manage backups across AWS services both on AWS and on premises. AWS Backup is a centralized service that offers backup scheduling, retention management, and backup monitoring. AWS Backup supports existing backup functionality provided by S3, EBS, RDS, Amazon FSx, DynamoDB, and Storage Gateway. For AWS services with backup functionality built on AWS Backup, such as Amazon EFS and DynamoDB, AWS Backup provides backup management capabilities. Additional features include lifecycle policies to transition backups to a low-cost storage tier, backup storage and encryption independent from its source data, and backup access policies.\n\nAmazon Data Lifecycle Manager policies and backup plans created in AWS Backup work independently from each other and provide two ways to manage EBS snapshots. Amazon Data Lifecycle Manager provides a streamlined way to manage the lifecycle of EBS resources, such as volume snapshots. Use Amazon Data Lifecycle Manager when you want to automate the creation, retention, and deletion of EBS snapshots. Use AWS Backup to manage and monitor backups across the AWS services you use, including EBS volumes, from a single place.\n\n", "question": "How does AWS Backup work with existing backup capabilities?", "answer": "Backups created using services with existing backup capabilities, such as EBS Snapshots, can be accessed using AWS Backup. Similarly, backups created by AWS Backup can be accessed using the source service.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "backup-faq-2", "model": "llama-3.1-8b", "chunk": "Yes. Backups created using services with existing backup capabilities, such as EBS Snapshots, can be accessed using AWS Backup. Similarly, backups created by AWS Backup can be accessed using the source service.\n\nAWS services offer backup features to protect your data, such as Amazon S3 Replication, Amazon EBS Snapshots, Amazon RDS snapshots, Amazon FSx backups, Amazon DynamoDB backups, and AWS Storage Gateway snapshots. All existing per-service backup capabilities remain unchanged. AWS Backup provides a common way to manage backups across AWS services both on AWS and on premises. AWS Backup is a centralized service that offers backup scheduling, retention management, and backup monitoring. AWS Backup supports existing backup functionality provided by S3, EBS, RDS, Amazon FSx, DynamoDB, and Storage Gateway. For AWS services with backup functionality built on AWS Backup, such as Amazon EFS and DynamoDB, AWS Backup provides backup management capabilities. Additional features include lifecycle policies to transition backups to a low-cost storage tier, backup storage and encryption independent from its source data, and backup access policies.\n\nAmazon Data Lifecycle Manager policies and backup plans created in AWS Backup work independently from each other and provide two ways to manage EBS snapshots. Amazon Data Lifecycle Manager provides a streamlined way to manage the lifecycle of EBS resources, such as volume snapshots. Use Amazon Data Lifecycle Manager when you want to automate the creation, retention, and deletion of EBS snapshots. Use AWS Backup to manage and monitor backups across the AWS services you use, including EBS volumes, from a single place.\n\n", "question": "What is the difference between using Amazon Data Lifecycle Manager and AWS Backup to manage EBS snapshots?", "answer": "Amazon Data Lifecycle Manager provides a streamlined way to automate the creation, retention, and deletion of EBS snapshots, whereas AWS Backup provides a centralized service to manage and monitor backups across AWS services, including EBS volumes, from a single place.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "batch-faq-2", "model": "llama-3.1-8b", "chunk": "Yes. You can set Fargate CE’s to have a max vCPU, which is the total amount of vCPU of all the jobs currently running in that CE. When your vCPU count hits max vCPU in a CE, Batch will begin scheduling jobs on the next Fargate CE in order attached to the queue, if there is one. This is useful if, for example, you want to set a Fargate CE to some minimum business requirement, then run the rest of your workload on Fargate Spot.\nWhile setting a Fargate Spot CE as first, followed by a Fargate CE, Batch will only spill over into Fargate when the vCPU used by your jobs is greater than max vCPU for that CE. In the event that Fargate Spot is reclaimed, max vCPU will not be met and Batch will not request Fargate resources in the subsequent CE to run your jobs.\nConnecting an AWS Batch job queue to Fargate/Fargate Spot CE and an EC2 or Spot CE is not allowed.\n\nAWS Batch handles job execution and compute resource management, allowing you to focus on developing applications or analyzing results instead of setting up and managing infrastructure. If you are considering running or moving batch workloads to AWS, you should consider using AWS Batch.\n\nAWS Batch is optimized for batch computing and applications that scale through the execution of multiple jobs in parallel. Deep learning, genomics analysis, financial risk models, Monte Carlo simulations, animation rendering, media transcoding, image processing, and engineering simulations are all excellent examples of batch computing applications.\n\n", "question": "Can you set a maximum vCPU for a Fargate CE?", "answer": "Yes, you can set Fargate CE's to have a max vCPU, which is the total amount of vCPU of all the jobs currently running in that CE.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "batch-faq-2", "model": "llama-3.1-8b", "chunk": "Yes. You can set Fargate CE’s to have a max vCPU, which is the total amount of vCPU of all the jobs currently running in that CE. When your vCPU count hits max vCPU in a CE, Batch will begin scheduling jobs on the next Fargate CE in order attached to the queue, if there is one. This is useful if, for example, you want to set a Fargate CE to some minimum business requirement, then run the rest of your workload on Fargate Spot.\nWhile setting a Fargate Spot CE as first, followed by a Fargate CE, Batch will only spill over into Fargate when the vCPU used by your jobs is greater than max vCPU for that CE. In the event that Fargate Spot is reclaimed, max vCPU will not be met and Batch will not request Fargate resources in the subsequent CE to run your jobs.\nConnecting an AWS Batch job queue to Fargate/Fargate Spot CE and an EC2 or Spot CE is not allowed.\n\nAWS Batch handles job execution and compute resource management, allowing you to focus on developing applications or analyzing results instead of setting up and managing infrastructure. If you are considering running or moving batch workloads to AWS, you should consider using AWS Batch.\n\nAWS Batch is optimized for batch computing and applications that scale through the execution of multiple jobs in parallel. Deep learning, genomics analysis, financial risk models, Monte Carlo simulations, animation rendering, media transcoding, image processing, and engineering simulations are all excellent examples of batch computing applications.\n\n", "question": "How does AWS Batch handle job execution and compute resource management?", "answer": "AWS Batch handles job execution and compute resource management, allowing you to focus on developing applications or analyzing results instead of setting up and managing infrastructure.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "batch-faq-2", "model": "llama-3.1-8b", "chunk": "Yes. You can set Fargate CE’s to have a max vCPU, which is the total amount of vCPU of all the jobs currently running in that CE. When your vCPU count hits max vCPU in a CE, Batch will begin scheduling jobs on the next Fargate CE in order attached to the queue, if there is one. This is useful if, for example, you want to set a Fargate CE to some minimum business requirement, then run the rest of your workload on Fargate Spot.\nWhile setting a Fargate Spot CE as first, followed by a Fargate CE, Batch will only spill over into Fargate when the vCPU used by your jobs is greater than max vCPU for that CE. In the event that Fargate Spot is reclaimed, max vCPU will not be met and Batch will not request Fargate resources in the subsequent CE to run your jobs.\nConnecting an AWS Batch job queue to Fargate/Fargate Spot CE and an EC2 or Spot CE is not allowed.\n\nAWS Batch handles job execution and compute resource management, allowing you to focus on developing applications or analyzing results instead of setting up and managing infrastructure. If you are considering running or moving batch workloads to AWS, you should consider using AWS Batch.\n\nAWS Batch is optimized for batch computing and applications that scale through the execution of multiple jobs in parallel. Deep learning, genomics analysis, financial risk models, Monte Carlo simulations, animation rendering, media transcoding, image processing, and engineering simulations are all excellent examples of batch computing applications.\n\n", "question": "What happens when the vCPU count hits the max vCPU in a CE?", "answer": "When your vCPU count hits max vCPU in a CE, Batch will begin scheduling jobs on the next Fargate CE in order attached to the queue, if there is one.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "bedrock-faq-2", "model": "llama-3.1-8b", "chunk": "Data security and compliance certifications: Amazon Bedrock offers several capabilities to support security and privacy requirements. Amazon Bedrock is in scope for common compliance standards such as Service and Organization Control (SOC), International Organization for Standardization (ISO), is Health Insurance Portability and Accountability Act (HIPAA) eligible, and customers can use Amazon Bedrock in compliance with the General Data Protection Regulation (GDPR). Amazon Bedrock is CSA Security Trust Assurance and Risk (STAR) Level 2 certified, which validates the use of best practices and the security posture of AWS cloud offerings. With Amazon Bedrock, your content is not used to improve the base models and is not shared with any model providers. Your data in Amazon Bedrock is always encrypted in transit and at rest, and you can optionally encrypt the data using your own keys. You can use AWS PrivateLink with Amazon Bedrock to establish private connectivity between your FMs and your Amazon Virtual Private Cloud (Amazon VPC) without exposing your traffic to the Internet.\n\n", "question": "What compliance standards is Amazon Bedrock in scope for?", "answer": "common compliance standards such as Service and Organization Control (SOC), International Organization for Standardization (ISO), is Health Insurance Portability and Accountability Act (HIPAA) eligible, and customers can use Amazon Bedrock in compliance with the General Data Protection Regulation (GDPR).", "question_type": "", "original_pair_id": ""}
{"chunk_id": "bedrock-faq-2", "model": "llama-3.1-8b", "chunk": "Data security and compliance certifications: Amazon Bedrock offers several capabilities to support security and privacy requirements. Amazon Bedrock is in scope for common compliance standards such as Service and Organization Control (SOC), International Organization for Standardization (ISO), is Health Insurance Portability and Accountability Act (HIPAA) eligible, and customers can use Amazon Bedrock in compliance with the General Data Protection Regulation (GDPR). Amazon Bedrock is CSA Security Trust Assurance and Risk (STAR) Level 2 certified, which validates the use of best practices and the security posture of AWS cloud offerings. With Amazon Bedrock, your content is not used to improve the base models and is not shared with any model providers. Your data in Amazon Bedrock is always encrypted in transit and at rest, and you can optionally encrypt the data using your own keys. You can use AWS PrivateLink with Amazon Bedrock to establish private connectivity between your FMs and your Amazon Virtual Private Cloud (Amazon VPC) without exposing your traffic to the Internet.\n\n", "question": "How does Amazon Bedrock ensure data security and privacy?", "answer": "With Amazon Bedrock, your content is not used to improve the base models and is not shared with any model providers. Your data in Amazon Bedrock is always encrypted in transit and at rest, and you can optionally encrypt the data using your own keys.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "bedrock-faq-2", "model": "llama-3.1-8b", "chunk": "Data security and compliance certifications: Amazon Bedrock offers several capabilities to support security and privacy requirements. Amazon Bedrock is in scope for common compliance standards such as Service and Organization Control (SOC), International Organization for Standardization (ISO), is Health Insurance Portability and Accountability Act (HIPAA) eligible, and customers can use Amazon Bedrock in compliance with the General Data Protection Regulation (GDPR). Amazon Bedrock is CSA Security Trust Assurance and Risk (STAR) Level 2 certified, which validates the use of best practices and the security posture of AWS cloud offerings. With Amazon Bedrock, your content is not used to improve the base models and is not shared with any model providers. Your data in Amazon Bedrock is always encrypted in transit and at rest, and you can optionally encrypt the data using your own keys. You can use AWS PrivateLink with Amazon Bedrock to establish private connectivity between your FMs and your Amazon Virtual Private Cloud (Amazon VPC) without exposing your traffic to the Internet.\n\n", "question": "How does Amazon Bedrock's encryption compare to using your own keys?", "answer": "Your data in Amazon Bedrock is always encrypted in transit and at rest, and you can optionally encrypt the data using your own keys.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "certificate-faq-3", "model": "llama-3.1-8b", "chunk": "ACM enables you to manage the lifecycle of your public and private certificates. ACM’s capabilities depend on whether the certificate is public or private, how you obtain the certificate, and where you deploy it.\nPublic certificates\n- You can request Amazon-issued public certificates in ACM. ACM manages the renewal and deployment of public certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway.\nExportable public certificates\n- ACM manages the renewal of exported public certificates that you may deploy with AWS services and/or in your on-premises environments.\nPrivate certificates\n– You can choose to delegate private certificate management to ACM. When used in this way, ACM can automatically renew and deploy private certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway. You can easily deploy these private certificates using the AWS Management console, APIs, and command-line interface (CLI). You can export private certificates from ACM and use them with EC2 instances, containers, on-premises servers, and IoT devices. AWS Private CA automatically renews these certificates and sends an Amazon CloudWatch notification when the renewal is completed. You can write client-side code to download renewed certificates and private keys and deploy them with your application.\nImported certificates\n– If you want to use a third-party certificate with Amazon CloudFront, Elastic Load Balancing, or Amazon API Gateway, you may import it into ACM using the AWS Management Console, AWS CLI, or ACM APIs. ACM can not renew imported certificates, but it can help you manage the renewal process. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire. You can use\nACM CloudWatch metrics\nto monitor the expiration dates of an imported certificates and import a new third-party certificate to replace an expiring one.\n\n", "question": "Can ACM manage the renewal of exported public certificates?", "answer": "Yes, ACM manages the renewal of exported public certificates that you may deploy with AWS services and/or in your on-premises environments.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "certificate-faq-3", "model": "llama-3.1-8b", "chunk": "ACM enables you to manage the lifecycle of your public and private certificates. ACM’s capabilities depend on whether the certificate is public or private, how you obtain the certificate, and where you deploy it.\nPublic certificates\n- You can request Amazon-issued public certificates in ACM. ACM manages the renewal and deployment of public certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway.\nExportable public certificates\n- ACM manages the renewal of exported public certificates that you may deploy with AWS services and/or in your on-premises environments.\nPrivate certificates\n– You can choose to delegate private certificate management to ACM. When used in this way, ACM can automatically renew and deploy private certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway. You can easily deploy these private certificates using the AWS Management console, APIs, and command-line interface (CLI). You can export private certificates from ACM and use them with EC2 instances, containers, on-premises servers, and IoT devices. AWS Private CA automatically renews these certificates and sends an Amazon CloudWatch notification when the renewal is completed. You can write client-side code to download renewed certificates and private keys and deploy them with your application.\nImported certificates\n– If you want to use a third-party certificate with Amazon CloudFront, Elastic Load Balancing, or Amazon API Gateway, you may import it into ACM using the AWS Management Console, AWS CLI, or ACM APIs. ACM can not renew imported certificates, but it can help you manage the renewal process. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire. You can use\nACM CloudWatch metrics\nto monitor the expiration dates of an imported certificates and import a new third-party certificate to replace an expiring one.\n\n", "question": "How does ACM help with private certificate management when it's delegated?", "answer": "When you delegate private certificate management to ACM, it can automatically renew and deploy private certificates used with ACM-integrated services.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "certificate-faq-3", "model": "llama-3.1-8b", "chunk": "ACM enables you to manage the lifecycle of your public and private certificates. ACM’s capabilities depend on whether the certificate is public or private, how you obtain the certificate, and where you deploy it.\nPublic certificates\n- You can request Amazon-issued public certificates in ACM. ACM manages the renewal and deployment of public certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway.\nExportable public certificates\n- ACM manages the renewal of exported public certificates that you may deploy with AWS services and/or in your on-premises environments.\nPrivate certificates\n– You can choose to delegate private certificate management to ACM. When used in this way, ACM can automatically renew and deploy private certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway. You can easily deploy these private certificates using the AWS Management console, APIs, and command-line interface (CLI). You can export private certificates from ACM and use them with EC2 instances, containers, on-premises servers, and IoT devices. AWS Private CA automatically renews these certificates and sends an Amazon CloudWatch notification when the renewal is completed. You can write client-side code to download renewed certificates and private keys and deploy them with your application.\nImported certificates\n– If you want to use a third-party certificate with Amazon CloudFront, Elastic Load Balancing, or Amazon API Gateway, you may import it into ACM using the AWS Management Console, AWS CLI, or ACM APIs. ACM can not renew imported certificates, but it can help you manage the renewal process. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire. You can use\nACM CloudWatch metrics\nto monitor the expiration dates of an imported certificates and import a new third-party certificate to replace an expiring one.\n\n", "question": "What's the difference between how ACM handles Amazon-issued public certificates and imported certificates?", "answer": "ACM manages the renewal of Amazon-issued public certificates, but it cannot renew imported certificates and instead helps you manage the renewal process.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "chatbot-faq-2", "model": "llama-3.1-8b", "chunk": "You can provision Microsoft Teams and Slack channel configurations using AWS CLI, AWS CloudFormation, AWS Cloud Control APIs, and SDKs. Terraform users can use AWS provider to manage Chatbot channel configurations.\n\nAWS Chatbot is a global service and can be used in all commercial AWS Regions. You can combine Amazon SNS topics from multiple Regions in a single AWS Chatbot configuration. Visit the\nAWS Regional Product and Services table\nfor details about availability.\n\nAWS Chatbot is a global service and we may store or process customer information, such as Chatbot configurations and permissions, Microsoft Teams team identifiers, Slack workspace identifiers and channel names, notifications, user inputs, and AWS Chatbot generated responses and images, in any of the commercial AWS Regions.\nYou can request deletion of data used for analytics and to improve the quality of service associated with your account by contacting AWS Support. Your trust, your privacy, and the security of your data are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your data and ensure that our use complies with our commitments to you. See\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\nWhen you opt out of use of your data to improve and develop the quality of AWS Chatbot and other Amazon machine-learning/artificial-intelligence technologies, your data will be removed from all AWS Regions. For information about how to opt out, contact AWS Support.\n\nYou can customize AWS Chatbot to suit your ChatOps use cases. You can designate different channels to monitor and operate different aspects of your cloud applications. You can operate resources across multiple accounts and regions from a channel. With IAM-based permissions, guarails, and Service Control Policies (SCPs), you can decide the type of the actions channel members can take from chat channels.\nYou can also send custom notifications so that you are informed about the state of your resources and applications. You can also customize action buttons on notifications and configure command aliases so that you can quickly run commands to dignose and remediate issues.\n\n", "question": "What services can be used to provision Microsoft Teams and Slack channel configurations?", "answer": "You can provision Microsoft Teams and Slack channel configurations using AWS CLI, AWS CloudFormation, AWS Cloud Control APIs, and SDKs.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "chatbot-faq-2", "model": "llama-3.1-8b", "chunk": "You can provision Microsoft Teams and Slack channel configurations using AWS CLI, AWS CloudFormation, AWS Cloud Control APIs, and SDKs. Terraform users can use AWS provider to manage Chatbot channel configurations.\n\nAWS Chatbot is a global service and can be used in all commercial AWS Regions. You can combine Amazon SNS topics from multiple Regions in a single AWS Chatbot configuration. Visit the\nAWS Regional Product and Services table\nfor details about availability.\n\nAWS Chatbot is a global service and we may store or process customer information, such as Chatbot configurations and permissions, Microsoft Teams team identifiers, Slack workspace identifiers and channel names, notifications, user inputs, and AWS Chatbot generated responses and images, in any of the commercial AWS Regions.\nYou can request deletion of data used for analytics and to improve the quality of service associated with your account by contacting AWS Support. Your trust, your privacy, and the security of your data are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your data and ensure that our use complies with our commitments to you. See\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\nWhen you opt out of use of your data to improve and develop the quality of AWS Chatbot and other Amazon machine-learning/artificial-intelligence technologies, your data will be removed from all AWS Regions. For information about how to opt out, contact AWS Support.\n\nYou can customize AWS Chatbot to suit your ChatOps use cases. You can designate different channels to monitor and operate different aspects of your cloud applications. You can operate resources across multiple accounts and regions from a channel. With IAM-based permissions, guarails, and Service Control Policies (SCPs), you can decide the type of the actions channel members can take from chat channels.\nYou can also send custom notifications so that you are informed about the state of your resources and applications. You can also customize action buttons on notifications and configure command aliases so that you can quickly run commands to dignose and remediate issues.\n\n", "question": "How does AWS Chatbot manage user data?", "answer": "AWS Chatbot may store or process customer information, such as Chatbot configurations and permissions, Microsoft Teams team identifiers, Slack workspace identifiers and channel names, notifications, user inputs, and AWS Chatbot generated responses and images, in any of the commercial AWS Regions.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "chatbot-faq-2", "model": "llama-3.1-8b", "chunk": "You can provision Microsoft Teams and Slack channel configurations using AWS CLI, AWS CloudFormation, AWS Cloud Control APIs, and SDKs. Terraform users can use AWS provider to manage Chatbot channel configurations.\n\nAWS Chatbot is a global service and can be used in all commercial AWS Regions. You can combine Amazon SNS topics from multiple Regions in a single AWS Chatbot configuration. Visit the\nAWS Regional Product and Services table\nfor details about availability.\n\nAWS Chatbot is a global service and we may store or process customer information, such as Chatbot configurations and permissions, Microsoft Teams team identifiers, Slack workspace identifiers and channel names, notifications, user inputs, and AWS Chatbot generated responses and images, in any of the commercial AWS Regions.\nYou can request deletion of data used for analytics and to improve the quality of service associated with your account by contacting AWS Support. Your trust, your privacy, and the security of your data are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your data and ensure that our use complies with our commitments to you. See\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\nWhen you opt out of use of your data to improve and develop the quality of AWS Chatbot and other Amazon machine-learning/artificial-intelligence technologies, your data will be removed from all AWS Regions. For information about how to opt out, contact AWS Support.\n\nYou can customize AWS Chatbot to suit your ChatOps use cases. You can designate different channels to monitor and operate different aspects of your cloud applications. You can operate resources across multiple accounts and regions from a channel. With IAM-based permissions, guarails, and Service Control Policies (SCPs), you can decide the type of the actions channel members can take from chat channels.\nYou can also send custom notifications so that you are informed about the state of your resources and applications. You can also customize action buttons on notifications and configure command aliases so that you can quickly run commands to dignose and remediate issues.\n\n", "question": "What's the difference between the regions where AWS Chatbot stores data and the regions where it can be used?", "answer": "AWS Chatbot is a global service and can be used in all commercial AWS Regions, but it may store or process customer information in any of the commercial AWS Regions.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "cloud-faq-2", "model": "llama-3.1-8b", "chunk": "We have coverage for many common AWS services and features with rich, high-level constructs, and complete coverage of the lower-level CloudFormation resources, including new resources shortly after they are available. We keep the AWS Construct Library up to date by autogenerating the resource-level APIs every time the CloudFormation specification changes. On top of these autogenerated APIs, we provide handcrafted, higher-level abstractions that make it even easier to work with each service. We do not have this high-level, convenient abstraction for every AWS service yet, but are adding new features all the time with a focus on services most used in modern cloud applications. Visit the\nAWS Construct Library API reference\nto learn more.\n\nWe keep the AWS Construct Library in sync with the CloudFormation resource specification by adding new CloudFormation features to AWS CDK shortly after their publication.\n\nAWS CDK construct libraries are just like any other library. They are consumed through the package manager of the programming language you use; keeping those libraries up to date is part of your normal workflow. All packages support semantic versioning, allowing you to make conscious choices about when to migrate to new infrastructure models.\n\nThink of the AWS CDK as a developer-centric toolkit leveraging the full power of modern programming languages to define your AWS infrastructure as code. When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection.\n\n", "question": "How often are new CloudFormation resources autogenerated in the AWS Construct Library?", "answer": "Every time the CloudFormation specification changes.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "cloud-faq-2", "model": "llama-3.1-8b", "chunk": "We have coverage for many common AWS services and features with rich, high-level constructs, and complete coverage of the lower-level CloudFormation resources, including new resources shortly after they are available. We keep the AWS Construct Library up to date by autogenerating the resource-level APIs every time the CloudFormation specification changes. On top of these autogenerated APIs, we provide handcrafted, higher-level abstractions that make it even easier to work with each service. We do not have this high-level, convenient abstraction for every AWS service yet, but are adding new features all the time with a focus on services most used in modern cloud applications. Visit the\nAWS Construct Library API reference\nto learn more.\n\nWe keep the AWS Construct Library in sync with the CloudFormation resource specification by adding new CloudFormation features to AWS CDK shortly after their publication.\n\nAWS CDK construct libraries are just like any other library. They are consumed through the package manager of the programming language you use; keeping those libraries up to date is part of your normal workflow. All packages support semantic versioning, allowing you to make conscious choices about when to migrate to new infrastructure models.\n\nThink of the AWS CDK as a developer-centric toolkit leveraging the full power of modern programming languages to define your AWS infrastructure as code. When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection.\n\n", "question": "How does the AWS Construct Library keep its resources in sync with the CloudFormation resource specification?", "answer": "By adding new CloudFormation features to AWS CDK shortly after their publication.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "cloud-faq-2", "model": "llama-3.1-8b", "chunk": "We have coverage for many common AWS services and features with rich, high-level constructs, and complete coverage of the lower-level CloudFormation resources, including new resources shortly after they are available. We keep the AWS Construct Library up to date by autogenerating the resource-level APIs every time the CloudFormation specification changes. On top of these autogenerated APIs, we provide handcrafted, higher-level abstractions that make it even easier to work with each service. We do not have this high-level, convenient abstraction for every AWS service yet, but are adding new features all the time with a focus on services most used in modern cloud applications. Visit the\nAWS Construct Library API reference\nto learn more.\n\nWe keep the AWS Construct Library in sync with the CloudFormation resource specification by adding new CloudFormation features to AWS CDK shortly after their publication.\n\nAWS CDK construct libraries are just like any other library. They are consumed through the package manager of the programming language you use; keeping those libraries up to date is part of your normal workflow. All packages support semantic versioning, allowing you to make conscious choices about when to migrate to new infrastructure models.\n\nThink of the AWS CDK as a developer-centric toolkit leveraging the full power of modern programming languages to define your AWS infrastructure as code. When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection.\n\n", "question": "What benefits do AWS CDK applications still enjoy when they are run and compile down to CloudFormation JSON/YAML templates?", "answer": "Safe deployment, automatic rollback, and drift detection, just like when using CloudFormation directly.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "cloudformation-faq-1", "model": "llama-3.1-8b", "chunk": "These services are designed to complement each other.\nAWS Elastic Beanstalk\nprovides an environment where you can easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for managing application lifecycle. If your application workloads can be managed as Elastic Beanstalk workloads, you can enjoy a more turn-key experience in creating and updating applications. Behind the scenes, Elastic Beanstalk uses CloudFormation to create and maintain resources. If your application requirements dictate more custom control, the additional functionality of CloudFormation gives you more options to control your workloads.\nAWS CloudFormation is a convenient provisioning mechanism for a broad range of\nAWS\nand third-party resources. It supports the infrastructure needs of many different types of applications such as existing enterprise applications, legacy applications, applications built using a variety of AWS resources, and container-based solutions (including those built using AWS Elastic Beanstalk).\nAWS CloudFormation supports Elastic Beanstalk application environments as one of the AWS resource types. This allows you, for example, to create and manage an AWS Elastic Beanstalk–hosted application along with an RDS database to store the application data. Any other supported AWS resource can be added to the group as well.\n\nCloudFormation introduces four concepts: A template is a JSON or YAML declarative code file that describes the intended state of all the resources you need to deploy your application. A stack implements and manages the group of resources outlined in your template, and allows the state and dependencies of those resources to be managed together. A change set is a preview of changes that will be executed by stack operations to create, update, or remove resources. A stack set is a group of stacks you manage together that can replicate a group.\n\nTo see a complete list of supported\nAWS resources\nand their features, visit the Supported AWS Services page in the Release History of the documentation.\nThe\nAWS CloudFormation Registry\nand AWS CloudFormation\ncustom resources\nenable management of additional AWS and third party resources.\n\n", "question": "What is the purpose of AWS Elastic Beanstalk?", "answer": "AWS Elastic Beanstalk provides an environment where you can easily deploy and run applications in the cloud, integrated with developer tools and offering a one-stop experience for managing application lifecycle.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "cloudformation-faq-1", "model": "llama-3.1-8b", "chunk": "These services are designed to complement each other.\nAWS Elastic Beanstalk\nprovides an environment where you can easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for managing application lifecycle. If your application workloads can be managed as Elastic Beanstalk workloads, you can enjoy a more turn-key experience in creating and updating applications. Behind the scenes, Elastic Beanstalk uses CloudFormation to create and maintain resources. If your application requirements dictate more custom control, the additional functionality of CloudFormation gives you more options to control your workloads.\nAWS CloudFormation is a convenient provisioning mechanism for a broad range of\nAWS\nand third-party resources. It supports the infrastructure needs of many different types of applications such as existing enterprise applications, legacy applications, applications built using a variety of AWS resources, and container-based solutions (including those built using AWS Elastic Beanstalk).\nAWS CloudFormation supports Elastic Beanstalk application environments as one of the AWS resource types. This allows you, for example, to create and manage an AWS Elastic Beanstalk–hosted application along with an RDS database to store the application data. Any other supported AWS resource can be added to the group as well.\n\nCloudFormation introduces four concepts: A template is a JSON or YAML declarative code file that describes the intended state of all the resources you need to deploy your application. A stack implements and manages the group of resources outlined in your template, and allows the state and dependencies of those resources to be managed together. A change set is a preview of changes that will be executed by stack operations to create, update, or remove resources. A stack set is a group of stacks you manage together that can replicate a group.\n\nTo see a complete list of supported\nAWS resources\nand their features, visit the Supported AWS Services page in the Release History of the documentation.\nThe\nAWS CloudFormation Registry\nand AWS CloudFormation\ncustom resources\nenable management of additional AWS and third party resources.\n\n", "question": "How does CloudFormation work in conjunction with Elastic Beanstalk?", "answer": "Behind the scenes, Elastic Beanstalk uses CloudFormation to create and maintain resources, giving you more options to control your workloads if your application requirements dictate more custom control.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "cloudformation-faq-1", "model": "llama-3.1-8b", "chunk": "These services are designed to complement each other.\nAWS Elastic Beanstalk\nprovides an environment where you can easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for managing application lifecycle. If your application workloads can be managed as Elastic Beanstalk workloads, you can enjoy a more turn-key experience in creating and updating applications. Behind the scenes, Elastic Beanstalk uses CloudFormation to create and maintain resources. If your application requirements dictate more custom control, the additional functionality of CloudFormation gives you more options to control your workloads.\nAWS CloudFormation is a convenient provisioning mechanism for a broad range of\nAWS\nand third-party resources. It supports the infrastructure needs of many different types of applications such as existing enterprise applications, legacy applications, applications built using a variety of AWS resources, and container-based solutions (including those built using AWS Elastic Beanstalk).\nAWS CloudFormation supports Elastic Beanstalk application environments as one of the AWS resource types. This allows you, for example, to create and manage an AWS Elastic Beanstalk–hosted application along with an RDS database to store the application data. Any other supported AWS resource can be added to the group as well.\n\nCloudFormation introduces four concepts: A template is a JSON or YAML declarative code file that describes the intended state of all the resources you need to deploy your application. A stack implements and manages the group of resources outlined in your template, and allows the state and dependencies of those resources to be managed together. A change set is a preview of changes that will be executed by stack operations to create, update, or remove resources. A stack set is a group of stacks you manage together that can replicate a group.\n\nTo see a complete list of supported\nAWS resources\nand their features, visit the Supported AWS Services page in the Release History of the documentation.\nThe\nAWS CloudFormation Registry\nand AWS CloudFormation\ncustom resources\nenable management of additional AWS and third party resources.\n\n", "question": "What is the difference between a stack and a stack set in CloudFormation?", "answer": "A stack implements and manages the group of resources outlined in your template, allowing the state and dependencies of those resources to be managed together, whereas a stack set is a group of stacks you manage together that can replicate a group.", "question_type": "", "original_pair_id": ""}
{"chunk_id": "amazon-faq-6", "model": "gpt-4o-mini", "chunk": "With Amazon MQ, you pay only for what you use. You are charged for the broker instance usage, storage usage, and standard data transfer fees.\nSee\nAmazon MQ pricing\nfor details.\nStarting July 15, 2025, new AWS customers will receive up to $200 in AWS Free Tier credits, which can be applied towards eligible AWS services, including Amazon MQ. At account sign-up, you can choose between a free plan and a paid plan. The free plan will be available for 6 months after account creation. If you upgrade to a paid plan, any remaining Free Tier credit balance will automatically apply to your AWS bills. All Free Tier credits must be used within 12 months of your account creation date. To learn more about the AWS Free Tier program, refer to\nAWS Free Tier website\nand\nAWS Free Tier documentation\n.\n\nNo, all inter-node data transfer is included with the service at no additional charge.\n\nNo, Amazon MQ does not offer reserved instance pricing.\n\nNo, each broker you provision includes boot volume storage managed by the Amazon MQ service.\n\nAmazon MQ is compliant or eligible for the following programs:\nHIPAA eligible\nPCI DSS compliant\nSOC 1,2,3 compliant\nISO 9001, 27001, 27017, and 27018 certified\nFor a complete list of AWS services and compliance programs, please see\nAWS Services in Scope by Compliance Program.\n\n", "question": "What are the charges associated with using Amazon MQ?", "answer": "With Amazon MQ, you are charged for broker instance usage, storage usage, and standard data transfer fees.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "amazon-faq-6", "model": "gpt-4o-mini", "chunk": "With Amazon MQ, you pay only for what you use. You are charged for the broker instance usage, storage usage, and standard data transfer fees.\nSee\nAmazon MQ pricing\nfor details.\nStarting July 15, 2025, new AWS customers will receive up to $200 in AWS Free Tier credits, which can be applied towards eligible AWS services, including Amazon MQ. At account sign-up, you can choose between a free plan and a paid plan. The free plan will be available for 6 months after account creation. If you upgrade to a paid plan, any remaining Free Tier credit balance will automatically apply to your AWS bills. All Free Tier credits must be used within 12 months of your account creation date. To learn more about the AWS Free Tier program, refer to\nAWS Free Tier website\nand\nAWS Free Tier documentation\n.\n\nNo, all inter-node data transfer is included with the service at no additional charge.\n\nNo, Amazon MQ does not offer reserved instance pricing.\n\nNo, each broker you provision includes boot volume storage managed by the Amazon MQ service.\n\nAmazon MQ is compliant or eligible for the following programs:\nHIPAA eligible\nPCI DSS compliant\nSOC 1,2,3 compliant\nISO 9001, 27001, 27017, and 27018 certified\nFor a complete list of AWS services and compliance programs, please see\nAWS Services in Scope by Compliance Program.\n\n", "question": "What options do new AWS customers have regarding the AWS Free Tier when signing up?", "answer": "New AWS customers can choose between a free plan and a paid plan at account sign-up. The free plan is available for 6 months after account creation, and any remaining Free Tier credit balance will automatically apply to AWS bills if the customer upgrades to a paid plan.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "amazon-faq-6", "model": "gpt-4o-mini", "chunk": "With Amazon MQ, you pay only for what you use. You are charged for the broker instance usage, storage usage, and standard data transfer fees.\nSee\nAmazon MQ pricing\nfor details.\nStarting July 15, 2025, new AWS customers will receive up to $200 in AWS Free Tier credits, which can be applied towards eligible AWS services, including Amazon MQ. At account sign-up, you can choose between a free plan and a paid plan. The free plan will be available for 6 months after account creation. If you upgrade to a paid plan, any remaining Free Tier credit balance will automatically apply to your AWS bills. All Free Tier credits must be used within 12 months of your account creation date. To learn more about the AWS Free Tier program, refer to\nAWS Free Tier website\nand\nAWS Free Tier documentation\n.\n\nNo, all inter-node data transfer is included with the service at no additional charge.\n\nNo, Amazon MQ does not offer reserved instance pricing.\n\nNo, each broker you provision includes boot volume storage managed by the Amazon MQ service.\n\nAmazon MQ is compliant or eligible for the following programs:\nHIPAA eligible\nPCI DSS compliant\nSOC 1,2,3 compliant\nISO 9001, 27001, 27017, and 27018 certified\nFor a complete list of AWS services and compliance programs, please see\nAWS Services in Scope by Compliance Program.\n\n", "question": "How does Amazon MQ's pricing model differ from reserved instance pricing?", "answer": "Amazon MQ does not offer reserved instance pricing, which means that users are charged only for what they use without the option to reserve instances for a lower cost.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "amplify-faq-7", "model": "gpt-4o-mini", "chunk": "Prices are the same across all regions.\n\n", "question": "Are prices consistent across different AWS regions?", "answer": "Yes, prices are the same across all regions.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "amplify-faq-7", "model": "gpt-4o-mini", "chunk": "Prices are the same across all regions.\n\n", "question": "What does it mean for prices to be the same across all regions in AWS?", "answer": "It means that regardless of the region you choose to deploy your services or resources in AWS, the cost for those services or resources will not vary; they remain uniform across all locations.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "amplify-faq-7", "model": "gpt-4o-mini", "chunk": "Prices are the same across all regions.\n\n", "question": "How do AWS pricing structures differ when comparing regions?", "answer": "There are no differences in AWS pricing structures when comparing regions, as prices are the same across all regions.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "api-faq-3", "model": "gpt-4o-mini", "chunk": "Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs.\nHTTP API\n: HTTP APIs are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads. They do not currently offer API management functionality.\nREST API\n: REST APIs offer API proxy functionality and API management features in a single solution. REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs.\nWebSocket API:\nWebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. With WebSocket APIs in API Gateway, you can define backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint to be invoked when messages are received from the connected clients.\n\nTo get started with HTTP APIs, you can use the Amazon API Gateway console, the AWS CLI, AWS SDKs, or AWS CloudFormation. To learn more about getting started with HTTP APIs, visit our\ndocumentation\n.\n\nTo get started with REST APIs, you can use the Amazon API Gateway console, the AWS CLI, or AWS SDKs. To learn more about getting started with REST APIs, visit our\ndocumentation\n.\n\n", "question": "What are the two options offered by Amazon API Gateway for creating RESTful APIs?", "answer": "Amazon API Gateway offers two options for creating RESTful APIs: HTTP APIs and REST APIs.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "api-faq-3", "model": "gpt-4o-mini", "chunk": "Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs.\nHTTP API\n: HTTP APIs are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads. They do not currently offer API management functionality.\nREST API\n: REST APIs offer API proxy functionality and API management features in a single solution. REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs.\nWebSocket API:\nWebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. With WebSocket APIs in API Gateway, you can define backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint to be invoked when messages are received from the connected clients.\n\nTo get started with HTTP APIs, you can use the Amazon API Gateway console, the AWS CLI, AWS SDKs, or AWS CloudFormation. To learn more about getting started with HTTP APIs, visit our\ndocumentation\n.\n\nTo get started with REST APIs, you can use the Amazon API Gateway console, the AWS CLI, or AWS SDKs. To learn more about getting started with REST APIs, visit our\ndocumentation\n.\n\n", "question": "What is the main difference between HTTP APIs and REST APIs in terms of features?", "answer": "HTTP APIs are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends and do not offer API management functionality. In contrast, REST APIs provide API proxy functionality along with API management features such as usage plans, API keys, publishing, and monetizing APIs.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "api-faq-3", "model": "gpt-4o-mini", "chunk": "Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs.\nHTTP API\n: HTTP APIs are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads. They do not currently offer API management functionality.\nREST API\n: REST APIs offer API proxy functionality and API management features in a single solution. REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs.\nWebSocket API:\nWebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. With WebSocket APIs in API Gateway, you can define backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint to be invoked when messages are received from the connected clients.\n\nTo get started with HTTP APIs, you can use the Amazon API Gateway console, the AWS CLI, AWS SDKs, or AWS CloudFormation. To learn more about getting started with HTTP APIs, visit our\ndocumentation\n.\n\nTo get started with REST APIs, you can use the Amazon API Gateway console, the AWS CLI, or AWS SDKs. To learn more about getting started with REST APIs, visit our\ndocumentation\n.\n\n", "question": "How do WebSocket APIs differ from HTTP APIs and REST APIs in terms of communication?", "answer": "WebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication, whereas HTTP APIs and REST APIs do not focus on real-time communication and instead are designed for request-response interactions.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "appflow-faq-0", "model": "gpt-4o-mini", "chunk": "Analytics\n›\nAmazon Appflow\n›\nFAQs\n\nGeneral\n7\nSaaS Integrations\n4\nPrivacy and Security\n2\nAWS Services\n5\n\nPrivacy and Security\n\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. With AppFlow, you can run data flows at nearly any scale at the frequency you choose - on a schedule, in response to a business event, or on demand. You can configure powerful data transformation capabilities like filtering and validation to generate rich, ready-to-use data as part of the flow itself, without additional steps. AppFlow automatically encrypts data in motion, and allows users to restrict data from flowing over the public Internet for SaaS applications that are integrated with AWS PrivateLink, reducing exposure to security threats.\n\n", "question": "What is Amazon AppFlow?", "answer": "Amazon AppFlow is a fully managed integration service that enables users to securely transfer data between Software-as-a-Service (SaaS) applications, such as Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "appflow-faq-0", "model": "gpt-4o-mini", "chunk": "Analytics\n›\nAmazon Appflow\n›\nFAQs\n\nGeneral\n7\nSaaS Integrations\n4\nPrivacy and Security\n2\nAWS Services\n5\n\nPrivacy and Security\n\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. With AppFlow, you can run data flows at nearly any scale at the frequency you choose - on a schedule, in response to a business event, or on demand. You can configure powerful data transformation capabilities like filtering and validation to generate rich, ready-to-use data as part of the flow itself, without additional steps. AppFlow automatically encrypts data in motion, and allows users to restrict data from flowing over the public Internet for SaaS applications that are integrated with AWS PrivateLink, reducing exposure to security threats.\n\n", "question": "How does Amazon AppFlow ensure data security during transfers?", "answer": "Amazon AppFlow ensures data security during transfers by automatically encrypting data in motion and allowing users to restrict data from flowing over the public Internet for SaaS applications that are integrated with AWS PrivateLink, thus reducing exposure to security threats.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "appflow-faq-0", "model": "gpt-4o-mini", "chunk": "Analytics\n›\nAmazon Appflow\n›\nFAQs\n\nGeneral\n7\nSaaS Integrations\n4\nPrivacy and Security\n2\nAWS Services\n5\n\nPrivacy and Security\n\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. With AppFlow, you can run data flows at nearly any scale at the frequency you choose - on a schedule, in response to a business event, or on demand. You can configure powerful data transformation capabilities like filtering and validation to generate rich, ready-to-use data as part of the flow itself, without additional steps. AppFlow automatically encrypts data in motion, and allows users to restrict data from flowing over the public Internet for SaaS applications that are integrated with AWS PrivateLink, reducing exposure to security threats.\n\n", "question": "What are the differences between running data flows on a schedule versus on demand in Amazon AppFlow?", "answer": "Running data flows on a schedule in Amazon AppFlow allows users to execute data transfers at predetermined times, while running them on demand enables users to initiate data transfers whenever they choose, in response to specific business events or needs.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "application-faq-3", "model": "gpt-4o-mini", "chunk": "Please refer to the\nAWS Regional Services List\nfor the most up-to-date information.\n\n", "question": "Where can I find the most up-to-date information on AWS regional services?", "answer": "You can find the most up-to-date information on AWS regional services in the AWS Regional Services List.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "application-faq-3", "model": "gpt-4o-mini", "chunk": "Please refer to the\nAWS Regional Services List\nfor the most up-to-date information.\n\n", "question": "Why is it important to refer to the AWS Regional Services List?", "answer": "It is important to refer to the AWS Regional Services List to ensure you have the latest information regarding the availability of AWS services in different regions.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "application-faq-3", "model": "gpt-4o-mini", "chunk": "Please refer to the\nAWS Regional Services List\nfor the most up-to-date information.\n\n", "question": "How does the AWS Regional Services List differ from other AWS documentation?", "answer": "The AWS Regional Services List specifically focuses on the availability and up-to-date status of AWS services across different regions, while other AWS documentation may cover a broader range of topics, including service features, pricing, and usage guidelines.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "appsync-faq-3", "model": "gpt-4o-mini", "chunk": "AWS AppSync enables customers to use custom domain names with their AWS AppSync API to access their GraphQl endpoint and real-time endpoint. To create a custom domain name in AppSync, you simply provide a domain name you own and indicate a valid AWS Certificate Manager (ACM) certificate that covers your domain. Once the custom domain name is created, you can associate the domain name with any available AppSync API in your account. After you have updated your DNS record to map to to the AppSync-provided domain name, you can configure your applications to use the new GraphQL and real-time endpoints. You can change the API association on your custom domain at any time without having to update your applications. When AppSync receives a request on the custom domain endpoint, it routes it to the associated API for handling.\n\nYes, AWS AppSync supports private APIs. With private APIs, you can create GraphQL APIs that can only be accessed from your Amazon Virtual Private Cloud (VPC).\n\nAn \"event\" is a data record expressing an occurrence and its context. Events are routed from an event producer (the source) to interested event consumers. AppSync Events gives developers the ability to publish events using the HTTP protocol and JSON format. Subscribing is done over WebSocket channels.\n\nA channel is a routing concept. It is the logical destination of an event. It allows a publisher to say: “this is where my event should be routed.” It allows a subscriber to say: “this is the route that I want to monitor for arriving events”.\n\nA namespace is a logical construct that is used to define capabilities shared by channels that live within it. For example, you can define multiple authorization modes for your Event API, then attach specific modes on your namespaces (e.g.: only allow IAM_AUTH mode on namespace /chat).\n\nYes. Simply attach an Event Handler that specifies an onPublish function. In that handler, implement your transformation logic.\n\n", "question": "What is required to create a custom domain name for an AWS AppSync API?", "answer": "To create a custom domain name for an AWS AppSync API, you need to provide a domain name that you own and indicate a valid AWS Certificate Manager (ACM) certificate that covers your domain.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "appsync-faq-3", "model": "gpt-4o-mini", "chunk": "AWS AppSync enables customers to use custom domain names with their AWS AppSync API to access their GraphQl endpoint and real-time endpoint. To create a custom domain name in AppSync, you simply provide a domain name you own and indicate a valid AWS Certificate Manager (ACM) certificate that covers your domain. Once the custom domain name is created, you can associate the domain name with any available AppSync API in your account. After you have updated your DNS record to map to to the AppSync-provided domain name, you can configure your applications to use the new GraphQL and real-time endpoints. You can change the API association on your custom domain at any time without having to update your applications. When AppSync receives a request on the custom domain endpoint, it routes it to the associated API for handling.\n\nYes, AWS AppSync supports private APIs. With private APIs, you can create GraphQL APIs that can only be accessed from your Amazon Virtual Private Cloud (VPC).\n\nAn \"event\" is a data record expressing an occurrence and its context. Events are routed from an event producer (the source) to interested event consumers. AppSync Events gives developers the ability to publish events using the HTTP protocol and JSON format. Subscribing is done over WebSocket channels.\n\nA channel is a routing concept. It is the logical destination of an event. It allows a publisher to say: “this is where my event should be routed.” It allows a subscriber to say: “this is the route that I want to monitor for arriving events”.\n\nA namespace is a logical construct that is used to define capabilities shared by channels that live within it. For example, you can define multiple authorization modes for your Event API, then attach specific modes on your namespaces (e.g.: only allow IAM_AUTH mode on namespace /chat).\n\nYes. Simply attach an Event Handler that specifies an onPublish function. In that handler, implement your transformation logic.\n\n", "question": "How does AWS AppSync handle requests on a custom domain endpoint?", "answer": "When AWS AppSync receives a request on the custom domain endpoint, it routes the request to the associated API for handling.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "appsync-faq-3", "model": "gpt-4o-mini", "chunk": "AWS AppSync enables customers to use custom domain names with their AWS AppSync API to access their GraphQl endpoint and real-time endpoint. To create a custom domain name in AppSync, you simply provide a domain name you own and indicate a valid AWS Certificate Manager (ACM) certificate that covers your domain. Once the custom domain name is created, you can associate the domain name with any available AppSync API in your account. After you have updated your DNS record to map to to the AppSync-provided domain name, you can configure your applications to use the new GraphQL and real-time endpoints. You can change the API association on your custom domain at any time without having to update your applications. When AppSync receives a request on the custom domain endpoint, it routes it to the associated API for handling.\n\nYes, AWS AppSync supports private APIs. With private APIs, you can create GraphQL APIs that can only be accessed from your Amazon Virtual Private Cloud (VPC).\n\nAn \"event\" is a data record expressing an occurrence and its context. Events are routed from an event producer (the source) to interested event consumers. AppSync Events gives developers the ability to publish events using the HTTP protocol and JSON format. Subscribing is done over WebSocket channels.\n\nA channel is a routing concept. It is the logical destination of an event. It allows a publisher to say: “this is where my event should be routed.” It allows a subscriber to say: “this is the route that I want to monitor for arriving events”.\n\nA namespace is a logical construct that is used to define capabilities shared by channels that live within it. For example, you can define multiple authorization modes for your Event API, then attach specific modes on your namespaces (e.g.: only allow IAM_AUTH mode on namespace /chat).\n\nYes. Simply attach an Event Handler that specifies an onPublish function. In that handler, implement your transformation logic.\n\n", "question": "What is the difference between a channel and a namespace in AWS AppSync Events?", "answer": "A channel is a routing concept that serves as the logical destination of an event, allowing publishers to specify where their events should be routed and subscribers to monitor for arriving events. In contrast, a namespace is a logical construct used to define capabilities shared by channels within it, such as defining multiple authorization modes for an Event API and attaching specific modes to namespaces.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "artifact-faq-0", "model": "gpt-4o-mini", "chunk": "Security, Identity and Compliance\n›\nAWS Artifact\n›\nAWS Artifact FAQs\n\nGeneral\n8\nAWS Reports\n3\nThird-Party Reports\n4\nAgreements\n20\nBusiness Associate Addendum (BAA)\n18\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n9\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n9\nTroubleshooting\n2\nNotifications\n8\n\nBusiness Associate Addendum (BAA)\n\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n\nAWS Artifact, available in the console, is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS’ compliance documentation and AWS agreements.\nYou can use AWS Artifact Reports to download AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports.\nYou can use AWS Artifact Agreements to review, accept, and track the status of AWS agreements such as the Business Associate Addendum (BAA).\n\nAll AWS Accounts have access to AWS Artifact. Root users and IAM users with admin permissions can download all audit artifacts available to their account by agreeing to the associated terms and conditions.\nYou will need to grant IAM users with non-admin permissions access to AWS Artifact using IAM permissions. This allows you to grant a user access to AWS Artifact, while restricting access to other services and resources within your AWS Account. For information on how to grant access using IAM, refer to this help topic in the\nAWS Artifact documentation\n.\n\nAn Agreement can have two states:\nInactive:\nAn Agreement is in Inactive state if the Agreement has not been accepted by the user or a previously accepted Agreement has been terminated by the user\nActive:\nAn Agreement is in Active state if the Agreement has been accepted by the user\n\n", "question": "What is AWS Artifact?", "answer": "AWS Artifact is a self-service audit artifact retrieval portal available in the console that provides customers with on-demand access to AWS' compliance documentation and AWS agreements.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "artifact-faq-0", "model": "gpt-4o-mini", "chunk": "Security, Identity and Compliance\n›\nAWS Artifact\n›\nAWS Artifact FAQs\n\nGeneral\n8\nAWS Reports\n3\nThird-Party Reports\n4\nAgreements\n20\nBusiness Associate Addendum (BAA)\n18\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n9\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n9\nTroubleshooting\n2\nNotifications\n8\n\nBusiness Associate Addendum (BAA)\n\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n\nAWS Artifact, available in the console, is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS’ compliance documentation and AWS agreements.\nYou can use AWS Artifact Reports to download AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports.\nYou can use AWS Artifact Agreements to review, accept, and track the status of AWS agreements such as the Business Associate Addendum (BAA).\n\nAll AWS Accounts have access to AWS Artifact. Root users and IAM users with admin permissions can download all audit artifacts available to their account by agreeing to the associated terms and conditions.\nYou will need to grant IAM users with non-admin permissions access to AWS Artifact using IAM permissions. This allows you to grant a user access to AWS Artifact, while restricting access to other services and resources within your AWS Account. For information on how to grant access using IAM, refer to this help topic in the\nAWS Artifact documentation\n.\n\nAn Agreement can have two states:\nInactive:\nAn Agreement is in Inactive state if the Agreement has not been accepted by the user or a previously accepted Agreement has been terminated by the user\nActive:\nAn Agreement is in Active state if the Agreement has been accepted by the user\n\n", "question": "How can IAM users with non-admin permissions access AWS Artifact?", "answer": "IAM users with non-admin permissions can access AWS Artifact by being granted access using IAM permissions. This allows you to grant a user access to AWS Artifact while restricting access to other services and resources within your AWS Account.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "artifact-faq-0", "model": "gpt-4o-mini", "chunk": "Security, Identity and Compliance\n›\nAWS Artifact\n›\nAWS Artifact FAQs\n\nGeneral\n8\nAWS Reports\n3\nThird-Party Reports\n4\nAgreements\n20\nBusiness Associate Addendum (BAA)\n18\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n9\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n9\nTroubleshooting\n2\nNotifications\n8\n\nBusiness Associate Addendum (BAA)\n\nAWS Australian Notifiable Data Breach Addendum (ANDB Addendum)\n\nAWS New Zealand Notifiable Data Breach Addendum (NZNDB Addendum)\n\nAWS Artifact, available in the console, is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS’ compliance documentation and AWS agreements.\nYou can use AWS Artifact Reports to download AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports.\nYou can use AWS Artifact Agreements to review, accept, and track the status of AWS agreements such as the Business Associate Addendum (BAA).\n\nAll AWS Accounts have access to AWS Artifact. Root users and IAM users with admin permissions can download all audit artifacts available to their account by agreeing to the associated terms and conditions.\nYou will need to grant IAM users with non-admin permissions access to AWS Artifact using IAM permissions. This allows you to grant a user access to AWS Artifact, while restricting access to other services and resources within your AWS Account. For information on how to grant access using IAM, refer to this help topic in the\nAWS Artifact documentation\n.\n\nAn Agreement can have two states:\nInactive:\nAn Agreement is in Inactive state if the Agreement has not been accepted by the user or a previously accepted Agreement has been terminated by the user\nActive:\nAn Agreement is in Active state if the Agreement has been accepted by the user\n\n", "question": "What is the difference between an Active Agreement and an Inactive Agreement in AWS Artifact?", "answer": "An Agreement is in an Active state if it has been accepted by the user, while it is in an Inactive state if it has not been accepted or if a previously accepted Agreement has been terminated by the user.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "athena-faq-3", "model": "gpt-4o-mini", "chunk": "AWS Glue\nis a fully managed extract, transform, and load (ETL) service. AWS Glue has three main components: 1) a crawler that automatically scans your data sources, identifies data formats, and infers schemas, 2) a fully managed ETL service that allows you to transform and move data to various destinations, and 3) a Data Catalog that stores metadata information about databases and tables either stored in S3 or an\nODBC\n- or\nJDBC\n-compliant data store. To use the benefits of AWS Glue, you must upgrade from using Athena’s internal Data Catalog to the Glue Data Catalog.\nBenefits of upgrading to the Data Catalog include the following:\nUnified metadata repository: AWS Glue is integrated across various AWS services. AWS Glue supports data stored in Amazon Aurora, Amazon Relational Database Service (RDS) for MySQL, Amazon RDS for PostgreSQL, Amazon Redshift, and S3, as well as MySQL and PostgreSQL databases in your Amazon Virtual Private Cloud (VPC) running on Amazon Elastic Compute Cloud (EC2). AWS Glue provides out-of-the-box integration with Athena, Amazon EMR, Amazon Redshift Spectrum, and applications compatible with Apache Hive metastore.\nAutomatic schema and partition recognition: AWS Glue automatically crawls your data sources, identifies data formats, and suggests schemas and transformations. Crawlers can help automate table creation and automatic loading of partitions.\nTo learn more about the Data Catalog, review the\nAWS Glue\nwebpage.\n\nYes. For a step-by-step process, review the Amazon Athena User Guide:\nIntegration with AWS Glue\n.\n\nFor details of Athena service availability by Region, review the\nAWS Regional Services List\n.\n\nTo learn more about service limits, review the Amazon Athena User Guide: Service Quotas.\n\n", "question": "What are the three main components of AWS Glue?", "answer": "The three main components of AWS Glue are: 1) a crawler that automatically scans your data sources, identifies data formats, and infers schemas; 2) a fully managed ETL service that allows you to transform and move data to various destinations; and 3) a Data Catalog that stores metadata information about databases and tables either stored in S3 or an ODBC- or JDBC-compliant data store.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "athena-faq-3", "model": "gpt-4o-mini", "chunk": "AWS Glue\nis a fully managed extract, transform, and load (ETL) service. AWS Glue has three main components: 1) a crawler that automatically scans your data sources, identifies data formats, and infers schemas, 2) a fully managed ETL service that allows you to transform and move data to various destinations, and 3) a Data Catalog that stores metadata information about databases and tables either stored in S3 or an\nODBC\n- or\nJDBC\n-compliant data store. To use the benefits of AWS Glue, you must upgrade from using Athena’s internal Data Catalog to the Glue Data Catalog.\nBenefits of upgrading to the Data Catalog include the following:\nUnified metadata repository: AWS Glue is integrated across various AWS services. AWS Glue supports data stored in Amazon Aurora, Amazon Relational Database Service (RDS) for MySQL, Amazon RDS for PostgreSQL, Amazon Redshift, and S3, as well as MySQL and PostgreSQL databases in your Amazon Virtual Private Cloud (VPC) running on Amazon Elastic Compute Cloud (EC2). AWS Glue provides out-of-the-box integration with Athena, Amazon EMR, Amazon Redshift Spectrum, and applications compatible with Apache Hive metastore.\nAutomatic schema and partition recognition: AWS Glue automatically crawls your data sources, identifies data formats, and suggests schemas and transformations. Crawlers can help automate table creation and automatic loading of partitions.\nTo learn more about the Data Catalog, review the\nAWS Glue\nwebpage.\n\nYes. For a step-by-step process, review the Amazon Athena User Guide:\nIntegration with AWS Glue\n.\n\nFor details of Athena service availability by Region, review the\nAWS Regional Services List\n.\n\nTo learn more about service limits, review the Amazon Athena User Guide: Service Quotas.\n\n", "question": "Why is it beneficial to upgrade from Athena’s internal Data Catalog to the Glue Data Catalog?", "answer": "Upgrading to the Glue Data Catalog provides benefits such as a unified metadata repository that is integrated across various AWS services, automatic schema and partition recognition, and support for data stored in multiple sources including Amazon Aurora, Amazon RDS for MySQL, Amazon RDS for PostgreSQL, Amazon Redshift, and S3.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "athena-faq-3", "model": "gpt-4o-mini", "chunk": "AWS Glue\nis a fully managed extract, transform, and load (ETL) service. AWS Glue has three main components: 1) a crawler that automatically scans your data sources, identifies data formats, and infers schemas, 2) a fully managed ETL service that allows you to transform and move data to various destinations, and 3) a Data Catalog that stores metadata information about databases and tables either stored in S3 or an\nODBC\n- or\nJDBC\n-compliant data store. To use the benefits of AWS Glue, you must upgrade from using Athena’s internal Data Catalog to the Glue Data Catalog.\nBenefits of upgrading to the Data Catalog include the following:\nUnified metadata repository: AWS Glue is integrated across various AWS services. AWS Glue supports data stored in Amazon Aurora, Amazon Relational Database Service (RDS) for MySQL, Amazon RDS for PostgreSQL, Amazon Redshift, and S3, as well as MySQL and PostgreSQL databases in your Amazon Virtual Private Cloud (VPC) running on Amazon Elastic Compute Cloud (EC2). AWS Glue provides out-of-the-box integration with Athena, Amazon EMR, Amazon Redshift Spectrum, and applications compatible with Apache Hive metastore.\nAutomatic schema and partition recognition: AWS Glue automatically crawls your data sources, identifies data formats, and suggests schemas and transformations. Crawlers can help automate table creation and automatic loading of partitions.\nTo learn more about the Data Catalog, review the\nAWS Glue\nwebpage.\n\nYes. For a step-by-step process, review the Amazon Athena User Guide:\nIntegration with AWS Glue\n.\n\nFor details of Athena service availability by Region, review the\nAWS Regional Services List\n.\n\nTo learn more about service limits, review the Amazon Athena User Guide: Service Quotas.\n\n", "question": "How does AWS Glue's automatic schema recognition compare with the manual management of schemas in Athena's internal Data Catalog?", "answer": "AWS Glue's automatic schema recognition allows for the automatic crawling of data sources, identification of data formats, and suggestion of schemas and transformations, which can help automate table creation and loading of partitions. In contrast, Athena's internal Data Catalog requires more manual management of schemas without these automated capabilities.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "audit-faq-3", "model": "gpt-4o-mini", "chunk": "A control is a prescriptive description that explains how to implement a procedure to conform to a given rule, such as a compliance requirement. It provides a reasonable assurance that the resources used by your organization operate as intended, that data is reliable, and that your organization is in compliance with applicable laws and regulations.\n\nAWS Audit Manager enables you to define your own controls to collect evidence from specific data sources to help you meet unique compliance requirements.\nA common control collects evidence that can support a range of overlapping compliance obligations. Each common control consists of one or more core controls that collects evidence from a predefined group of AWS managed data sources. AWS updates these underlying data sources for you when regulations and standards change and new data sources are identified.\n\nAn AWS Audit Manager assessment is an implementation of an AWS Audit Manager framework. Using a framework as a starting point, you can create an assessment and define the AWS accounts that you want to include in the scope of your audit. After your assessment is created, AWS Audit Manager begins to automatically assess resources in your AWS accounts and services based on the controls deﬁned in the framework. Next, it collects the relevant evidence and converts it into an auditor-friendly format, and then attaches it to the controls in your assessment.\n\nA resource assessment is a process that collects, stores, and manages evidence, which you can use to assess risk and compliance with industry standards and regulations. When you define and launch an assessment based on an assessment framework, Audit Manager will execute a resource assessment for each individual resource, such as your Amazon EC2 instances, Amazon RDS instances, Amazon S3 buckets, or Amazon VPC subnets.\n\nEvidence is a record that contains the information needed to demonstrate compliance with the requirements specified by a control. Examples of evidence could be a change activity triggered by a user, or a system configuration snapshot.\n\nAn assessment report is a finalized document generated from an AWS Audit Manager assessment. The report summarizes the relevant evidence collected for your audit. The report links to the relevant evidence folders, which are named and organized according to the controls that are specified in your assessment.\n\n", "question": "What is the purpose of a control in the context of AWS Audit Manager?", "answer": "A control is a prescriptive description that explains how to implement a procedure to conform to a given rule, such as a compliance requirement. It provides reasonable assurance that the resources used by your organization operate as intended, that data is reliable, and that your organization is in compliance with applicable laws and regulations.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "audit-faq-3", "model": "gpt-4o-mini", "chunk": "A control is a prescriptive description that explains how to implement a procedure to conform to a given rule, such as a compliance requirement. It provides a reasonable assurance that the resources used by your organization operate as intended, that data is reliable, and that your organization is in compliance with applicable laws and regulations.\n\nAWS Audit Manager enables you to define your own controls to collect evidence from specific data sources to help you meet unique compliance requirements.\nA common control collects evidence that can support a range of overlapping compliance obligations. Each common control consists of one or more core controls that collects evidence from a predefined group of AWS managed data sources. AWS updates these underlying data sources for you when regulations and standards change and new data sources are identified.\n\nAn AWS Audit Manager assessment is an implementation of an AWS Audit Manager framework. Using a framework as a starting point, you can create an assessment and define the AWS accounts that you want to include in the scope of your audit. After your assessment is created, AWS Audit Manager begins to automatically assess resources in your AWS accounts and services based on the controls deﬁned in the framework. Next, it collects the relevant evidence and converts it into an auditor-friendly format, and then attaches it to the controls in your assessment.\n\nA resource assessment is a process that collects, stores, and manages evidence, which you can use to assess risk and compliance with industry standards and regulations. When you define and launch an assessment based on an assessment framework, Audit Manager will execute a resource assessment for each individual resource, such as your Amazon EC2 instances, Amazon RDS instances, Amazon S3 buckets, or Amazon VPC subnets.\n\nEvidence is a record that contains the information needed to demonstrate compliance with the requirements specified by a control. Examples of evidence could be a change activity triggered by a user, or a system configuration snapshot.\n\nAn assessment report is a finalized document generated from an AWS Audit Manager assessment. The report summarizes the relevant evidence collected for your audit. The report links to the relevant evidence folders, which are named and organized according to the controls that are specified in your assessment.\n\n", "question": "How does AWS Audit Manager help organizations meet unique compliance requirements?", "answer": "AWS Audit Manager enables organizations to define their own controls to collect evidence from specific data sources, which helps them meet unique compliance requirements. It allows for the customization of controls and the collection of evidence that supports various compliance obligations.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "audit-faq-3", "model": "gpt-4o-mini", "chunk": "A control is a prescriptive description that explains how to implement a procedure to conform to a given rule, such as a compliance requirement. It provides a reasonable assurance that the resources used by your organization operate as intended, that data is reliable, and that your organization is in compliance with applicable laws and regulations.\n\nAWS Audit Manager enables you to define your own controls to collect evidence from specific data sources to help you meet unique compliance requirements.\nA common control collects evidence that can support a range of overlapping compliance obligations. Each common control consists of one or more core controls that collects evidence from a predefined group of AWS managed data sources. AWS updates these underlying data sources for you when regulations and standards change and new data sources are identified.\n\nAn AWS Audit Manager assessment is an implementation of an AWS Audit Manager framework. Using a framework as a starting point, you can create an assessment and define the AWS accounts that you want to include in the scope of your audit. After your assessment is created, AWS Audit Manager begins to automatically assess resources in your AWS accounts and services based on the controls deﬁned in the framework. Next, it collects the relevant evidence and converts it into an auditor-friendly format, and then attaches it to the controls in your assessment.\n\nA resource assessment is a process that collects, stores, and manages evidence, which you can use to assess risk and compliance with industry standards and regulations. When you define and launch an assessment based on an assessment framework, Audit Manager will execute a resource assessment for each individual resource, such as your Amazon EC2 instances, Amazon RDS instances, Amazon S3 buckets, or Amazon VPC subnets.\n\nEvidence is a record that contains the information needed to demonstrate compliance with the requirements specified by a control. Examples of evidence could be a change activity triggered by a user, or a system configuration snapshot.\n\nAn assessment report is a finalized document generated from an AWS Audit Manager assessment. The report summarizes the relevant evidence collected for your audit. The report links to the relevant evidence folders, which are named and organized according to the controls that are specified in your assessment.\n\n", "question": "What is the relationship between an AWS Audit Manager assessment and a resource assessment?", "answer": "An AWS Audit Manager assessment is an implementation of an AWS Audit Manager framework that defines the scope of the audit, including specific AWS accounts. In contrast, a resource assessment is a process that collects, stores, and manages evidence for each individual resource within the scope of the assessment, such as Amazon EC2 instances or Amazon S3 buckets.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "augmented-faq-1", "model": "gpt-4o-mini", "chunk": "Amazon A2I provides a managed experience where you can set up an entire human review workflow in a few easy steps. To get started with Amazon A2I, sign in to your AWS Console, and navigate to the Amazon SageMaker console. From there, select Human review workflows under Augmented AI. First, as a part of the human review workflow, you provide a pointer to the S3 bucket where the review results should be stored. Next, you select the appropriate task type and define conditions when a human review should be triggered. Amazon A2I provides pre-built workflows where you only need to enter a few choices and provide instructions on how your objects should be reviewed by humans. Alternatively, you can create your own custom workflow and use your own custom review templates. Once created, the workflow can be used directly in your applications using a generated unique identifier for this workflow.\n\nWith A2I, you can define what is an acceptable prediction confidence for your business problem. You can define business rules for the machine learning predictions, based on which a human review is triggered. For Amazon Rekognition image moderation tasks, you can use the confidence score that Amazon Rekognition provides for each label it outputs to trigger human review. For Amazon Textract tasks, you can trigger a human review when specific form keys are missing or when form key detection confidence is low. You can also trigger a human review if, after evaluating all form keys in the text, confidence is lower than your required threshold for any form key. For your own custom workflow, you can write the code for business conditions in AWS Lambda or directly in your client application.\n\nWith Amazon A2I, you can choose from three workforce options: (1) Amazon Mechanical Turk; (2) Third party data labeling service providers available through the AWS Marketplace; and (3) Your own employees. See the\nAmazon A2I developer guide\nfor more information.\n\nPlease see the\nAmazon A2I pricing page\nfor the current pricing information.\n\nThe\nAWS Region Table\nlists all the AWS regions where Amazon A2I is currently available.\n\n", "question": "How do you get started with Amazon A2I?", "answer": "To get started with Amazon A2I, sign in to your AWS Console, navigate to the Amazon SageMaker console, and select Human review workflows under Augmented AI.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "augmented-faq-1", "model": "gpt-4o-mini", "chunk": "Amazon A2I provides a managed experience where you can set up an entire human review workflow in a few easy steps. To get started with Amazon A2I, sign in to your AWS Console, and navigate to the Amazon SageMaker console. From there, select Human review workflows under Augmented AI. First, as a part of the human review workflow, you provide a pointer to the S3 bucket where the review results should be stored. Next, you select the appropriate task type and define conditions when a human review should be triggered. Amazon A2I provides pre-built workflows where you only need to enter a few choices and provide instructions on how your objects should be reviewed by humans. Alternatively, you can create your own custom workflow and use your own custom review templates. Once created, the workflow can be used directly in your applications using a generated unique identifier for this workflow.\n\nWith A2I, you can define what is an acceptable prediction confidence for your business problem. You can define business rules for the machine learning predictions, based on which a human review is triggered. For Amazon Rekognition image moderation tasks, you can use the confidence score that Amazon Rekognition provides for each label it outputs to trigger human review. For Amazon Textract tasks, you can trigger a human review when specific form keys are missing or when form key detection confidence is low. You can also trigger a human review if, after evaluating all form keys in the text, confidence is lower than your required threshold for any form key. For your own custom workflow, you can write the code for business conditions in AWS Lambda or directly in your client application.\n\nWith Amazon A2I, you can choose from three workforce options: (1) Amazon Mechanical Turk; (2) Third party data labeling service providers available through the AWS Marketplace; and (3) Your own employees. See the\nAmazon A2I developer guide\nfor more information.\n\nPlease see the\nAmazon A2I pricing page\nfor the current pricing information.\n\nThe\nAWS Region Table\nlists all the AWS regions where Amazon A2I is currently available.\n\n", "question": "What are the key components involved in setting up a human review workflow with Amazon A2I?", "answer": "The key components involved in setting up a human review workflow with Amazon A2I include providing a pointer to the S3 bucket for storing review results, selecting the appropriate task type, defining conditions for when a human review should be triggered, and optionally using pre-built workflows or creating custom workflows with specific review templates.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "augmented-faq-1", "model": "gpt-4o-mini", "chunk": "Amazon A2I provides a managed experience where you can set up an entire human review workflow in a few easy steps. To get started with Amazon A2I, sign in to your AWS Console, and navigate to the Amazon SageMaker console. From there, select Human review workflows under Augmented AI. First, as a part of the human review workflow, you provide a pointer to the S3 bucket where the review results should be stored. Next, you select the appropriate task type and define conditions when a human review should be triggered. Amazon A2I provides pre-built workflows where you only need to enter a few choices and provide instructions on how your objects should be reviewed by humans. Alternatively, you can create your own custom workflow and use your own custom review templates. Once created, the workflow can be used directly in your applications using a generated unique identifier for this workflow.\n\nWith A2I, you can define what is an acceptable prediction confidence for your business problem. You can define business rules for the machine learning predictions, based on which a human review is triggered. For Amazon Rekognition image moderation tasks, you can use the confidence score that Amazon Rekognition provides for each label it outputs to trigger human review. For Amazon Textract tasks, you can trigger a human review when specific form keys are missing or when form key detection confidence is low. You can also trigger a human review if, after evaluating all form keys in the text, confidence is lower than your required threshold for any form key. For your own custom workflow, you can write the code for business conditions in AWS Lambda or directly in your client application.\n\nWith Amazon A2I, you can choose from three workforce options: (1) Amazon Mechanical Turk; (2) Third party data labeling service providers available through the AWS Marketplace; and (3) Your own employees. See the\nAmazon A2I developer guide\nfor more information.\n\nPlease see the\nAmazon A2I pricing page\nfor the current pricing information.\n\nThe\nAWS Region Table\nlists all the AWS regions where Amazon A2I is currently available.\n\n", "question": "What are the different workforce options available in Amazon A2I, and how do they compare?", "answer": "Amazon A2I provides three workforce options: (1) Amazon Mechanical Turk, which allows for crowdsourced labor; (2) third-party data labeling service providers available through the AWS Marketplace, which can offer specialized labeling services; and (3) your own employees, allowing for internal resources to conduct reviews. Each option caters to different needs for human review based on the nature of the tasks and the required expertise.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "autoscaling-faq-3", "model": "gpt-4o-mini", "chunk": "You should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling lets you define dynamic scaling policies for multiple EC2 Auto Scaling groups or other resources using predefined scaling strategies. Using AWS Auto Scaling to configure scaling policies for all of the scalable resources in your application is faster than managing scaling policies for each resource via its individual service console. It’s also easier, as AWS Auto Scaling includes predefined scaling strategies that simplify the setup of scaling policies. You should also use AWS Auto Scaling if you want to create predictive scaling for EC2 resources.\nYou should use EC2 Auto Scaling if you only need to scale Amazon EC2 Auto Scaling groups, or if you are only interested in maintaining the health of your EC2 fleet. You should also use EC2 Auto Scaling if you need to create or configure Amazon EC2 Auto Scaling groups, or if you need to set up scheduled or step scaling policies (as AWS Auto Scaling supports only target tracking scaling policies).\nEC2 Auto Scaling groups must be created and configured outside of AWS Auto Scaling, such as through the EC2 console, Auto Scaling API or via CloudFormation. AWS Auto Scaling can help you configure dynamic scaling policies for your existing EC2 Auto Scaling groups.\n\nYou should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling enables unified scaling for multiple resources, and has predefined guidance that helps make it easier and faster to configure scaling. If you prefer, you can instead choose to use the individual service consoles, Auto Scaling API, or Application Auto Scaling API to scale individual AWS services. You should also use the individual consoles or API if you want to setup step scaling policies or scheduled scaling, as AWS Auto Scaling creates target tracking scaling policies only.\n\nPredictive Scaling is a feature of AWS Auto Scaling that looks at historic traffic patterns and forecasts them into the future to schedule changes in the number of EC2 instances at the appropriate times going forward. Predictive Scaling uses machine learning models to forecast daily and weekly patterns.\nAuto Scaling enhanced with Predictive Scaling delivers faster, simpler, and more accurate capacity provisioning resulting in lower cost and more responsive applications. By predicting traffic changes, Predictive Scaling provisions EC2 instances in advance of changing traffic, making Auto Scaling faster and more accurate.\n\n", "question": "What is the primary purpose of AWS Auto Scaling?", "answer": "The primary purpose of AWS Auto Scaling is to manage scaling for multiple resources across multiple services by defining dynamic scaling policies for EC2 Auto Scaling groups or other resources using predefined scaling strategies.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "autoscaling-faq-3", "model": "gpt-4o-mini", "chunk": "You should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling lets you define dynamic scaling policies for multiple EC2 Auto Scaling groups or other resources using predefined scaling strategies. Using AWS Auto Scaling to configure scaling policies for all of the scalable resources in your application is faster than managing scaling policies for each resource via its individual service console. It’s also easier, as AWS Auto Scaling includes predefined scaling strategies that simplify the setup of scaling policies. You should also use AWS Auto Scaling if you want to create predictive scaling for EC2 resources.\nYou should use EC2 Auto Scaling if you only need to scale Amazon EC2 Auto Scaling groups, or if you are only interested in maintaining the health of your EC2 fleet. You should also use EC2 Auto Scaling if you need to create or configure Amazon EC2 Auto Scaling groups, or if you need to set up scheduled or step scaling policies (as AWS Auto Scaling supports only target tracking scaling policies).\nEC2 Auto Scaling groups must be created and configured outside of AWS Auto Scaling, such as through the EC2 console, Auto Scaling API or via CloudFormation. AWS Auto Scaling can help you configure dynamic scaling policies for your existing EC2 Auto Scaling groups.\n\nYou should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling enables unified scaling for multiple resources, and has predefined guidance that helps make it easier and faster to configure scaling. If you prefer, you can instead choose to use the individual service consoles, Auto Scaling API, or Application Auto Scaling API to scale individual AWS services. You should also use the individual consoles or API if you want to setup step scaling policies or scheduled scaling, as AWS Auto Scaling creates target tracking scaling policies only.\n\nPredictive Scaling is a feature of AWS Auto Scaling that looks at historic traffic patterns and forecasts them into the future to schedule changes in the number of EC2 instances at the appropriate times going forward. Predictive Scaling uses machine learning models to forecast daily and weekly patterns.\nAuto Scaling enhanced with Predictive Scaling delivers faster, simpler, and more accurate capacity provisioning resulting in lower cost and more responsive applications. By predicting traffic changes, Predictive Scaling provisions EC2 instances in advance of changing traffic, making Auto Scaling faster and more accurate.\n\n", "question": "Why is using AWS Auto Scaling considered faster and easier than managing scaling policies for individual resources?", "answer": "Using AWS Auto Scaling is considered faster and easier because it allows you to configure scaling policies for all scalable resources in your application in one place, rather than managing each resource via its individual service console. Additionally, AWS Auto Scaling includes predefined scaling strategies that simplify the setup of scaling policies.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "autoscaling-faq-3", "model": "gpt-4o-mini", "chunk": "You should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling lets you define dynamic scaling policies for multiple EC2 Auto Scaling groups or other resources using predefined scaling strategies. Using AWS Auto Scaling to configure scaling policies for all of the scalable resources in your application is faster than managing scaling policies for each resource via its individual service console. It’s also easier, as AWS Auto Scaling includes predefined scaling strategies that simplify the setup of scaling policies. You should also use AWS Auto Scaling if you want to create predictive scaling for EC2 resources.\nYou should use EC2 Auto Scaling if you only need to scale Amazon EC2 Auto Scaling groups, or if you are only interested in maintaining the health of your EC2 fleet. You should also use EC2 Auto Scaling if you need to create or configure Amazon EC2 Auto Scaling groups, or if you need to set up scheduled or step scaling policies (as AWS Auto Scaling supports only target tracking scaling policies).\nEC2 Auto Scaling groups must be created and configured outside of AWS Auto Scaling, such as through the EC2 console, Auto Scaling API or via CloudFormation. AWS Auto Scaling can help you configure dynamic scaling policies for your existing EC2 Auto Scaling groups.\n\nYou should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. AWS Auto Scaling enables unified scaling for multiple resources, and has predefined guidance that helps make it easier and faster to configure scaling. If you prefer, you can instead choose to use the individual service consoles, Auto Scaling API, or Application Auto Scaling API to scale individual AWS services. You should also use the individual consoles or API if you want to setup step scaling policies or scheduled scaling, as AWS Auto Scaling creates target tracking scaling policies only.\n\nPredictive Scaling is a feature of AWS Auto Scaling that looks at historic traffic patterns and forecasts them into the future to schedule changes in the number of EC2 instances at the appropriate times going forward. Predictive Scaling uses machine learning models to forecast daily and weekly patterns.\nAuto Scaling enhanced with Predictive Scaling delivers faster, simpler, and more accurate capacity provisioning resulting in lower cost and more responsive applications. By predicting traffic changes, Predictive Scaling provisions EC2 instances in advance of changing traffic, making Auto Scaling faster and more accurate.\n\n", "question": "How does AWS Auto Scaling differ from EC2 Auto Scaling in terms of scalability features?", "answer": "AWS Auto Scaling is designed to manage scaling for multiple resources across multiple services and supports predefined scaling strategies, including predictive scaling. In contrast, EC2 Auto Scaling is specifically focused on scaling Amazon EC2 Auto Scaling groups and maintaining the health of the EC2 fleet, and it only supports target tracking scaling policies, not predictive scaling or scheduled scaling.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "aws-faq-3", "model": "gpt-4o-mini", "chunk": "FTPS and SFTP can both be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer a secure tunnel for transmission of commands and data. SFTP is a newer protocol and uses a single channel for commands and data, requiring fewer port openings than FTPS.\n\nSFTP, FTPS, and AS2 can all be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer secure transmission of data. Aside from support for encrypted and signed messages, AS2’s built in mechanism for Message Disposition Notification (MDN) alerts the sender that the message has been successfully received and decrypted by the recipient. This provides proof to the sender that their message was delivered without being tampered in transit. Use of AS2 is prevalent in workflows operating in retail, e-commerce, payments, supply chain for interacting with business partners who are also able to use AS2 to transact messages so that it is securely transmitted and delivered. AS2 provides you with options to ensure identity of the sender and receiver, integrity of the message, and confirm whether the message was successfully delivered and decrypted by the receiver.\n\nYes. Any existing file transfer client application will continue to work as long as you have enabled your endpoint for the chosen protocols. Examples of commonly used SFTP/FTPS/FTP clients include WinSCP, FileZilla, CyberDuck, lftp, and OpenSSH clients.\n\nYou can use AWS Transfer SFTP connectors to access files stored on external SFTP sites. Refer to\nSFTP connectors documentation\nto get started with SFTP connectors\n\nYou can use AWS Transfer Family’s fully managed SFTP/FTPS/AS2 capabilities to receive EDI documents that are generated from your trading partner’s business systems. EDI documents received using AWS Transfer Family’s connectivity capabilities are automatically uploaded to Amazon S3 where they can then be transformed into JSON and XML formatted outputs using AWS B2B Data Interchange. Alternatively, you can use any other any other EDI connectivity tool to upload EDI documents to S3.\n\n", "question": "What types of protocols can be used for secure transfers according to the context?", "answer": "The types of protocols that can be used for secure transfers according to the context are FTPS, SFTP, and AS2.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "aws-faq-3", "model": "gpt-4o-mini", "chunk": "FTPS and SFTP can both be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer a secure tunnel for transmission of commands and data. SFTP is a newer protocol and uses a single channel for commands and data, requiring fewer port openings than FTPS.\n\nSFTP, FTPS, and AS2 can all be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer secure transmission of data. Aside from support for encrypted and signed messages, AS2’s built in mechanism for Message Disposition Notification (MDN) alerts the sender that the message has been successfully received and decrypted by the recipient. This provides proof to the sender that their message was delivered without being tampered in transit. Use of AS2 is prevalent in workflows operating in retail, e-commerce, payments, supply chain for interacting with business partners who are also able to use AS2 to transact messages so that it is securely transmitted and delivered. AS2 provides you with options to ensure identity of the sender and receiver, integrity of the message, and confirm whether the message was successfully delivered and decrypted by the receiver.\n\nYes. Any existing file transfer client application will continue to work as long as you have enabled your endpoint for the chosen protocols. Examples of commonly used SFTP/FTPS/FTP clients include WinSCP, FileZilla, CyberDuck, lftp, and OpenSSH clients.\n\nYou can use AWS Transfer SFTP connectors to access files stored on external SFTP sites. Refer to\nSFTP connectors documentation\nto get started with SFTP connectors\n\nYou can use AWS Transfer Family’s fully managed SFTP/FTPS/AS2 capabilities to receive EDI documents that are generated from your trading partner’s business systems. EDI documents received using AWS Transfer Family’s connectivity capabilities are automatically uploaded to Amazon S3 where they can then be transformed into JSON and XML formatted outputs using AWS B2B Data Interchange. Alternatively, you can use any other any other EDI connectivity tool to upload EDI documents to S3.\n\n", "question": "How does AS2 ensure message delivery and integrity compared to SFTP?", "answer": "AS2 ensures message delivery and integrity by providing a built-in mechanism for Message Disposition Notification (MDN) that alerts the sender when the message has been successfully received and decrypted by the recipient. This offers proof of delivery and integrity, which is not explicitly mentioned for SFTP in the context.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "aws-faq-3", "model": "gpt-4o-mini", "chunk": "FTPS and SFTP can both be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer a secure tunnel for transmission of commands and data. SFTP is a newer protocol and uses a single channel for commands and data, requiring fewer port openings than FTPS.\n\nSFTP, FTPS, and AS2 can all be used for secure transfers. Since they are different protocols, they use different clients and technologies to offer secure transmission of data. Aside from support for encrypted and signed messages, AS2’s built in mechanism for Message Disposition Notification (MDN) alerts the sender that the message has been successfully received and decrypted by the recipient. This provides proof to the sender that their message was delivered without being tampered in transit. Use of AS2 is prevalent in workflows operating in retail, e-commerce, payments, supply chain for interacting with business partners who are also able to use AS2 to transact messages so that it is securely transmitted and delivered. AS2 provides you with options to ensure identity of the sender and receiver, integrity of the message, and confirm whether the message was successfully delivered and decrypted by the receiver.\n\nYes. Any existing file transfer client application will continue to work as long as you have enabled your endpoint for the chosen protocols. Examples of commonly used SFTP/FTPS/FTP clients include WinSCP, FileZilla, CyberDuck, lftp, and OpenSSH clients.\n\nYou can use AWS Transfer SFTP connectors to access files stored on external SFTP sites. Refer to\nSFTP connectors documentation\nto get started with SFTP connectors\n\nYou can use AWS Transfer Family’s fully managed SFTP/FTPS/AS2 capabilities to receive EDI documents that are generated from your trading partner’s business systems. EDI documents received using AWS Transfer Family’s connectivity capabilities are automatically uploaded to Amazon S3 where they can then be transformed into JSON and XML formatted outputs using AWS B2B Data Interchange. Alternatively, you can use any other any other EDI connectivity tool to upload EDI documents to S3.\n\n", "question": "What is the difference in channel usage between SFTP and FTPS?", "answer": "SFTP uses a single channel for commands and data, which requires fewer port openings than FTPS, which uses different channels for commands and data.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "backup-faq-2", "model": "gpt-4o-mini", "chunk": "Yes. Backups created using services with existing backup capabilities, such as EBS Snapshots, can be accessed using AWS Backup. Similarly, backups created by AWS Backup can be accessed using the source service.\n\nAWS services offer backup features to protect your data, such as Amazon S3 Replication, Amazon EBS Snapshots, Amazon RDS snapshots, Amazon FSx backups, Amazon DynamoDB backups, and AWS Storage Gateway snapshots. All existing per-service backup capabilities remain unchanged. AWS Backup provides a common way to manage backups across AWS services both on AWS and on premises. AWS Backup is a centralized service that offers backup scheduling, retention management, and backup monitoring. AWS Backup supports existing backup functionality provided by S3, EBS, RDS, Amazon FSx, DynamoDB, and Storage Gateway. For AWS services with backup functionality built on AWS Backup, such as Amazon EFS and DynamoDB, AWS Backup provides backup management capabilities. Additional features include lifecycle policies to transition backups to a low-cost storage tier, backup storage and encryption independent from its source data, and backup access policies.\n\nAmazon Data Lifecycle Manager policies and backup plans created in AWS Backup work independently from each other and provide two ways to manage EBS snapshots. Amazon Data Lifecycle Manager provides a streamlined way to manage the lifecycle of EBS resources, such as volume snapshots. Use Amazon Data Lifecycle Manager when you want to automate the creation, retention, and deletion of EBS snapshots. Use AWS Backup to manage and monitor backups across the AWS services you use, including EBS volumes, from a single place.\n\n", "question": "What types of backups can AWS Backup access?", "answer": "AWS Backup can access backups created using services with existing backup capabilities, such as EBS Snapshots. Additionally, backups created by AWS Backup can also be accessed using the source service.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "backup-faq-2", "model": "gpt-4o-mini", "chunk": "Yes. Backups created using services with existing backup capabilities, such as EBS Snapshots, can be accessed using AWS Backup. Similarly, backups created by AWS Backup can be accessed using the source service.\n\nAWS services offer backup features to protect your data, such as Amazon S3 Replication, Amazon EBS Snapshots, Amazon RDS snapshots, Amazon FSx backups, Amazon DynamoDB backups, and AWS Storage Gateway snapshots. All existing per-service backup capabilities remain unchanged. AWS Backup provides a common way to manage backups across AWS services both on AWS and on premises. AWS Backup is a centralized service that offers backup scheduling, retention management, and backup monitoring. AWS Backup supports existing backup functionality provided by S3, EBS, RDS, Amazon FSx, DynamoDB, and Storage Gateway. For AWS services with backup functionality built on AWS Backup, such as Amazon EFS and DynamoDB, AWS Backup provides backup management capabilities. Additional features include lifecycle policies to transition backups to a low-cost storage tier, backup storage and encryption independent from its source data, and backup access policies.\n\nAmazon Data Lifecycle Manager policies and backup plans created in AWS Backup work independently from each other and provide two ways to manage EBS snapshots. Amazon Data Lifecycle Manager provides a streamlined way to manage the lifecycle of EBS resources, such as volume snapshots. Use Amazon Data Lifecycle Manager when you want to automate the creation, retention, and deletion of EBS snapshots. Use AWS Backup to manage and monitor backups across the AWS services you use, including EBS volumes, from a single place.\n\n", "question": "How does AWS Backup enhance backup management for AWS services?", "answer": "AWS Backup enhances backup management by providing a centralized service that offers backup scheduling, retention management, and backup monitoring across AWS services. It supports existing backup functionality from services like S3, EBS, RDS, Amazon FSx, DynamoDB, and Storage Gateway, and allows for management of backups both on AWS and on premises.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "backup-faq-2", "model": "gpt-4o-mini", "chunk": "Yes. Backups created using services with existing backup capabilities, such as EBS Snapshots, can be accessed using AWS Backup. Similarly, backups created by AWS Backup can be accessed using the source service.\n\nAWS services offer backup features to protect your data, such as Amazon S3 Replication, Amazon EBS Snapshots, Amazon RDS snapshots, Amazon FSx backups, Amazon DynamoDB backups, and AWS Storage Gateway snapshots. All existing per-service backup capabilities remain unchanged. AWS Backup provides a common way to manage backups across AWS services both on AWS and on premises. AWS Backup is a centralized service that offers backup scheduling, retention management, and backup monitoring. AWS Backup supports existing backup functionality provided by S3, EBS, RDS, Amazon FSx, DynamoDB, and Storage Gateway. For AWS services with backup functionality built on AWS Backup, such as Amazon EFS and DynamoDB, AWS Backup provides backup management capabilities. Additional features include lifecycle policies to transition backups to a low-cost storage tier, backup storage and encryption independent from its source data, and backup access policies.\n\nAmazon Data Lifecycle Manager policies and backup plans created in AWS Backup work independently from each other and provide two ways to manage EBS snapshots. Amazon Data Lifecycle Manager provides a streamlined way to manage the lifecycle of EBS resources, such as volume snapshots. Use Amazon Data Lifecycle Manager when you want to automate the creation, retention, and deletion of EBS snapshots. Use AWS Backup to manage and monitor backups across the AWS services you use, including EBS volumes, from a single place.\n\n", "question": "What is the relationship between Amazon Data Lifecycle Manager and AWS Backup regarding EBS snapshots?", "answer": "Amazon Data Lifecycle Manager and AWS Backup provide two independent ways to manage EBS snapshots. Amazon Data Lifecycle Manager offers a streamlined approach to automate the creation, retention, and deletion of EBS snapshots, while AWS Backup is used to manage and monitor backups across various AWS services, including EBS volumes, from a single interface.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "batch-faq-2", "model": "gpt-4o-mini", "chunk": "Yes. You can set Fargate CE’s to have a max vCPU, which is the total amount of vCPU of all the jobs currently running in that CE. When your vCPU count hits max vCPU in a CE, Batch will begin scheduling jobs on the next Fargate CE in order attached to the queue, if there is one. This is useful if, for example, you want to set a Fargate CE to some minimum business requirement, then run the rest of your workload on Fargate Spot.\nWhile setting a Fargate Spot CE as first, followed by a Fargate CE, Batch will only spill over into Fargate when the vCPU used by your jobs is greater than max vCPU for that CE. In the event that Fargate Spot is reclaimed, max vCPU will not be met and Batch will not request Fargate resources in the subsequent CE to run your jobs.\nConnecting an AWS Batch job queue to Fargate/Fargate Spot CE and an EC2 or Spot CE is not allowed.\n\nAWS Batch handles job execution and compute resource management, allowing you to focus on developing applications or analyzing results instead of setting up and managing infrastructure. If you are considering running or moving batch workloads to AWS, you should consider using AWS Batch.\n\nAWS Batch is optimized for batch computing and applications that scale through the execution of multiple jobs in parallel. Deep learning, genomics analysis, financial risk models, Monte Carlo simulations, animation rendering, media transcoding, image processing, and engineering simulations are all excellent examples of batch computing applications.\n\n", "question": "What happens when the vCPU count hits max vCPU in a Fargate Compute Environment (CE)?", "answer": "When your vCPU count hits max vCPU in a Fargate Compute Environment, AWS Batch will begin scheduling jobs on the next Fargate CE in order that is attached to the queue, if there is one.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "batch-faq-2", "model": "gpt-4o-mini", "chunk": "Yes. You can set Fargate CE’s to have a max vCPU, which is the total amount of vCPU of all the jobs currently running in that CE. When your vCPU count hits max vCPU in a CE, Batch will begin scheduling jobs on the next Fargate CE in order attached to the queue, if there is one. This is useful if, for example, you want to set a Fargate CE to some minimum business requirement, then run the rest of your workload on Fargate Spot.\nWhile setting a Fargate Spot CE as first, followed by a Fargate CE, Batch will only spill over into Fargate when the vCPU used by your jobs is greater than max vCPU for that CE. In the event that Fargate Spot is reclaimed, max vCPU will not be met and Batch will not request Fargate resources in the subsequent CE to run your jobs.\nConnecting an AWS Batch job queue to Fargate/Fargate Spot CE and an EC2 or Spot CE is not allowed.\n\nAWS Batch handles job execution and compute resource management, allowing you to focus on developing applications or analyzing results instead of setting up and managing infrastructure. If you are considering running or moving batch workloads to AWS, you should consider using AWS Batch.\n\nAWS Batch is optimized for batch computing and applications that scale through the execution of multiple jobs in parallel. Deep learning, genomics analysis, financial risk models, Monte Carlo simulations, animation rendering, media transcoding, image processing, and engineering simulations are all excellent examples of batch computing applications.\n\n", "question": "What is the purpose of setting a max vCPU for a Fargate CE in AWS Batch?", "answer": "Setting a max vCPU for a Fargate CE allows you to control the total amount of vCPU used by all jobs currently running in that CE, which helps manage resource allocation and ensures that certain business requirements are met before spilling over into other compute environments.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "batch-faq-2", "model": "gpt-4o-mini", "chunk": "Yes. You can set Fargate CE’s to have a max vCPU, which is the total amount of vCPU of all the jobs currently running in that CE. When your vCPU count hits max vCPU in a CE, Batch will begin scheduling jobs on the next Fargate CE in order attached to the queue, if there is one. This is useful if, for example, you want to set a Fargate CE to some minimum business requirement, then run the rest of your workload on Fargate Spot.\nWhile setting a Fargate Spot CE as first, followed by a Fargate CE, Batch will only spill over into Fargate when the vCPU used by your jobs is greater than max vCPU for that CE. In the event that Fargate Spot is reclaimed, max vCPU will not be met and Batch will not request Fargate resources in the subsequent CE to run your jobs.\nConnecting an AWS Batch job queue to Fargate/Fargate Spot CE and an EC2 or Spot CE is not allowed.\n\nAWS Batch handles job execution and compute resource management, allowing you to focus on developing applications or analyzing results instead of setting up and managing infrastructure. If you are considering running or moving batch workloads to AWS, you should consider using AWS Batch.\n\nAWS Batch is optimized for batch computing and applications that scale through the execution of multiple jobs in parallel. Deep learning, genomics analysis, financial risk models, Monte Carlo simulations, animation rendering, media transcoding, image processing, and engineering simulations are all excellent examples of batch computing applications.\n\n", "question": "How does AWS Batch manage job scheduling between Fargate Spot CE and Fargate CE?", "answer": "AWS Batch will spill over into Fargate CE from a Fargate Spot CE only when the vCPU used by jobs exceeds the max vCPU for the Fargate Spot CE. If Fargate Spot is reclaimed and the max vCPU is not met, Batch will not request Fargate resources in the subsequent CE to run jobs.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "bedrock-faq-2", "model": "gpt-4o-mini", "chunk": "Data security and compliance certifications: Amazon Bedrock offers several capabilities to support security and privacy requirements. Amazon Bedrock is in scope for common compliance standards such as Service and Organization Control (SOC), International Organization for Standardization (ISO), is Health Insurance Portability and Accountability Act (HIPAA) eligible, and customers can use Amazon Bedrock in compliance with the General Data Protection Regulation (GDPR). Amazon Bedrock is CSA Security Trust Assurance and Risk (STAR) Level 2 certified, which validates the use of best practices and the security posture of AWS cloud offerings. With Amazon Bedrock, your content is not used to improve the base models and is not shared with any model providers. Your data in Amazon Bedrock is always encrypted in transit and at rest, and you can optionally encrypt the data using your own keys. You can use AWS PrivateLink with Amazon Bedrock to establish private connectivity between your FMs and your Amazon Virtual Private Cloud (Amazon VPC) without exposing your traffic to the Internet.\n\n", "question": "What compliance certifications does Amazon Bedrock support?", "answer": "Amazon Bedrock supports several compliance certifications including Service and Organization Control (SOC), International Organization for Standardization (ISO), is Health Insurance Portability and Accountability Act (HIPAA) eligible, and can be used in compliance with the General Data Protection Regulation (GDPR). Additionally, Amazon Bedrock is CSA Security Trust Assurance and Risk (STAR) Level 2 certified.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "bedrock-faq-2", "model": "gpt-4o-mini", "chunk": "Data security and compliance certifications: Amazon Bedrock offers several capabilities to support security and privacy requirements. Amazon Bedrock is in scope for common compliance standards such as Service and Organization Control (SOC), International Organization for Standardization (ISO), is Health Insurance Portability and Accountability Act (HIPAA) eligible, and customers can use Amazon Bedrock in compliance with the General Data Protection Regulation (GDPR). Amazon Bedrock is CSA Security Trust Assurance and Risk (STAR) Level 2 certified, which validates the use of best practices and the security posture of AWS cloud offerings. With Amazon Bedrock, your content is not used to improve the base models and is not shared with any model providers. Your data in Amazon Bedrock is always encrypted in transit and at rest, and you can optionally encrypt the data using your own keys. You can use AWS PrivateLink with Amazon Bedrock to establish private connectivity between your FMs and your Amazon Virtual Private Cloud (Amazon VPC) without exposing your traffic to the Internet.\n\n", "question": "How does Amazon Bedrock ensure the security of customer data?", "answer": "Amazon Bedrock ensures the security of customer data by always encrypting data in transit and at rest. Additionally, customers have the option to encrypt their data using their own keys. Furthermore, Amazon Bedrock does not use customer content to improve its base models nor share it with model providers, ensuring privacy.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "bedrock-faq-2", "model": "gpt-4o-mini", "chunk": "Data security and compliance certifications: Amazon Bedrock offers several capabilities to support security and privacy requirements. Amazon Bedrock is in scope for common compliance standards such as Service and Organization Control (SOC), International Organization for Standardization (ISO), is Health Insurance Portability and Accountability Act (HIPAA) eligible, and customers can use Amazon Bedrock in compliance with the General Data Protection Regulation (GDPR). Amazon Bedrock is CSA Security Trust Assurance and Risk (STAR) Level 2 certified, which validates the use of best practices and the security posture of AWS cloud offerings. With Amazon Bedrock, your content is not used to improve the base models and is not shared with any model providers. Your data in Amazon Bedrock is always encrypted in transit and at rest, and you can optionally encrypt the data using your own keys. You can use AWS PrivateLink with Amazon Bedrock to establish private connectivity between your FMs and your Amazon Virtual Private Cloud (Amazon VPC) without exposing your traffic to the Internet.\n\n", "question": "How does the use of AWS PrivateLink with Amazon Bedrock compare to traditional internet connectivity?", "answer": "Using AWS PrivateLink with Amazon Bedrock allows for private connectivity between your foundation models (FMs) and your Amazon Virtual Private Cloud (Amazon VPC) without exposing your traffic to the Internet. This contrasts with traditional internet connectivity, which involves exposing traffic to potential security risks over the public internet.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "certificate-faq-3", "model": "gpt-4o-mini", "chunk": "ACM enables you to manage the lifecycle of your public and private certificates. ACM’s capabilities depend on whether the certificate is public or private, how you obtain the certificate, and where you deploy it.\nPublic certificates\n- You can request Amazon-issued public certificates in ACM. ACM manages the renewal and deployment of public certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway.\nExportable public certificates\n- ACM manages the renewal of exported public certificates that you may deploy with AWS services and/or in your on-premises environments.\nPrivate certificates\n– You can choose to delegate private certificate management to ACM. When used in this way, ACM can automatically renew and deploy private certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway. You can easily deploy these private certificates using the AWS Management console, APIs, and command-line interface (CLI). You can export private certificates from ACM and use them with EC2 instances, containers, on-premises servers, and IoT devices. AWS Private CA automatically renews these certificates and sends an Amazon CloudWatch notification when the renewal is completed. You can write client-side code to download renewed certificates and private keys and deploy them with your application.\nImported certificates\n– If you want to use a third-party certificate with Amazon CloudFront, Elastic Load Balancing, or Amazon API Gateway, you may import it into ACM using the AWS Management Console, AWS CLI, or ACM APIs. ACM can not renew imported certificates, but it can help you manage the renewal process. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire. You can use\nACM CloudWatch metrics\nto monitor the expiration dates of an imported certificates and import a new third-party certificate to replace an expiring one.\n\n", "question": "What services can ACM manage public certificates for?", "answer": "ACM can manage public certificates for ACM-integrated services including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "certificate-faq-3", "model": "gpt-4o-mini", "chunk": "ACM enables you to manage the lifecycle of your public and private certificates. ACM’s capabilities depend on whether the certificate is public or private, how you obtain the certificate, and where you deploy it.\nPublic certificates\n- You can request Amazon-issued public certificates in ACM. ACM manages the renewal and deployment of public certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway.\nExportable public certificates\n- ACM manages the renewal of exported public certificates that you may deploy with AWS services and/or in your on-premises environments.\nPrivate certificates\n– You can choose to delegate private certificate management to ACM. When used in this way, ACM can automatically renew and deploy private certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway. You can easily deploy these private certificates using the AWS Management console, APIs, and command-line interface (CLI). You can export private certificates from ACM and use them with EC2 instances, containers, on-premises servers, and IoT devices. AWS Private CA automatically renews these certificates and sends an Amazon CloudWatch notification when the renewal is completed. You can write client-side code to download renewed certificates and private keys and deploy them with your application.\nImported certificates\n– If you want to use a third-party certificate with Amazon CloudFront, Elastic Load Balancing, or Amazon API Gateway, you may import it into ACM using the AWS Management Console, AWS CLI, or ACM APIs. ACM can not renew imported certificates, but it can help you manage the renewal process. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire. You can use\nACM CloudWatch metrics\nto monitor the expiration dates of an imported certificates and import a new third-party certificate to replace an expiring one.\n\n", "question": "How does ACM handle the renewal of private certificates?", "answer": "When private certificate management is delegated to ACM, it can automatically renew and deploy private certificates used with ACM-integrated services. Additionally, AWS Private CA automatically renews these certificates and sends an Amazon CloudWatch notification when the renewal is completed.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "certificate-faq-3", "model": "gpt-4o-mini", "chunk": "ACM enables you to manage the lifecycle of your public and private certificates. ACM’s capabilities depend on whether the certificate is public or private, how you obtain the certificate, and where you deploy it.\nPublic certificates\n- You can request Amazon-issued public certificates in ACM. ACM manages the renewal and deployment of public certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway.\nExportable public certificates\n- ACM manages the renewal of exported public certificates that you may deploy with AWS services and/or in your on-premises environments.\nPrivate certificates\n– You can choose to delegate private certificate management to ACM. When used in this way, ACM can automatically renew and deploy private certificates used with ACM-integrated services, including Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway. You can easily deploy these private certificates using the AWS Management console, APIs, and command-line interface (CLI). You can export private certificates from ACM and use them with EC2 instances, containers, on-premises servers, and IoT devices. AWS Private CA automatically renews these certificates and sends an Amazon CloudWatch notification when the renewal is completed. You can write client-side code to download renewed certificates and private keys and deploy them with your application.\nImported certificates\n– If you want to use a third-party certificate with Amazon CloudFront, Elastic Load Balancing, or Amazon API Gateway, you may import it into ACM using the AWS Management Console, AWS CLI, or ACM APIs. ACM can not renew imported certificates, but it can help you manage the renewal process. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire. You can use\nACM CloudWatch metrics\nto monitor the expiration dates of an imported certificates and import a new third-party certificate to replace an expiring one.\n\n", "question": "What is the difference in renewal capabilities between imported certificates and public certificates managed by ACM?", "answer": "ACM can manage the renewal of public certificates automatically, while it cannot renew imported certificates. For imported certificates, users are responsible for monitoring their expiration dates and renewing them before they expire.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "chatbot-faq-2", "model": "gpt-4o-mini", "chunk": "You can provision Microsoft Teams and Slack channel configurations using AWS CLI, AWS CloudFormation, AWS Cloud Control APIs, and SDKs. Terraform users can use AWS provider to manage Chatbot channel configurations.\n\nAWS Chatbot is a global service and can be used in all commercial AWS Regions. You can combine Amazon SNS topics from multiple Regions in a single AWS Chatbot configuration. Visit the\nAWS Regional Product and Services table\nfor details about availability.\n\nAWS Chatbot is a global service and we may store or process customer information, such as Chatbot configurations and permissions, Microsoft Teams team identifiers, Slack workspace identifiers and channel names, notifications, user inputs, and AWS Chatbot generated responses and images, in any of the commercial AWS Regions.\nYou can request deletion of data used for analytics and to improve the quality of service associated with your account by contacting AWS Support. Your trust, your privacy, and the security of your data are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your data and ensure that our use complies with our commitments to you. See\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\nWhen you opt out of use of your data to improve and develop the quality of AWS Chatbot and other Amazon machine-learning/artificial-intelligence technologies, your data will be removed from all AWS Regions. For information about how to opt out, contact AWS Support.\n\nYou can customize AWS Chatbot to suit your ChatOps use cases. You can designate different channels to monitor and operate different aspects of your cloud applications. You can operate resources across multiple accounts and regions from a channel. With IAM-based permissions, guarails, and Service Control Policies (SCPs), you can decide the type of the actions channel members can take from chat channels.\nYou can also send custom notifications so that you are informed about the state of your resources and applications. You can also customize action buttons on notifications and configure command aliases so that you can quickly run commands to dignose and remediate issues.\n\n", "question": "What services can you use to provision Microsoft Teams and Slack channel configurations?", "answer": "You can provision Microsoft Teams and Slack channel configurations using AWS CLI, AWS CloudFormation, AWS Cloud Control APIs, and SDKs.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "chatbot-faq-2", "model": "gpt-4o-mini", "chunk": "You can provision Microsoft Teams and Slack channel configurations using AWS CLI, AWS CloudFormation, AWS Cloud Control APIs, and SDKs. Terraform users can use AWS provider to manage Chatbot channel configurations.\n\nAWS Chatbot is a global service and can be used in all commercial AWS Regions. You can combine Amazon SNS topics from multiple Regions in a single AWS Chatbot configuration. Visit the\nAWS Regional Product and Services table\nfor details about availability.\n\nAWS Chatbot is a global service and we may store or process customer information, such as Chatbot configurations and permissions, Microsoft Teams team identifiers, Slack workspace identifiers and channel names, notifications, user inputs, and AWS Chatbot generated responses and images, in any of the commercial AWS Regions.\nYou can request deletion of data used for analytics and to improve the quality of service associated with your account by contacting AWS Support. Your trust, your privacy, and the security of your data are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your data and ensure that our use complies with our commitments to you. See\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\nWhen you opt out of use of your data to improve and develop the quality of AWS Chatbot and other Amazon machine-learning/artificial-intelligence technologies, your data will be removed from all AWS Regions. For information about how to opt out, contact AWS Support.\n\nYou can customize AWS Chatbot to suit your ChatOps use cases. You can designate different channels to monitor and operate different aspects of your cloud applications. You can operate resources across multiple accounts and regions from a channel. With IAM-based permissions, guarails, and Service Control Policies (SCPs), you can decide the type of the actions channel members can take from chat channels.\nYou can also send custom notifications so that you are informed about the state of your resources and applications. You can also customize action buttons on notifications and configure command aliases so that you can quickly run commands to dignose and remediate issues.\n\n", "question": "How does AWS Chatbot ensure the security and privacy of customer data?", "answer": "AWS Chatbot implements appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, customer data. They prioritize customer trust, privacy, and data security while ensuring compliance with their commitments.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "chatbot-faq-2", "model": "gpt-4o-mini", "chunk": "You can provision Microsoft Teams and Slack channel configurations using AWS CLI, AWS CloudFormation, AWS Cloud Control APIs, and SDKs. Terraform users can use AWS provider to manage Chatbot channel configurations.\n\nAWS Chatbot is a global service and can be used in all commercial AWS Regions. You can combine Amazon SNS topics from multiple Regions in a single AWS Chatbot configuration. Visit the\nAWS Regional Product and Services table\nfor details about availability.\n\nAWS Chatbot is a global service and we may store or process customer information, such as Chatbot configurations and permissions, Microsoft Teams team identifiers, Slack workspace identifiers and channel names, notifications, user inputs, and AWS Chatbot generated responses and images, in any of the commercial AWS Regions.\nYou can request deletion of data used for analytics and to improve the quality of service associated with your account by contacting AWS Support. Your trust, your privacy, and the security of your data are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your data and ensure that our use complies with our commitments to you. See\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\nWhen you opt out of use of your data to improve and develop the quality of AWS Chatbot and other Amazon machine-learning/artificial-intelligence technologies, your data will be removed from all AWS Regions. For information about how to opt out, contact AWS Support.\n\nYou can customize AWS Chatbot to suit your ChatOps use cases. You can designate different channels to monitor and operate different aspects of your cloud applications. You can operate resources across multiple accounts and regions from a channel. With IAM-based permissions, guarails, and Service Control Policies (SCPs), you can decide the type of the actions channel members can take from chat channels.\nYou can also send custom notifications so that you are informed about the state of your resources and applications. You can also customize action buttons on notifications and configure command aliases so that you can quickly run commands to dignose and remediate issues.\n\n", "question": "In what ways can AWS Chatbot be customized for ChatOps use cases?", "answer": "AWS Chatbot can be customized to suit ChatOps use cases by designating different channels to monitor and operate different aspects of cloud applications, operating resources across multiple accounts and regions from a channel, and using IAM-based permissions, guardrails, and Service Control Policies (SCPs) to determine actions channel members can take. Additionally, users can send custom notifications, customize action buttons on notifications, and configure command aliases for quick command execution.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "cloud-faq-2", "model": "gpt-4o-mini", "chunk": "We have coverage for many common AWS services and features with rich, high-level constructs, and complete coverage of the lower-level CloudFormation resources, including new resources shortly after they are available. We keep the AWS Construct Library up to date by autogenerating the resource-level APIs every time the CloudFormation specification changes. On top of these autogenerated APIs, we provide handcrafted, higher-level abstractions that make it even easier to work with each service. We do not have this high-level, convenient abstraction for every AWS service yet, but are adding new features all the time with a focus on services most used in modern cloud applications. Visit the\nAWS Construct Library API reference\nto learn more.\n\nWe keep the AWS Construct Library in sync with the CloudFormation resource specification by adding new CloudFormation features to AWS CDK shortly after their publication.\n\nAWS CDK construct libraries are just like any other library. They are consumed through the package manager of the programming language you use; keeping those libraries up to date is part of your normal workflow. All packages support semantic versioning, allowing you to make conscious choices about when to migrate to new infrastructure models.\n\nThink of the AWS CDK as a developer-centric toolkit leveraging the full power of modern programming languages to define your AWS infrastructure as code. When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection.\n\n", "question": "What does the AWS Construct Library provide in relation to AWS services and features?", "answer": "The AWS Construct Library provides coverage for many common AWS services and features with rich, high-level constructs, as well as complete coverage of the lower-level CloudFormation resources. The library is kept up to date by autogenerating resource-level APIs every time the CloudFormation specification changes.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "cloud-faq-2", "model": "gpt-4o-mini", "chunk": "We have coverage for many common AWS services and features with rich, high-level constructs, and complete coverage of the lower-level CloudFormation resources, including new resources shortly after they are available. We keep the AWS Construct Library up to date by autogenerating the resource-level APIs every time the CloudFormation specification changes. On top of these autogenerated APIs, we provide handcrafted, higher-level abstractions that make it even easier to work with each service. We do not have this high-level, convenient abstraction for every AWS service yet, but are adding new features all the time with a focus on services most used in modern cloud applications. Visit the\nAWS Construct Library API reference\nto learn more.\n\nWe keep the AWS Construct Library in sync with the CloudFormation resource specification by adding new CloudFormation features to AWS CDK shortly after their publication.\n\nAWS CDK construct libraries are just like any other library. They are consumed through the package manager of the programming language you use; keeping those libraries up to date is part of your normal workflow. All packages support semantic versioning, allowing you to make conscious choices about when to migrate to new infrastructure models.\n\nThink of the AWS CDK as a developer-centric toolkit leveraging the full power of modern programming languages to define your AWS infrastructure as code. When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection.\n\n", "question": "How does the AWS CDK enhance the experience of defining AWS infrastructure compared to using CloudFormation directly?", "answer": "The AWS CDK enhances the experience of defining AWS infrastructure by providing a developer-centric toolkit that leverages modern programming languages. It allows developers to define infrastructure as code and compiles applications down to fully formed CloudFormation JSON/YAML templates, which are then submitted to the CloudFormation service for provisioning. This process incorporates the benefits of CloudFormation, such as safe deployment, automatic rollback, and drift detection.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "cloud-faq-2", "model": "gpt-4o-mini", "chunk": "We have coverage for many common AWS services and features with rich, high-level constructs, and complete coverage of the lower-level CloudFormation resources, including new resources shortly after they are available. We keep the AWS Construct Library up to date by autogenerating the resource-level APIs every time the CloudFormation specification changes. On top of these autogenerated APIs, we provide handcrafted, higher-level abstractions that make it even easier to work with each service. We do not have this high-level, convenient abstraction for every AWS service yet, but are adding new features all the time with a focus on services most used in modern cloud applications. Visit the\nAWS Construct Library API reference\nto learn more.\n\nWe keep the AWS Construct Library in sync with the CloudFormation resource specification by adding new CloudFormation features to AWS CDK shortly after their publication.\n\nAWS CDK construct libraries are just like any other library. They are consumed through the package manager of the programming language you use; keeping those libraries up to date is part of your normal workflow. All packages support semantic versioning, allowing you to make conscious choices about when to migrate to new infrastructure models.\n\nThink of the AWS CDK as a developer-centric toolkit leveraging the full power of modern programming languages to define your AWS infrastructure as code. When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection.\n\n", "question": "How do AWS CDK construct libraries compare to traditional libraries in terms of versioning and updates?", "answer": "AWS CDK construct libraries are similar to traditional libraries in that they are consumed through the package manager of the programming language used, and keeping them up to date is part of the normal workflow. All packages support semantic versioning, which allows developers to make conscious choices about when to migrate to new infrastructure models, just like with traditional libraries.", "question_type": "comparison", "original_pair_id": ""}
{"chunk_id": "cloudformation-faq-1", "model": "gpt-4o-mini", "chunk": "These services are designed to complement each other.\nAWS Elastic Beanstalk\nprovides an environment where you can easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for managing application lifecycle. If your application workloads can be managed as Elastic Beanstalk workloads, you can enjoy a more turn-key experience in creating and updating applications. Behind the scenes, Elastic Beanstalk uses CloudFormation to create and maintain resources. If your application requirements dictate more custom control, the additional functionality of CloudFormation gives you more options to control your workloads.\nAWS CloudFormation is a convenient provisioning mechanism for a broad range of\nAWS\nand third-party resources. It supports the infrastructure needs of many different types of applications such as existing enterprise applications, legacy applications, applications built using a variety of AWS resources, and container-based solutions (including those built using AWS Elastic Beanstalk).\nAWS CloudFormation supports Elastic Beanstalk application environments as one of the AWS resource types. This allows you, for example, to create and manage an AWS Elastic Beanstalk–hosted application along with an RDS database to store the application data. Any other supported AWS resource can be added to the group as well.\n\nCloudFormation introduces four concepts: A template is a JSON or YAML declarative code file that describes the intended state of all the resources you need to deploy your application. A stack implements and manages the group of resources outlined in your template, and allows the state and dependencies of those resources to be managed together. A change set is a preview of changes that will be executed by stack operations to create, update, or remove resources. A stack set is a group of stacks you manage together that can replicate a group.\n\nTo see a complete list of supported\nAWS resources\nand their features, visit the Supported AWS Services page in the Release History of the documentation.\nThe\nAWS CloudFormation Registry\nand AWS CloudFormation\ncustom resources\nenable management of additional AWS and third party resources.\n\n", "question": "What is the primary function of AWS Elastic Beanstalk?", "answer": "The primary function of AWS Elastic Beanstalk is to provide an environment where users can easily deploy and run applications in the cloud. It is integrated with developer tools and offers a one-stop experience for managing the application lifecycle.", "question_type": "factual", "original_pair_id": ""}
{"chunk_id": "cloudformation-faq-1", "model": "gpt-4o-mini", "chunk": "These services are designed to complement each other.\nAWS Elastic Beanstalk\nprovides an environment where you can easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for managing application lifecycle. If your application workloads can be managed as Elastic Beanstalk workloads, you can enjoy a more turn-key experience in creating and updating applications. Behind the scenes, Elastic Beanstalk uses CloudFormation to create and maintain resources. If your application requirements dictate more custom control, the additional functionality of CloudFormation gives you more options to control your workloads.\nAWS CloudFormation is a convenient provisioning mechanism for a broad range of\nAWS\nand third-party resources. It supports the infrastructure needs of many different types of applications such as existing enterprise applications, legacy applications, applications built using a variety of AWS resources, and container-based solutions (including those built using AWS Elastic Beanstalk).\nAWS CloudFormation supports Elastic Beanstalk application environments as one of the AWS resource types. This allows you, for example, to create and manage an AWS Elastic Beanstalk–hosted application along with an RDS database to store the application data. Any other supported AWS resource can be added to the group as well.\n\nCloudFormation introduces four concepts: A template is a JSON or YAML declarative code file that describes the intended state of all the resources you need to deploy your application. A stack implements and manages the group of resources outlined in your template, and allows the state and dependencies of those resources to be managed together. A change set is a preview of changes that will be executed by stack operations to create, update, or remove resources. A stack set is a group of stacks you manage together that can replicate a group.\n\nTo see a complete list of supported\nAWS resources\nand their features, visit the Supported AWS Services page in the Release History of the documentation.\nThe\nAWS CloudFormation Registry\nand AWS CloudFormation\ncustom resources\nenable management of additional AWS and third party resources.\n\n", "question": "How does AWS CloudFormation relate to AWS Elastic Beanstalk?", "answer": "AWS CloudFormation relates to AWS Elastic Beanstalk by serving as a provisioning mechanism that supports Elastic Beanstalk application environments as one of the AWS resource types. This allows users to create and manage an Elastic Beanstalk-hosted application along with other AWS resources, such as an RDS database, all managed together.", "question_type": "conceptual", "original_pair_id": ""}
{"chunk_id": "cloudformation-faq-1", "model": "gpt-4o-mini", "chunk": "These services are designed to complement each other.\nAWS Elastic Beanstalk\nprovides an environment where you can easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for managing application lifecycle. If your application workloads can be managed as Elastic Beanstalk workloads, you can enjoy a more turn-key experience in creating and updating applications. Behind the scenes, Elastic Beanstalk uses CloudFormation to create and maintain resources. If your application requirements dictate more custom control, the additional functionality of CloudFormation gives you more options to control your workloads.\nAWS CloudFormation is a convenient provisioning mechanism for a broad range of\nAWS\nand third-party resources. It supports the infrastructure needs of many different types of applications such as existing enterprise applications, legacy applications, applications built using a variety of AWS resources, and container-based solutions (including those built using AWS Elastic Beanstalk).\nAWS CloudFormation supports Elastic Beanstalk application environments as one of the AWS resource types. This allows you, for example, to create and manage an AWS Elastic Beanstalk–hosted application along with an RDS database to store the application data. Any other supported AWS resource can be added to the group as well.\n\nCloudFormation introduces four concepts: A template is a JSON or YAML declarative code file that describes the intended state of all the resources you need to deploy your application. A stack implements and manages the group of resources outlined in your template, and allows the state and dependencies of those resources to be managed together. A change set is a preview of changes that will be executed by stack operations to create, update, or remove resources. A stack set is a group of stacks you manage together that can replicate a group.\n\nTo see a complete list of supported\nAWS resources\nand their features, visit the Supported AWS Services page in the Release History of the documentation.\nThe\nAWS CloudFormation Registry\nand AWS CloudFormation\ncustom resources\nenable management of additional AWS and third party resources.\n\n", "question": "In what ways do AWS Elastic Beanstalk and AWS CloudFormation differ in terms of control over application workloads?", "answer": "AWS Elastic Beanstalk provides a more turn-key experience for managing application workloads, ideal for users who want ease of use and integration with developer tools. In contrast, AWS CloudFormation offers additional functionality for users who require more custom control over their workloads, allowing for detailed management of resources through templates and stacks.", "question_type": "comparison", "original_pair_id": ""}
