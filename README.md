# LLM Fine-tuning End-to-End Tutorial

> Production-grade LLM fine-tuning: Dataset engineering, LoRA training, vLLM serving - completely self-hosted.

ğŸš§ **Status:** Work in Progress - Week 1  
ğŸ“– **Tutorial Blog:** [Coming Soon]  
ğŸ¯ **Goal:** Complete self-hosted LLM fine-tuning pipeline

## What This Tutorial Covers

- âœ… Dataset engineering from scratch (no pre-existing datasets)
- âœ… LoRA fine-tuning on consumer GPU (NVIDIA T4)
- âœ… Real debugging stories (EOS token problem, pad_token anti-pattern)
- âœ… Production deployment with vLLM on Kubernetes
- âœ… Complete data sovereignty (self-hosted everything)

## Project Structure
```
â”œâ”€â”€ data/              # Dataset generation & processing
â”œâ”€â”€ training/          # LoRA fine-tuning scripts
â”œâ”€â”€ serving/           # vLLM deployment (K8s)
â”œâ”€â”€ evaluation/        # Multi-modal evaluation
â”œâ”€â”€ experiments/       # Config sweeps & experiments
â”œâ”€â”€ pipelines/         # Argo Workflows (optional)
â””â”€â”€ docs/              # Tutorial blog (GitHub Pages)
```

## Quick Start

Coming soon...

## Why This Tutorial?

Most LLM tutorials show:
- âŒ "Load dataset, run trainer, done"
- âŒ Copy-paste code without explanation
- âŒ No real problems or debugging
- âŒ Cloud/API dependent

This tutorial shows:
- âœ… Dataset generation from scratch
- âœ… Real debugging (20h EOS token problem)
- âœ… Design trade-offs & constraints
- âœ… Production deployment on K8s
- âœ… **Complete data sovereignty**

## Progress

- [ ] Week 1: Repository + vLLM Deployment
- [ ] Week 2: Optimization + Monitoring
- [ ] Week 3: Blog Post 1 (Why Fine-tune?)
- [ ] Week 4-6: Core Content (Posts 2-6)
- [ ] Week 7-8: Advanced + Polish

## License

MIT License - See [LICENSE](LICENSE)

## Contributing

This is a learning project. Issues and PRs welcome!

---

**Author:** [hanasobi](https://github.com/hanasobi)  
**Started:** January 2026