{"content": "Yes, the IAM policy simulator can include the effects of SCPs. You can use the policy simulator in a member account in your organization to understand the effect on individual principals in that account. An administrator in a member account with the appropriate AWS Organizations permissions can see if an SCP is affecting the access for the principals (account root, IAM user, and IAM role) in your member account.\nFor more information, see\nService Control Policies\n.\n\nYes. You decide which policies that you want to enforce. For example, you could create an organization that takes advantage only of the consolidated billing functionality. This allows you to have a single-payer account for all accounts in your organization and automatically receive default tiered-pricing benefits.\n\nRCP is an AWS Organizations policy that you can use to define and enforce preventative controls on AWS resources in your organization. Using RCPs, you can centrally set the maximum available permissions to your AWS resources as you scale your workloads on AWS. For example, an RCP can help restrict access to your resources so that they can only be accessed by identities that belong to your organization, or specifying the conditions under which identities external to your organization can access your resources.\n\nDeclarative policy is a management policy that helps you enforce durable intent such as baseline configuration for a given AWS service in your organization with a few simple clicks or commands. Once enforced, these policies prevent non-compliant actions. The configuration defined in the declarative policy is maintained when AWS introduces new APIs and features or if there are changes to your organization such as addition of resources, principals and accounts.\n\n", "token_count": 360, "metadata": {"service": "ORGANIZATIONS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "organizations-faq-8", "chunk_start": 3546, "chunk_end": 3906, "heading_hierarchy": ["AWS Organizations FAQs", "Control Management", "Can I simulate the effect of an SCP on an AWS account?"]}}
{"content": "You can assign logical names to AWS resources in a template. When a stack is created, AWS CloudFormation binds the logical name to the name of the corresponding actual AWS resource. Actual resource names are a combination of the stack and logical resource name. This allows multiple stacks to be created from a template without fear of name collisions between AWS resources.\n\nAlthough AWS CloudFormation allows you to name some resources (such as Amazon S3 buckets), CloudFormation doesn’t allow this for all resources. Naming resources restricts the reusability of templates and results in naming conflicts when an update causes a resource to be replaced. To minimize these issues, CloudFormation supports resource naming on a case by case basis.\n\nYes. AWS CloudFormation provides a set of application bootstrapping scripts that enable you to install packages, files, and services on your EC2 instances simply by describing them in your CloudFormation template. For more details and a how-to, see\nBootstrapping Applications via AWS CloudFormation\n.\nCloudFormation can also be integrated with Systems Manager to drive and maintain software installations with Systems Manager Automation Documents.\n\nYes. AWS CloudFormation can be used to bootstrap both the Chef Server and Chef Client software on your EC2 instances. For more details and a how-to, see\nIntegrating AWS CloudFormation with Chef\n.\n\nYes. AWS CloudFormation can be used to bootstrap both the Puppet Master and Puppet Client software on your EC2 instances. For more details and a how-to, see Integrating AWS CloudFormation with Puppet.\n\nYes. CloudFormation can bootstrap your Terraform engine on your EC2 instances, and you can use Terraform resource providers to create resources in stacks, leveraging stack state management, dependencies, stabilization and rollback.\n\nYes. Amazon EC2 resources that support the tagging feature can also be tagged in an AWS template. The tag values can refer to template parameters, other resource names, resource attribute values (e.g. addresses), or values computed by simple functions (e.g., a concatenated a list of strings). CloudFormation automatically tags Amazon EBS volumes and Amazon EC2 instances with the name of the CloudFormation stack they are part of.\n\n", "token_count": 500, "metadata": {"service": "CLOUDFORMATION", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "cloudformation-faq-3", "chunk_start": 1203, "chunk_end": 1703, "heading_hierarchy": ["AWS Cloudformation FAQs", "General", "How does AWS CloudFormation choose actual resource names?"]}}
{"content": "Yes, management accounts and member accounts can have AWS Artifact Account Agreements (i.e. agreements under the\nAccount agreements\ntab) and AWS Artifact Organization Agreements (i.e. agreements under the\nOrganization agreements\ntab) of the same type in place at the same time.\nIf your account has an account agreement and an organization agreement of the same type in place at the same time, the organization agreement will apply instead of the account agreement. If, with respect to an individual account, an organization agreement is terminated (e.g. by removal of a member account from the organization), the account agreement in place for that individual account (viewable under the Account agreements tab) will remain active and will continue to apply.\n\nThe organization agreement will apply because according to its terms, it applies instead of the account agreement when both are active. If the organization agreement is terminated, and if you have an account agreement of the same type in place (under the\nAccount agreements\ntab), the account agreement will apply to that account.\nNote\n: Terminating the organization agreement does not terminate the account agreement.\n\nWhen a member account is removed from an organization (e.g. by leaving the organization, or by being removed from the organization by the management account), any organization agreements accepted on its behalf will no longer apply to that member account.\nManagement account administrators should alert member accounts prior to removing those accounts from the organization so that member accounts can put new account agreements in place, if necessary. Before member account owners leave an organization, they should determine (with the assistance of legal, privacy, or compliance teams, if appropriate) whether it is necessary to put new agreements in place.\n\nCurrently, member accounts are not notified when they are removed from an organization. We are developing functionality that will alert member accounts when they have been removed from an organization and are no longer covered by an organization agreement.\nManagement account administrators should alert member accounts prior to removing those accounts from the organization so that member accounts can put new account agreements in place, if necessary. Before member account owners leave an organization, they should determine (with the assistance of legal, privacy, or compliance teams, if appropriate) whether it is necessary to put new agreements in place.\n\n", "token_count": 477, "metadata": {"service": "ARTIFACT", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "artifact-faq-7", "chunk_start": 3132, "chunk_end": 3609, "heading_hierarchy": ["AWS Artifact FAQs", "Agreements", "17. Can I accept an agreement in the Organization agreement tab if my account already has an agreement of the same type accepted in the Account agreements tab?"]}}
{"content": "Amazon Route 53 Traffic Flow is an easy-to-use and cost-effective global traffic management service. With Amazon Route 53 Traffic Flow, you can improve the performance and availability of your application for your end users by running multiple endpoints around the world, using Amazon Route 53 Traffic Flow to connect your users to the best endpoint based on latency, geography, and endpoint health. Amazon Route 53 Traffic Flow makes it easy for developers to create policies that route traffic based on the constraints they care most about, including latency, endpoint health, load, geoproximity and geography. Customers can customize these templates or build policies from scratch using a simple visual policy builder in the AWS Management Console.\n\nA\ntraffic policy\nis the set of rules that you define to route end users’ requests to one of your application’s endpoints. You can create a traffic policy using the visual policy builder in the Amazon Route 53 Traffic Flow section of the Amazon Route 53 console. You can also create traffic policies as JSON-formatted text files and upload these policies using the Route 53 API, the AWS CLI, or the various AWS SDKs.\nBy itself, a traffic policy doesn’t affect how end users are routed to your application because it isn’t yet associated with your application’s DNS name (such as www.example.com). To start using Amazon Route 53 Traffic Flow to route traffic to your application using the traffic policy you’ve created, you create a\npolicy record\nwhich associates the traffic policy with the appropriate DNS name within an Amazon Route 53 hosted zone that you own. For example, if you want to use a traffic policy that you’ve named my-first-traffic-policy to manage traffic for your application at www.example.com, you will create a policy record for www.example.com within your hosted zone example.com and choose my-first-traffic-policy as the traffic policy.\nPolicy records are visible in both the Amazon Route 53 Traffic Flow and Amazon Route 53 Hosted Zone sections of the Amazon Route 53 console.\n\n", "token_count": 467, "metadata": {"service": "ROUTE53", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "route53-faq-16", "chunk_start": 6569, "chunk_end": 7036, "heading_hierarchy": ["Amazon Route 53 FAQs", "Traffic Flow", "What is Amazon Route 53 Traffic Flow?"]}}
{"content": "Unlike other databases, after a\ndatabase crash\n, Amazon DocumentDB does not need to replay the redo log from the last database checkpoint (typically five minutes) and confirm that all changes have been applied before making the database available for operations. This reduces database restart times to less than 60 seconds in most cases. Amazon DocumentDB moves the cache out of the database process and makes it available immediately at restart time. This prevents you from having to throttle access until the cache is repopulated to avoid brownouts.\n\nAmazon DocumentDB supports\nread replicas\n, which share the same underlying storage volume as the primary instance. Updates made by the primary instance are visible to all Amazon DocumentDB replicas. You can configure up to 15 read replicas. Replication is asynchronous and typically completes in milliseconds, with low impact on the performance of the primary instance. To learn more, see\nAmazon DocumentDB High availability and replication\n.\n\nYes, you can replicate your data across Regions using the\nGlobal Clusters feature\n. Global Clusters span across multiple AWS Regions. Global Clusters replicate your data to clusters in up to five Regions with little to no impact on performance. Global Clusters provide faster recovery from Region-wide outages and enable low-latency global reads. To learn more see the\nGlobal Clusters feature page\nand\nblog post\n.\n\nYes. You can\nassign a promotion priority tier\nto each instance on your cluster. If the primary instance fails, Amazon DocumentDB will promote the replica with the highest priority to primary. If there are inconsistencies between two or more replicas in the same priority tier, then Amazon DocumentDB will promote the replica that is the same size as the primary instance.\n\nYou can\nmodify the priority tier\nfor an instance at any time. Simply modifying priority tiers will not trigger a\nfailover\n.\n\nYou can assign lower priority tiers to replicas that you do not want promoted to the primary instance. However, if the higher priority replicas on the cluster are unhealthy or unavailable for some reason, then Amazon DocumentDB will promote the lower priority replica.\n\n", "token_count": 477, "metadata": {"service": "DOCUMENTDB", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "documentdb-faq-10", "chunk_start": 4529, "chunk_end": 5006, "heading_hierarchy": ["Amazon DocumentDB (with MongoDB compatibility) FAQs", "Resiliency", "How does Amazon DocumentDB improve recovery time after a database crash?"]}}
{"content": "Developer Tools\n›\nAWS Device Farm\n›\nFAQs\n\nTesting on real mobile devices\n4\nSetting up tests and remote access sessions\n8\nSelecting devices\n8\nTesting your app\n22\nReviewing results\n3\nPricing\n10\nTesting on desktop browsers\n11\n\nTesting on real mobile devices\n\nSetting up tests and remote access sessions\n\nTesting on desktop browsers\n\nAWS Device Farm allows developers to increase application quality, time to market, and customer satisfaction by testing and interacting with real Android and iOS devices in the AWS Cloud. Developers can upload their app and test scripts and run automated tests in parallel across 100s of real devices, getting results, screenshots, video, and performance data in minutes. They can also debug and reproduce customer issues by swiping, gesturing, and interacting with a device through their web browser.\n\nAWS Device Farm is designed for developers, QA teams, and customer support representatives who are building, testing, and supporting mobile apps to increase the quality of their apps. Application quality is increasingly important, and also getting complex due to the number of device models, variations in firmware and OS versions, carrier and manufacturer customizations, and dependencies on remote services and other apps. AWS Device Farm accelerates the development process by executing tests on multiple devices, giving developers, QA and support professionals the ability to perform automated tests and manual tasks like reproducing customer issues, exploratory testing of new functionality, and executing manual test plans. AWS Device Farm also offers significant savings by eliminating the need for internal device labs, lab managers, and automation infrastructure development.\n\nAWS Device Farm supports native and hybrid Android, iOS, and web apps, and cross-platform apps including those created with PhoneGap, Titanium, Xamarin, Unity, and other frameworks.\n\nAWS Device Farm tests are run on real devices. The devices are a mixture of OEM and carrier-branded devices.\n\nPlease see our\ngetting started guide\n.\n\nAWS Device Farm works on Internet Explorer 9 or later and the latest versions of Chrome, Firefox, and Safari.\n\nYour web applications will be tested in Chrome on Android and Safari on iOS.\n\nAWS Device Farm supports files up to 4 GB.\n\n", "token_count": 503, "metadata": {"service": "DEVICE", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "device-faq-0", "chunk_start": 0, "chunk_end": 503, "heading_hierarchy": []}}
{"content": "Firehose delivery can deliver to a different account in Amazon OpenSearch Service only when Firehose and Amazon OpenSearch Service are connected through public end point.\nIf Firehose and Amazon OpenSearch Service are connected through in a private VPC. Then Firehose stream and destination Amazon OpenSearch Service domain VPC need to be in the same account.\n\nNo, your Firehose stream and destination Amazon OpenSearch Service domain need to be in the same region.\n\nThe frequency of data delivery to Amazon OpenSearch Service is determined by the OpenSearch buffer size and buffer interval values that you configured for your Firehose stream. Firehose buffers incoming data before delivering it to Amazon OpenSearch Service. You can configure the values for OpenSearch buffer size (1 MB to 100 MB) or buffer interval (0 to 900 seconds), and the condition satisfied first triggers data delivery to Amazon OpenSearch Service. Note that in circumstances where data delivery to the destination is falling behind data ingestion into the Firehose stream, Amazon Data Firehose raises the buffer size automatically to catch up and make sure that all data is delivered to the destination.\n\nFor Redshift destinations, Amazon Data Firehose generates manifest files to load Amazon S3 objects to Redshift instances in batch. The manifests folder stores the manifest files generated by Firehose.\n\nIf “all documents” mode is used, Amazon Data Firehose concatenates multiple incoming records based on buffering configuration of your Firehose stream, and then delivers them to your S3 bucket as an S3 object. Regardless of which backup mode is configured, the failed documents are delivered to your S3 bucket using a certain JSON format that provides additional information such as error code and time of delivery attempt. For more information, see\nAmazon S3 Backup for the Amazon OpenSearch Destination\nin the Amazon Data Firehose developer guide.\n\nA single Firehose stream can currently only deliver data to one Amazon S3 bucket. If you want to have data delivered to multiple S3 buckets, you can create multiple Firehose streams.\n\nA single Firehose stream can currently only deliver data to one Redshift instance and one table. If you want to have data delivered to multiple Redshift instances or tables, you can create multiple Firehose streams.\n\n", "token_count": 502, "metadata": {"service": "DATA", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "data-faq-11", "chunk_start": 4751, "chunk_end": 5253, "heading_hierarchy": ["Amazon Data Firehose FAQs", "Data Delivery and Destinations", "Can I use a Firehose stream in one account to deliver my data into an Amazon OpenSearch Service domain VPC destination in a different account?"]}}
{"content": "Security, Identity and Compliance\n›\nAWS CloudHSM\n›\nFAQs\n\nGeneral\n12\nBilling\n4\nProvisioning and operations\n8\nSecurity and Compliance\n16\nPerformance and capacity\n2\n3rd Party Integrations\n2\nAWS CloudHSM client, API and SDK\n7\nMigrating to CloudHSM\n3\nSupport and maintenance\n2\n\nProvisioning and operations\n\nSecurity and Compliance\n\nPerformance and capacity\n\n3rd Party Integrations\n\nAWS CloudHSM client, API and SDK\n\nMigrating to CloudHSM\n\nSupport and maintenance\n\nThe AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud. AWS and AWS Marketplace partners offer a variety of solutions for protecting sensitive data within your AWS environment, but for some applications and data subject to contractual or regulatory mandates for managing cryptographic keys, additional protection may be necessary. AWS CloudHSM complements existing data protection solutions and allows you to protect your encryption keys within HSMs that are designed and validated to government standards for secure key management. AWS CloudHSM allows you to securely generate, store, and manage cryptographic keys used for data encryption in a way that keys are accessible only by you.\n\nA hardware security module (HSM) provides secure key storage and cryptographic operations within a tamper-resistant hardware device. HSMs are designed to securely store cryptographic key material and use the key material without exposing it outside the cryptographic boundary of the hardware.\n\nYou can use AWS CloudHSM to support a variety of use cases and applications, such as database encryption, Digital Rights Management (DRM), Public Key Infrastructure (PKI), authentication and authorization, document signing, and transaction processing. For more information, see\nAWS CloudHSM use cases.\n\n", "token_count": 429, "metadata": {"service": "CLOUDHSM", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "cloudhsm-faq-0", "chunk_start": 0, "chunk_end": 429, "heading_hierarchy": []}}
{"content": "No. Amazon Route 53 is an authoritative DNS service and does not provide\nwebsite hosting\n. However, you can use Amazon Simple Storage Service (Amazon S3) to host a static website. To host a dynamic website or other web applications, you can use Amazon Elastic Compute Cloud (Amazon EC2), which provides flexibility, control, and significant cost savings over traditional\nweb hosting\nsolutions. Learn more about Amazon EC2\nhere\n. For both static and dynamic websites, you can provide low latency delivery to your global end users with Amazon CloudFront. Learn more about Amazon CloudFront\nhere\n.\n\nAmazon Route 53 currently supports the following DNS record types:\nA (address record)\nAAAA (IPv6 address record)\nCNAME (canonical name record)\nCAA (certification authority authorization)\nMX (mail exchange record)\nNAPTR (name authority pointer record)\nNS (name server record)\nPTR (pointer record)\nSOA (start of authority record)\nSPF (sender policy framework)\nSRV (service locator)\nTXT (text record)\nAmazon Route 53 also offers alias records, which are an Amazon Route 53-specific extension to DNS. You can create alias records to route traffic to selected AWS resources, including Amazon Elastic Load Balancing load balancers, Amazon CloudFront distributions, AWS Elastic Beanstalk environments, API Gateways, VPC interface endpoints, and Amazon S3 buckets that are configured as websites. Alias record typically have a type of A or AAAA, but they work like a CNAME record. Using an alias record, you can map your record name (example.com) to the DNS name for an AWS resource (elb1234.elb.amazonaws.com). Resolvers see the A or AAAA record and the IP address of the AWS resource.\nWe anticipate adding additional record types in the future.\n\n", "token_count": 429, "metadata": {"service": "ROUTE53", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "route53-faq-6", "chunk_start": 2434, "chunk_end": 2863, "heading_hierarchy": ["Amazon Route 53 FAQs", "Domain Name Systems (DNS)", "Does Amazon Route 53 also provide website hosting?"]}}
{"content": "Using a CNAME record allows ACM to renew certificates for as long as the CNAME record exists. The CNAME record directs to a TXT record in an AWS domain (acm-validations.aws) that ACM can update as needed to validate or re-validate a domain name, without any action from you.\n\nYes. You can create one DNS CNAME record and use it to obtain certificates in the same AWS account in any AWS Region where ACM is offered. Configure the CNAME record once and you can get certificates issued and renewed from ACM for that name without creating another record.\n\nNo. Each certificate can have only one validation method.\n\nACM automatically renews certificates that are in use (associated with other AWS resources) as long as the DNS validation record remains in place.\n\nYes. Simply remove the CNAME record. ACM does not issue or renew certificates for your domain using DNS validation after you remove the CNAME record and the change is distributed through DNS. The propagation time to remove the record depends on your DNS provider.\n\nACM cannot issue or renew certificates for your domain using DNS validation if you remove the CNAME record.\n\nWith email validation, an approval request email is sent to the registered domain owner for each domain name in the certificate request. The domain owner or an authorized representative (approver) can approve the certificate request by following the instructions in the email. The instructions direct the approver to navigate to the approval website and click the link in the email or paste the link from the email into a browser to navigate to the approval web site. The approver confirms the information associated with the certificate request, such as the domain name, certificate ID (ARN), and the AWS account ID initiating the request, and approves the request if the information is accurate.\n\n", "token_count": 398, "metadata": {"service": "CERTIFICATE", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "certificate-faq-13", "chunk_start": 5925, "chunk_end": 6323, "heading_hierarchy": ["AWS Certificate Manager FAQs", "DNS Validation", "Why does ACM use CNAME records for DNS validation instead of TXT records?"]}}
{"content": "Amazon Relational Database Service (Amazon RDS)\nis a managed service that makes it easy to set up, operate, and scale a\nrelational database\nin\nthe cloud\n. It provides cost-efficient and resizable capacity, while managing time-consuming database administration tasks, freeing you to focus on your applications and business.\nAmazon RDS gives you access to the capabilities of a familiar\nRDS for PostgreSQL\n,\nRDS for MySQL\n,\nRDS for MariaDB\n,\nRDS for SQL Server\n,\nRDS for Oracle\n, or\nRDS for Db2\ndatabase. This means that the code, applications, and tools you already use today with your existing databases should work seamlessly with Amazon RDS. Amazon RDS can automatically back up your database and keep your database software up to date with the latest version. You benefit from the flexibility of being able to scale the compute resources or storage capacity associated with your relational database instance. In addition, Amazon RDS makes it easy to use replication to enhance database availability, improve data durability, or scale beyond the capacity constraints of a single database instance for read-heavy database workloads. As with all AWS services, there are no upfront investments required, and you pay only for the resources you use.\n\nAmazon Web Services provides a number of database alternatives for developers. Amazon RDS enables you to run a fully managed and fully featured relational database while offloading database administration. Using one of our many relational database AMIs on\nAmazon EC2\nallows you to manage your own relational database in the cloud. There are important differences between these alternatives that may make one more appropriate for your use case. See\nCloud Databases with AWS\nfor guidance on which solution is best for you.\n\nYes, you can run Amazon RDS on premises using Amazon RDS on Outposts. Please see the\nAmazon RDS on Outposts FAQs\nfor additional information.\n\nYes, Amazon RDS specialists are available to answer questions and provide support.\nContact Us\nand you’ll hear back from us in one business day to discuss how AWS can help your organization.\n\n", "token_count": 465, "metadata": {"service": "RDS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "rds-faq-1", "chunk_start": 276, "chunk_end": 741, "heading_hierarchy": ["Amazon RDS FAQs", "General", "What is Amazon RDS?"]}}
{"content": "Route 53 Geo DNS lets you balance load by directing requests to specific endpoints based on the geographic location from which the request originates. Geo DNS makes it possible to customize localized content, such as presenting detail pages in the right language or restricting distribution of content to only the markets you have licensed. Geo DNS also lets you balance load across endpoints in a predictable, easy-to-manage way, ensuring that each end-user location is consistently routed to the same endpoint. Geo DNS provides three levels of geographic granularity: continent, country, and state, and Geo DNS also provides a global record which is served in cases where an end user’s location doesn’t match any of the specific Geo DNS records you have created. You can also combine Geo DNS with other routing types, such as Latency Based Routing and DNS Failover, to enable a variety of low-latency and fault-tolerant architectures. For information on how to configure various routing types, please see the\nAmazon Route 53 documentation\n.\n\nYou can start using Amazon Route 53’s Geo DNS feature quickly and easily by using either the AWS Management Console or the Route 53 API. You simply create a record set and specify the applicable values for that type of record set, mark that record set as a Geo DNS-enabled Record Set, and select the geographic region (global, continent, country, or state) that you want the record to apply to. You can learn more about how to use Geo DNS in the Amazon Route 53 Developer Guide.\n\nYes, we strongly recommend that you configure a global record, to ensure that Route 53 can provide a response to DNS queries from all possible locations—even if you have created specific records for each continent, country, or state where you expect your end users will be located. Route 53 will return the value contained in your global record in the following cases:\nThe DNS query comes from an IP address not recognized by Route 53’s Geo IP database.\nThe DNS query comes from a location not included in any of the specific Geo DNS records you have created.\n\n", "token_count": 485, "metadata": {"service": "ROUTE53", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "route53-faq-14", "chunk_start": 5610, "chunk_end": 6095, "heading_hierarchy": ["Amazon Route 53 FAQs", "DNS Routing Policies", "What is Amazon Route 53's Geo DNS feature?"]}}
{"content": "Machine Learning\n›\nAmazon Textract\n›\nFAQs\n\nGeneral\n15\nBilling\n5\nData Privacy\n7\n\nAmazon Textract is a document analysis service that detects and extracts printed text, handwriting, structured data (such as fields of interest and their values) and tables from images and scans of documents. Amazon Textract's machine learning models have been trained on millions of documents so that virtually any document type you upload is automatically recognized and processed for text extraction. When information is extracted from documents, the service returns a confidence score for each element it identifies so that you can make informed decisions about how you want to use the results. For instance, if you are extracting information from tax documents you can set custom rules to flag any extracted information with a confidence score lower than 95%. Also, all extracted data are returned with bounding box coordinates, which is a rectangular frame that fully encompasses each piece of data identified, so that you can quickly identify where a word or number appears on a document. You can access these features with the Amazon Textract API, in the AWS Management Console, or using the AWS command-line interface (CLI).\n\nThe most common use cases for Amazon Textract include:\nImporting documents and forms into business applications\nCreating smart search indexes\nBuilding automated document processing workflows\nMaintaining compliance in document archives\nExtracting text for Natural Language Processing (NLP)\nExtracting text for document classification\n\nAmazon Textract can detect printed text and handwriting from the Standard English alphabet and ASCII symbols. Amazon Textract can extract printed text, forms and tables in English, German, French, Spanish, Italian and Portuguese. Amazon Textract also extracts explicitly labeled data, implied data, and line items from an itemized list of goods or services from almost any invoice or receipt in English without any templates or configuration. Amazon Textract can also extract specific or implied data such as names and addresses from identity documents in English such as U.S. passports and driver’s licenses without the need for templates or configuration. Finally, Amazon Textract can extract any specific data from documents without worrying about the structure or variations of the data in the document using Queries in English.\n\n", "token_count": 484, "metadata": {"service": "TEXTRACT", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "textract-faq-0", "chunk_start": 0, "chunk_end": 484, "heading_hierarchy": []}}
{"content": "The Amazon Kinesis and DynamoDB Streams records sent to your AWS Lambda function are strictly serialized, per shard. This means that if you put two records in the same shard, Lambda guarantees that your Lambda function will be successfully invoked with the first record before it is invoked with the second record. If the invocation for one record times out, is throttled, or encounters any other error, Lambda will retry until it succeeds (or the record reaches its 24-hour expiration) before moving on to the next record. The ordering of records across different shards is not guaranteed, and processing of each shard happens in parallel.\n\nAWS Lambda allows you to perform time-based aggregations (such as count, max, sum, average, etc.) over a short window of up to 15 minutes for your data in Amazon Kinesis or Amazon DynamoDB Streams over a single logical partition such as a shard. This gives you the option to easily set up simple analytics for your event-based application without adding architectural complexity, as your business and analytics logic can be located in the same function. Lambda allows aggregations over a maximum of a 15-minute tumbling window, based on the event timestamp.\nAmazon Kinesis Data Analytics\nallows you to build more complex analytics applications that support flexible processing choices and robust fault-tolerance with exactly-once processing without duplicates, and analytics that can be performed over an entire data stream across multiple logical partitions. With KDA, you can analyze data over multiple types of aggregation windows (tumbling window, stagger window, sliding window, session window) using either the event time or the processing time.\nAWS Lambda\nAmazon KDA\nTumbling Window\nYes\nYes\nStagger Window\nNo\nYes\nSliding Window\nNo\nYes\nSession Window\nNo\nYes\nEnrichment\nNo\nYes\nJoint input and reference tables\nNo\nYes\nSplit input stream\nNo\nYes\nExactly-once processing\nNo\nYes\nMaximum time window\n15 mins\nNo limit\nAggregation scope\nPartition/shard\nStream\nTime semantics\nEvent time\nEvent time, Processing time\n\n", "token_count": 486, "metadata": {"service": "LAMBDA", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "lambda-faq-10", "chunk_start": 4507, "chunk_end": 4993, "heading_hierarchy": ["AWS Lambda FAQs", "Using AWS Lambda to process AWS events", "Q: How does AWS Lambda process data from Amazon Kinesis streams and Amazon DynamoDB Streams?"]}}
{"content": "Amazon Bedrock Knowledge Bases provides a managed Natural Language to SQL to convert natural language into actionable SQL queries and retrieve data, allowing you to build application using data from these sources.\n\nYes, session context management is built-in, allowing your applications to maintain context across multiple interactions, which is essential for supporting multi-turn conversations.\n\nYes, all information retrieved includes citations, improving transparency and minimizing the risk of hallucinations in the generated responses.\n\nAmazon Bedrock Knowledge Bases supports multi-modal data processing, allowing developers to build generative AI applications that analyze both text and visual data, including images, charts, diagrams, and tables. Model responses can leverage insights from visual elements in addition to text, providing. more accurate and contextually relevant answers. Additionally, source attribution for responses includes visual elements, enhancing transparency and trust in the responses.\n\nAmazon Bedrock Knowledge Bases can process visually rich documents in PDF format, which may contain images, tables, charts, and diagrams. For image-only data, Bedrock Knowledge Bases supports standard image formats like JPEG and PNG, enabling search capabilities where users can retrieve relevant images based on text-based queries.\n\nCustomers have three parsing options for Bedrock Knowledge Bases. For text-only processing, the built-in default Bedrock parser is available at no additional cost, ideal for cases where multimodal data processing is not required. Amazon Bedrock Data Automation (BDA) or foundation models can be used to parse multimodal data. For more information, refer to the\nproduct documentation\n.\n\nAmazon Bedrock Knowledge Base handles various workflow complexities such as content comparison, failure handling, throughput control, and encryption, ensuring that your data is securely processed and managed according to AWS’s stringent security standards.\n\n", "token_count": 399, "metadata": {"service": "BEDROCK", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "bedrock-faq-11", "chunk_start": 4226, "chunk_end": 4625, "heading_hierarchy": ["Amazon Bedrock FAQs", "Knowledge Bases / RAG", "How does Amazon Bedrock Knowledge Base retrieve data from structured data sources?"]}}
{"content": "MSK Provisioned is an MSK cluster deployment option that allows you to manually configure and scale your Apache Kafka clusters. This provides you with varying levels of control over the infrastructure powering your Apache Kafka environment.\nWith MSK Provisioned, you can choose the instance types, storage volumes on Standard broker type, and number of broker nodes that make up your Kafka clusters. You can also scale your cluster by adding or removing brokers as your data processing needs evolve. This flexibility enables you to optimize the clusters for your specific workload requirements, whether that's maximizing throughput, retention capacity, or other performance characteristics.\nIn addition to the infrastructure configuration options, MSK Provisioned provides enterprise-grade security, monitoring, and operational benefits. This includes features such as Apache Kafka version upgrades, built-in security through encryption and access control, and integration with other AWS services such as Amazon CloudWatch for monitoring. MSK Provisioned offers two main broker types—Standard and Express.\nStandard brokers give you the most flexibility to configure your clusters, while Express brokers offer more elasticity, throughput, resilience, and ease-of-use for running high performance streaming applications. See the sub-sections below for more details on each offering. The table below also highlights the key feature comparisons between Standard and Express brokers.\nFeature\nStandard\nExpress\nStorage Management\nCustomer managed (Feature include EBS storage, tiered storage, Provisioned storage throughput, Auto-scaling, Storage capacity alerts)\nFully MSK managed\nSupported instances\nT3, M5, M7g\nM7g\nSizing and scaling considerations\nThroughput, connections, partitions, storage\nThroughput, connections, partitions\nBroker Scaling\nVertical and horizontal scaling\nVertical and horizontal scaling\nKafka versions\nSee documentation\nStarts at version 3.6\nApache Kafka Configuration\nMore configurable\nMostly MSK Managed for higher resilience\nSecurity\nEncryption, Private/Public access, Authentication & Authorization - IAM, SASL/SCRAM, mTLS, plaintext, Kafka ACLs\nEncryption, Private/Public access, Authentication & Authorization - IAM, SASL/SCRAM, mTLS, plaintext, Kafka ACLs\nMonitoring\n", "token_count": 508, "metadata": {"service": "MANAGED", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "managed-faq-6", "chunk_start": 2383, "chunk_end": 2891, "heading_hierarchy": ["FAQs", "Amazon MSK Provisioned", "What is MSK Provisioned?"]}}
{"content": "There are three ways in which ElastiCache for Valkey is different than ElastiCache for Redis OSS. First, with ElastiCache Valkey you can achieve faster scaling and 20% more memory efficiency versus ElastiCache for Redis OSS. Second, ElastiCache for Valkey is priced up to 33% lower than ElastiCache for Redis OSS. Finally, ElastiCache for Valkey uses Valkey, the open source project, which avoids vendor lock-in.\n\nYou can upgrade your existing Redis OSS engine version to the most recent version of Valkey in-place without downtime, and in just a few clicks. For a step-by-step guide to upgrade your engine, see the\nElastiCache engine upgrade documentation\n. You can upgrade from any version of Redis OSS supported on ElastiCache to any version of ElastiCache for Valkey. If you upgrade from Redis OSS versions earlier than 5.0.6, it requires DNS propagation that may experience a failover time of 30 to 60 seconds. For a complete list of changes between all major and minor versions, see the\nElastiCache version documentation\n.\n\nYou can migrate your data from self-managed Valkey or Redis OSS running on Amazon EC2 to Amazon ElastiCache with the\nonline migration capability\n. Alternatively, you can create a snapshot of your Redis OSS cluster running on EC2 and then restore it in ElastiCache for Valkey.\n\nElastiCache will continue to support previous versions of Redis OSS, including Redis OSS 7.1. Over time we will end-of-life older versions of all engines (including Valkey, Memcached, and Redis OSS) and provide some period of time for users to upgrade their engines before a given engine version is deprecated.\n\nYes, you can upgrade your existing ElastiCache for Redis OSS clusters to ElastiCache for Valkey via\nCloudFormation\nby changing the Engine and EngineVersion properties.\n\nAll Redis OSS clients that support Redis OSS 7.2 are fully compatible with all versions of Valkey. You can find a list of official Valkey client libraries on the\nvalkey.io client page\n.\n\n", "token_count": 511, "metadata": {"service": "ELASTICACHE", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "elasticache-faq-8", "chunk_start": 3672, "chunk_end": 4183, "heading_hierarchy": ["Amazon ElastiCache FAQs", "Valkey engine", "How is ElastiCache for Valkey different from ElastiCache for Redis OSS?"]}}
{"content": "Notifications will be delivered to the email addresses provided by the user while creating the notification configuration. Please note that notifications will only be sent to verified email addresses. Additionally, notifications will also be delivered within the\nAWS User Notifications center\nconsole.\n\nAWS Artifact notifications leverage AWS User Notifications service in order to configure notifications and to send emails. You can configure notifications using the AWS Artifact console. You can also view and configure notifications using the AWS User Notifications console.\n\nOn AWS Artifact notifications feature, you can provide up to 20 email addresses per notification configuration. Additionally, service quotas of AWS User Notification service will also apply, please refer to more details\nhere\n.\n\n", "token_count": 156, "metadata": {"service": "ARTIFACT", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "artifact-faq-21", "chunk_start": 9580, "chunk_end": 9736, "heading_hierarchy": ["AWS Artifact FAQs", "Notifications", "6. Where will notifications be delivered?"]}}
{"content": "Error messages are usually the result of your IAM user not having sufficient permissions to perform the desired action in AWS Artifact. Refer to the table below for a complete list of error messages and how to resolve them:\nError message in AWS Artifact console\nIssues\nResolution\nYou don’t have the permissions to accept the agreement\nYou need permissions to accept agreements in AWS Artifact. Contact your account administrator to attach the following permission to your IAM user: artifact:AcceptAgreement\nFor an example IAM policy, refer to\nAgreement Permissions\n.\nYou don’t have the permissions to terminate the agreement\nYou need permissions to terminate agreements in AWS Artifact. Contact your account administrator to attach the following permission to your IAM user: artifact:TerminateAgreement\nFor an example IAM policy, refer to\nAgreement Permissions\n.\nYou don’t have the permissions to download the agreement\nYou need permissions to download agreements in AWS Artifact. Contact your account administrator to attach the following permission to your IAM user: artifact:DownloadAgreement\nFor an example IAM policy, refer to\nAgreement Permissions\n.\nYou don't have the permissions to download this report\nYou need permissions to download reports in AWS Artifact. Contact your account administrator to attach the following permission to your IAM user: artifact:get.\nYour organization must be enabled for all features\nYour organization is configured only for consolidated billing. To use organization agreements in AWS Artifact, your organization must be enabled for all features.\nLearn more\nBefore you can manage agreements for your organization, you need the following permissions: organizations:EnableAWSServiceAccess and organizations:ListAWSServiceAccessForOrganization. These permissions enable AWS Artifact to access organization information in AWS Organizations.\nContact your account administrator to attach the following permission to your IAM user:\niam:CreateRole\niam:AttachRolePolicy\niam:ListRoles\nFor an example IAM policy, refer to\nAgreement Permissions\n.\nBefore you can manage\nagreements for your organization, you need the following permissions to list, create, and attach IAM roles: iam:ListRoles, iam:CreateRole, and iam:AttachRolePolicy.\n", "token_count": 501, "metadata": {"service": "ARTIFACT", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "artifact-faq-17", "chunk_start": 7643, "chunk_end": 8144, "heading_hierarchy": ["AWS Artifact FAQs", "Troubleshooting", "2. I am receiving an error message, what does it mean?"]}}
{"content": "While Amazon Bedrock can be accessed through the AWS Management Console, APIs, or Amazon SageMaker Unified Studio, its capabilities within SageMaker Unified Studio build upon the original Amazon Bedrock Studio (that is no longer available) with several key improvements. When accessed through Amazon SageMaker Unified Studio, it provides access to advanced AI models from leading companies, tools for creating and testing AI prompts, and seamless integration with Amazon Bedrock Knowledge Bases, Amazon Bedrock Guardrails, Amazon Bedrock Flows, and Amazon Bedrock Agents. Teams can collaborate in a shared workspace to build custom AI applications tailored to their needs.\nNew features include a model hub for side-by-side AI model comparison, an expanded playground supporting chat, image, and video interactions, and improved Knowledge Base creation with web crawling. It introduces Agent creation for more complex chat applications and simplifies sharing of AI apps and prompts within organizations. It also offers access to underlying application code and the ability to export chat apps as CloudFormation templates. By managing AWS infrastructure details, it enables users of various skill levels to create AI applications more efficiently, making it a more versatile and powerful tool than its predecessor.\nAmazon Bedrock IDE was renamed to better represent the core capability of Amazon Bedrock being accessed through Amazon SageMaker Unified Studio's governed environment.\n\nWhen accessing Amazon Bedrock’s interface through Amazon SageMaker Unified Studio, teams benefit from a governed environment that enables collaboration. Teams can create projects, invite colleagues, and collaboratively build generative AI applications together. They can receive quick feedback on their prototypes and share the applications with anyone in SageMaker Unified Studio or with specific users in the domain. Robust access controls and governance features allow only authorized members to access project resources such as data or the generative AI applications, supporting data privacy and compliance, and thus fostering secure cross-functional collaboration and sharing. In addition, generative AI applications can be shared from a builder to specific users in the SageMaker Unified Studio domain, or with specific individuals, allowing for proper access rights, controls, and governance of such assets.\n\n", "token_count": 471, "metadata": {"service": "BEDROCK", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "bedrock-faq-24", "chunk_start": 9051, "chunk_end": 9522, "heading_hierarchy": ["Amazon Bedrock FAQs", "Amazon Bedrock in SageMaker Unified Studio", "What are the key features and capabilities of Amazon Bedrock in Amazon SageMaker Unified Studio? How is it different from Amazon Bedrock Studio and Amazon Bedrock IDE?"]}}
{"content": "A source is where your streaming data is continuously generated and captured. For example, a source can be a logging server running on Amazon EC2 instances, an application running on mobile devices, or a sensor on an IoT device. You can connect your sources to Firehose using 1) Amazon Data Firehose API, which uses the AWS SDK for Java, .NET, Node.js, Python, or Ruby. 2) Kinesis Data Stream, where Firehose reads data easily from an existing Kinesis data stream and load it into Firehose destinations. 3) Amazon MSK, where Firehose reads data easily from an existing Amazon MSK cluster and load it into Amazon S3 buckets. 4) AWS natively supported Service like AWS Cloudwatch, AWS EventBridge, AWS IOT, or AWS Pinpoint. For complete list, see the Amazon Data Firehose developer guide. 5) Kinesis Agents, which is a stand-alone Java software application that continuously monitors a set of files and sends new data to your stream. 6) Fluentbit, which an open source Log Processor and Forwarder. 7) AWS Lambda, which is a serverless compute service that lets you run code without provisioning or managing servers. You can use write your Lambda function to send traffic from S3 or DynamoDB to Firehose based on a triggered event.\n\nA destination is the data store where your data will be delivered. Firehose currently supports Amazon S3, Amazon Redshift, Amazon OpenSearch Service, Snowflake, Apache Iceberg tables, Splunk, Datadog, NewRelic, Dynatrace, Sumo Logic, LogicMonitor, MongoDB, and HTTP End Point as destinations.\n\n", "token_count": 383, "metadata": {"service": "DATA", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "data-faq-1", "chunk_start": 387, "chunk_end": 770, "heading_hierarchy": ["Amazon Data Firehose FAQs", "General and Streaming ETL Concepts", "What is a source in Firehose?"]}}
{"content": "Yes, VPC endpoints are supported for data movement use cases. You can\nuse VPC endpoints\nto ensure data transferred between your AWS DataSync agent, either deployed on-premises or in-cloud, doesn't traverse the public internet or need public IP addresses. Using VPC endpoints increases the security of your data by keeping network traffic within your\nAmazon Virtual Private Cloud (Amazon VPC)\n. VPC endpoints for DataSync are powered by\nAWS PrivateLink\n, a highly available, scalable technology that enables you to privately connect your VPC to supported AWS services.\n\nTo use VPC endpoints with AWS DataSync, you create an\nAWS PrivateLink\ninterface VPC endpoint for the DataSync service in your chosen VPC, and then choose this endpoint elastic network interface (ENI) when creating your DataSync agent. Your agent will connect to this ENI to activate, and subsequently all data transferred by the agent will remain within your configured VPC. You can use either the\nAWS DataSync Console\n, AWS Command Line Interface (CLI), or AWS SDK, to configure VPC endpoints. To learn more, see\nUsing AWS DataSync in a Virtual Private Cloud\n.\n\nAWS DataSync supports moving data to, from, or between Amazon Simple Storage Service\n(Amazon S3)\n, Amazon Elastic File System\n(Amazon EFS)\n,\nAmazon FSx for Windows File Server\n,\nAmazon FSx for Lustre\n,\nAmazon FSx for OpenZFS\n, and\nAmazon FSx for NetApp ONTAP.\n\n", "token_count": 360, "metadata": {"service": "DATASYNC", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "datasync-faq-11", "chunk_start": 4265, "chunk_end": 4625, "heading_hierarchy": ["AWS DataSync FAQs", "Usage", "Does AWS DataSync support VPC endpoints or AWS PrivateLink?"]}}
{"content": "IAM Access Analyzer custom policy checks validate before deployments that your IAM policies adhere to your security standards. Custom policy checks use the power of\nautomated reasoning\n—provable security assurance backed by mathematical proof—so that security teams can proactively detect nonconformant updates to policies. For example, IAM policy changes that are more permissive than their previous version would be flagged for additional review. Security teams can use these checks to streamline their reviews, automatically approving policies that conform with their security standards and inspecting more deeply when they don't. This kind of validation provides higher security assurance in the cloud. Security and development teams can automate policy reviews at scale by integrating these custom policy checks into the tools and environments where developers author their policies, such as their CI/CD pipelines.\n\nIAM Access Analyzer simplifies inspecting unused access to guide you toward least privilege. Security teams can use IAM Access Analyzer to gain visibility into unused access across their AWS organization and automate how they rightsize permissions. When an unused access analyzer is enabled, IAM Access Analyzer continuously analyzes your accounts to identify unused access and creates a centralized dashboard with findings. Security teams can use the dashboard to review findings centrally and prioritize which accounts to review based on the volume of findings. The findings highlight unused roles, unused access keys for IAM users, and unused passwords for IAM users. For active IAM roles and users, the findings provide visibility into unused services and actions.\n\n", "token_count": 329, "metadata": {"service": "IAM", "doc_type": "Guide", "source_file": "aws-iam-faq.html", "chunk_id": "iam-aws-iam-faq-8", "chunk_start": 3185, "chunk_end": 3514, "heading_hierarchy": ["AWS Identity and Access Management (IAM) FAQs", "Analyzing access", "What are IAM Access Analyzer custom policy checks?"]}}
{"content": "MemoryDB is a durable, in-memory database for workloads that require an ultra-fast, Valkey- or Redis OSS-compatible primary database. You should consider using MemoryDB if your workload requires a durable database that provides ultra-fast performance (microsecond read and single-digit millisecond write latency). MemoryDB may also be a good fit for your use case if you want to build an application using Valkey or Redis OSS data structures and APIs with a primary, durable database. Finally, you should consider using MemoryDB to simplify your application architecture and lower costs by replacing usage of a database with a cache for durability and performance.\nElastiCache is a service that is commonly used to cache data from other databases and data stores using Valkey, Memcached, or Redis OSS. You should consider ElastiCache for caching workloads where you want to accelerate data access with your existing primary database or data store (microsecond read and write performance). You should also consider ElastiCache for use cases where you want to use Valkey or Redis OSS data structures and APIs to access data stored in a primary database or data store.\n\nPlease refer to the\nservice level agreement (SLA)\n.\n\nFor current limits and quotas, see the MemoryDB documentation.\n\nAmazon MemoryDB Multi-Region is a fully-managed, active-active, multi-Region database enabling you to build applications with up to 99.999% availability and microsecond read and single-digit millisecond write latency. It provides data redundancy across multiple AWS Regions, so you can improve availability and resiliency of your multi-Region applications, even if an application processing is interrupted in one Region and cannot connect to its MemoryDB endpoint. MemoryDB Multi-Region offers active-active replication so you can serve reads and writes locally from the Regions closest to your customers with microsecond read and single-digit millisecond write latency. It asynchronously replicates data between Regions, and data is typically propagated within a second. MemoryDB Multi-Region automatically resolves update conflicts and corrects for data divergence issues, so you can focus on your application.\n\n", "token_count": 479, "metadata": {"service": "MEMORYDB", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "memorydb-faq-2", "chunk_start": 722, "chunk_end": 1201, "heading_hierarchy": ["Amazon MemoryDB FAQs", "General", "When should I use MemoryDB versus Amazon ElastiCache?"]}}
{"content": "No. When a member account leaves an organization, any accepted organization agreement(s) no longer apply to that account. If the member account wants one or more of the agreements to continue to apply after leaving the organization, the member account should accept the relevant account agreement(s) under the\nAccount agreements\ntab in AWS Artifact prior to leaving the organization.\n\nYou may use any AWS service in an account designated as a HIPAA Account, but you may only include PHI in HIPAA Eligible Services. Our\nHIPAA Eligible Services Reference\npage contains the latest list of HIPAA Eligible Services.\n\nYes. If you prefer to enter into an offline BAA with AWS, please contact your AWS Account Manager or\ncontact us\nto submit your request. However, we encourage you to take advantage of the speed, efficiency and visibility provided by AWS Artifact Agreements.\n\nIf you previously signed an offline BAA, the terms of that BAA will continue to apply to the accounts you designated as HIPAA Accounts under that offline BAA.\nFor any accounts that you have not already designated as a HIPAA Account under your offline BAA, you can use AWS Artifact Agreements to accept an online BAA for those accounts.\n\nYes. The management account in your organization can use the\nOrganization agreements\ntab in AWS Artifact Agreements to accept an organization BAA on behalf of all existing and future member accounts in your organization.\n\nNo. In order to protect the confidentiality of your offline BAA, you will not be able to download a copy of it in AWS Artifact Agreements. If you would like to view a copy of your previously signed offline BAA, you can reach out to your AWS Account Manager to request it.\n\n", "token_count": 385, "metadata": {"service": "ARTIFACT", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "artifact-faq-10", "chunk_start": 4554, "chunk_end": 4939, "heading_hierarchy": ["AWS Artifact FAQs", "Business Associate Addendum (BAA)", "9. If a member account leaves an organization, does the organization agreement still apply to the account?"]}}
{"content": "Management and Governance\n›\nAWS License Manager\n›\nFAQs\n\nGeneral\n7\nCompliance\n1\nManaged entitlements\n8\nAutomated discovery\n8\nUser-based license subscriptions\n4\nLinux subscriptions\n6\nPricing\n1\n\nManaged entitlements\n\nUser-based license subscriptions\n\n", "token_count": 81, "metadata": {"service": "LICENSE", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "license-faq-0", "chunk_start": 0, "chunk_end": 81, "heading_hierarchy": []}}
{"content": "AWS Config rules do not directly affect how end users consume AWS. AWS Config rules evaluate resource configurations only after a configuration change has been completed and recorded by AWS Config. AWS Config rules do not prevent the user from making changes that could be non-compliant. To control what you can provision on AWS and configuration parameters used during provisioning, use\nAWS Identity and Access Management\n(IAM) Policies and\nAWS Service Catalog\nrespectively.\n\nYes, AWS Config rules can be set to proactive only, detective only, or both proactive and detective modes. For a full list of these rules,\nsee documentation\n.\n\nYou can use the existing PutConfigRule API or the AWS Config console to enable proactive mode on an AWS Config rule in your account.\n\nAWS Config helps you record configurations for third-party resources or custom resource types such as on-premises servers, software as a service (SaaS) monitoring tools, and version control systems. To do this, you must create a resource provider schema that conforms to and validates the configuration of the resource type. You must register the custom resource using\nAWS CloudFormation\nor your infrastructure as code (IaC) tool.\nIf you have configured AWS Config to record all resource types, then third-party resources that are managed (created, updated, or deleted) through AWS CloudFormation are automatically tracked in AWS Config as configuration items. To dive deeper into the steps required for this and understand in which AWS Regions this is available, see the AWS Config Developer Guide:\nRecord Configurations for Third-Party Resources\n.\n\n", "token_count": 346, "metadata": {"service": "CONFIG", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "config-faq-2", "chunk_start": 922, "chunk_end": 1268, "heading_hierarchy": ["Amazon Config FAQs", "General", "Does the service prevent users from taking non-compliant actions?"]}}
{"content": "Zero-ETL is a set of fully managed integrations by AWS that removes or minimizes the need to build extract, transform, and load (ETL) data pipelines. Zero-ETL makes data available in the\nlakehouse in SageMaker\nand\nAmazon Redshift\nfrom multiple operational sources, transactional sources, and enterprise applications. ETL is the process of combining, cleaning, and normalizing data from different sources to get it ready for analytics,\nAI\n,\nand\nML\nworkloads. Traditional ETL processes are time-consuming and complex to develop, maintain, and scale. Instead, zero-ETL integrations facilitate point-to-point data movement without the need to create and operate ETL data pipelines.\nVisit\nWhat is zero-ETL\n?\nto learn more.\n\nThe zero-ETL integrations solve many of the existing data movement challenges in traditional ETL processes, including:\nIncreased system complexity due to intricate data-mapping rules, error handling, and security requirements\nAdditional costs from growing data volumes, infrastructure upgrades, and maintenance\nDelayed time to analytics, AI, and ML due to custom code development and deployment, causing missed opportunities for real-time use cases.\n\nIncreased agility: Zero-ETL simplifies data architecture and reduces data-engineering efforts. It allows for the inclusion of new data sources without the need to reprocess large amounts of data. This flexibility enhances agility, supporting data-driven decision-making and rapid innovation.\nCost-efficiency: Zero-ETL uses data integration technologies that are cloud-native and scalable, allowing businesses to optimize costs based on actual usage and data-processing needs. Organizations reduce infrastructure costs, development efforts, and maintenance overheads.\nFast time to insights: Traditional ETL processes often involve periodic batch updates, resulting in delayed data availability. Zero-ETL integrations, on the other hand, provides near real-time data access, to help provide fresher data for analytics, AI/ML, and reporting. You get more accurate and timely insights for use cases like real-time dashboards, optimized gaming experience, data-quality monitoring, and customer behavior analysis. Organizations can make data-driven predictions with more confidence, improve customer experiences, and promote data-driven insights across the business.\n\n", "token_count": 505, "metadata": {"service": "REDSHIFT", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "redshift-faq-20", "chunk_start": 8168, "chunk_end": 8673, "heading_hierarchy": ["Amazon Redshift FAQs", "Zero-ETL integrations", "What is zero-ETL?"]}}
{"content": "Amazon Personalize has launched an integration with self-managed OpenSearch that enables you to personalize search results for each user and assists in predicting your search needs. The Amazon Personalize Search Ranking plugin within OpenSearch helps you to leverage the deep learning capabilities offered by Amazon Personalize and apply personalized re-ranking to OpenSearch search results, without any ML expertise. With personalized search, you can go beyond the traditional keyword matching approach and boost relevant items in a specific user's search results based on their interests, context, and past interactions in real-time. You can also fine tune the level of personalization for every search query to have greater control over your search experience, improving the end-user engagement and conversion from their search.\n\nThe Amazon Personalize Search Ranking plugin is available for both self-managed and Amazon OpenSearch. If you are using Amazon OpenSearch, to get started, simply\nsetup\nan OpenSearch domain, then\nsetup\nAmazon Personalize campaign with AWS-personalized-ranking recipe. Next, associate the Amazon Personalize Search Ranking plugin with your domain, and finally configure the plugin. You can also use OpenSearch dashboard to compare your search results.\nIf you are using self-managed OpenSearch, to get started, simply\nsetup\nan OpenSearch cluster, then\nsetup\nAmazon Personalize campaign with AWS-personalized-ranking recipe, and finally,\ninstall and configure\nthe Amazon Personalize Search Ranking plugin within OpenSearch. You can use OpenSearch dashboard to compare your search results.\nTo learn more, visit our\ndocumentation\n.\n\nWith the Amazon Personalize Next-Best-Action (aws-next-best-action) recipe, you can determine the next best action to recommend to each individual user based on their preferences, interests and history in real-time. You can recommend actions such as an add-on service, joining a customer loyalty program, subscribing to a newsletter, etc. that encourage conversion. This allows you to improve each user’s experience by nudging them to take certain actions across their user journey that will help in promoting long-term brand engagement. It also enables you to improve return on marketing investment by recommending actions that have a high degree of relevance to the user, thereby resulting in increased revenue and loyalty.\nLearn more.\n\n", "token_count": 496, "metadata": {"service": "PERSONALIZE", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "personalize-faq-6", "chunk_start": 1991, "chunk_end": 2487, "heading_hierarchy": ["Amazon Personalize FAQs", "Using Amazon Personalize", "Can I personalize my search results using Amazon Personalize?"]}}
{"content": "Yes. You can use Amazon RDS Proxy APIs to create a proxy and then define target groups to associate the proxy with specific database instances or clusters. For example:\naws rds create-db-proxy \n        --db-proxy-name '…' \n        --engine-family <mysql|postgresql>       \n        --auth [{}, {}] \n        --role-arn '…'\n        --subnet-ids {}\n        --require-tls <true|false>\n        --tags {}\naws rds register-db-proxy-targets \n        --target-group-name '…'\n        --db-cluster-identifier  '…'\n        --db-instance-identifier '…'\n\nTrusted Language Extensions (TLE) for PostgreSQL\nenables developers to build high performance PostgreSQL extensions and run them safely on\nAmazon Aurora\nand\nAmazon RDS\n. In doing so, TLE improves your time to market and removes the burden placed on database administrators to certify custom and third-party code for use in production database workloads. You can move forward as soon as you decide an extension meets your needs. With TLE, independent software vendors (ISVs) can provide new PostgreSQL extensions to customers running on Aurora and Amazon RDS.\n\n", "token_count": 292, "metadata": {"service": "RDS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "rds-faq-52", "chunk_start": 20376, "chunk_end": 20668, "heading_hierarchy": ["Amazon RDS FAQs", "Amazon RDS Proxy", "Can I access Amazon RDS Proxy using APIs?"]}}
{"content": "With ElastiCache Serverless, you only pay for the data you store and the compute your application uses. Visit the\nElastiCache pricing page\nto learn more.\n\nReserved Nodes or Reserved Instances (RIs) provide you with a significant discount over on-demand usage when you commit to a one-year or three-year term. With Reserved Nodes, you can make a one-time, up-front payment to create a one- or three-year reservation to run your cache in a specific Region and receive a significant discount off the ongoing hourly usage charge. There are three Reserved Node types – All Upfront, No Upfront, and Partial Upfront – that allow you to balance the amount you pay upfront with your effective hourly price.\n\nYour Redis OSS reservations automatically apply to Valkey nodes in the same instance family and Region. Because Valkey is priced 20% lower than Redis OSS, if you have an existing Redis OSS reserved node, you can upgrade your cache to the Valkey engine and continue receiving your reservation benefits with 20% more value. For example, if you purchased a reservation for 5 cache.r7g.2xlarge nodes for the Redis OSS engine, then when you upgrade your nodes to the Valkey engine, you can create a sixth cache.r7g2xlarge node (20% more than 5 nodes) in the same region at no additional cost.\"\n\nReserved Nodes provide a discount that applies to ElastiCache on-demand usage. ElastiCache Serverless is not compatible with Reserved Nodes.\n\nYou can purchase up to 300 Reserved Nodes. If you wish to run more than 300 nodes, please complete the\nElastiCache node request form\n.\n\nPurchase a node reservation with the same node class within the same Region as the node you are currently running and would like to reserve. If the reservation purchase is successful, ElastiCache will automatically apply your new hourly usage charge to your existing node.\n\n", "token_count": 451, "metadata": {"service": "ELASTICACHE", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "elasticache-faq-3", "chunk_start": 1315, "chunk_end": 1766, "heading_hierarchy": ["Amazon ElastiCache FAQs", "Serverless", "What is ElastiCache Serverless?"]}}
{"content": "When you create or update your Firehose stream through AWS console or Firehose APIs, you can configure Direct PUT as the source of your Firehose stream. Once the stream is created, you can configure the created Firehose stream as your Firehose stream in the Vended Logs section of the VPC Flow logs console.\n\nYou add data to yourFirehose stream from CloudWatch Logs by creating a CloudWatch Logs subscription filter that sends events to your Firehose stream. For more information, see Using\nCloudWatch Logs Subscription Filters\nin Amazon CloudWatch user guide.\n\nYou add data to yourFirehose stream from CloudWatch Events by creating a CloudWatch Events rule with your Firehose stream as target. For more information, see\nWriting to Amazon Data Firehose Using CloudWatch Events\nin the Firehose developer guide.\n\nYou add data to your Firehose stream from AWS EventBridge console. For more information, see\nAWS EventBridge documentation\n.\n\nFirehose allows you to encrypt your data after it’s delivered to your Amazon S3 bucket. While creating your Firehose stream, you can choose to encrypt your data with an AWS Key Management Service (KMS) key that you own. For more information about KMS, see\nAWS Key Management Service\n.\n\nFirehose assumes the IAM role you specify to access resources such as your Amazon S3 bucket and Amazon OpenSearch domain. For more information, see\nControlling Access with Firehose\nin the Firehose developer guide.\n\n", "token_count": 347, "metadata": {"service": "DATA", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "data-faq-5", "chunk_start": 2249, "chunk_end": 2596, "heading_hierarchy": ["Amazon Data Firehose FAQs", "Data Sources", "How can I stream my VPC flow logs to Firehose?"]}}
{"content": "Amazon EMR uses the Hadoop data processing engine to conduct computations implemented in the MapReduce programming model. The customer implements their algorithm in terms of map() and reduce() functions. The service starts a customer-specified number of Amazon EC2 instances, comprised of one master and multiple other nodes. Amazon EMR runs Hadoop software on these instances. The master node divides input data into blocks, and distributes the processing of the blocks to the other nodes. Each node then runs the map function on the data it has been allocated, generating intermediate data. The intermediate data is then sorted and partitioned and sent to processes which apply the reducer function to it locally on the nodes. Finally, the output from the reducer tasks is collected in files. A single “cluster” may involve a sequence of such MapReduce steps.\n\nSee the\nEMR pricing page\nfor details on latest available instance types and pricing per region.\n\nThe time to run your cluster will depend on several factors including the type of your cluster, the amount of input data, and the number and type of Amazon EC2 instances you choose for your cluster.\n\nYes. You can launch an EMR cluster (version 5.23 or later) with three master nodes and support high availability of applications like YARN Resource Manager, HDFS Name Node, Spark, Hive, and Ganglia. Amazon EMR automatically fails over to a standby master node if the primary master node fails or if critical processes, like Resource Manager or Name Node, crash. Since the master node is not a potential single point of failure, you can run your long-lived EMR clusters without interruption. In the event of a failover, Amazon EMR automatically replaces the failed master node with a new master node with the same configuration and boot-strap actions.\n\nYes. Amazon EMR is fault tolerant for node failures and continues job execution if a node goes down. Amazon EMR will also provision a new node when a core node fails. However, Amazon EMR will not replace nodes if all nodes in the cluster are lost.\n\nYes. You can SSH onto your cluster nodes and execute Hadoop commands directly from there. If you need to SSH into a specific node, you have to first SSH to the master node, and then SSH into the desired node.\n\n", "token_count": 498, "metadata": {"service": "EMR", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "emr-faq-15", "chunk_start": 6677, "chunk_end": 7175, "heading_hierarchy": ["Amazon EMR FAQs", "Amazon EMR on Amazon EC2", "Q: How is a computation done in Amazon EMR?"]}}
{"content": "In Apache Kafka, brokers are the individual servers that make up the Apache Kafka cluster. They are responsible for storing and replicating the data published to Kafka topics, managing the partitions within those topics, handling client requests (producing and consuming messages), and coordinating with each other to maintain the overall state of the Kafka deployment. Brokers are the core components that enable Kafka's distributed, scalable, and fault-tolerant architecture.\n\nFor provisioned clusters, you can choose EC2 T3.small instances or instances within the EC2 M7g and M5 instance families. For serverless clusters, brokers are completely abstracted. MSK also offers Standard and Express Broker types.\n\nNo, each broker you provision includes boot volume storage managed by the Amazon MSK service.\n\nSome resources, such as elastic network interfaces (ENIs), will show up in your Amazon EC2 account. Other Amazon MSK resources will not show up in your Amazon EC2 account because they are managed by the Amazon MSK service.\n\nFor provisioned clusters, you need to provision broker instances with every cluster you create. On Standard brokers, you will provision storage and optionally enable provisioned storage throughput for storage volumes, which can be used to scale I/O without having to provision additional brokers. With Express brokers, you do not need to provision or manage storage. For all cluster types, you do not need to provision metadata nodes such as Apache ZooKeeper or KRaft nodes because these resources are included at no additional charge with each cluster you create. For serverless clusters, you just create a cluster as a resource.\n\n", "token_count": 357, "metadata": {"service": "MANAGED", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "managed-faq-3", "chunk_start": 1426, "chunk_end": 1783, "heading_hierarchy": ["FAQs", "Resources", "What are brokers?"]}}
{"content": "If you want to use MongoDB APIs, you should use the native database capabilities in Amazon DocumentDB to perform vector search on your documents. The Amazon DocumentDB zero-ETL integration with Amazon OpenSearch Service is well suited for searching across collections and for storing and indexing vectors with more than 2,000 dimensions.\n\nThe zero-ETL integration of Amazon DocumentDB with Amazon OpenSearch Service uses Amazon OpenSearch Ingestion to seamlessly move operational data from Amazon DocumentDB to Amazon OpenSearch Service. To get started, you enable change stream functionality on the Amazon DocumentDB collection that needs to be replicated. The zero-ETL integration feature sets up an Amazon OpenSearch Ingestion pipeline in your account that automatically replicates the data to an Amazon OpenSearch Service managed cluster or serverless collection.\nAmazon OpenSearch Ingestion automatically understands the format of the data in Amazon DocumentDB collections and maps the data to Amazon OpenSearch Service to yield the most performant search results. You can synchronize data from multiple Amazon DocumentDB collections via multiple pipelines into one Amazon OpenSearch managed cluster or serverless collection to offer holistic insights across several applications. Optionally, you can specify custom data processors when defining the ingestion configuration in Amazon OpenSearch Service. Subsequent updates to Amazon DocumentDB collections are also replicated to Amazon OpenSearch Service without any manual intervention.\n\nThis zero-ETL leverages the native data transformational capabilities of Amazon OpenSearch Ingestion pipelines to aggregate and filter the data while it is in motion.\n\nYou can also write custom transformation logic if you want bespoke transformational capability, and Amazon OpenSearch Ingestion will manage the transformation process. Alternatively, if want to move entire data from source to sink without customization, Amazon OpenSearch Ingestion provides out-of-the box blueprints so that you can perform the integrations with just a few button clicks.\n\nIn order to ensure that Amazon OpenSearch Ingestion has the necessary permissions to replicate data from Amazon DocumentDB, the zero-ETL integration feature creates an IAM role with the necessary permissions to read data from Amazon DocumentDB collection and write to an Amazon OpenSearch domain or collection. This role is then assumed by Amazon OpenSearch Ingestion pipelines to ensure that the right security posture is always maintained when moving the data from source to destination.\n\n", "token_count": 504, "metadata": {"service": "DOCUMENTDB", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "documentdb-faq-16", "chunk_start": 7281, "chunk_end": 7785, "heading_hierarchy": ["Amazon DocumentDB (with MongoDB compatibility) FAQs", "Zero-ETL integration", "When I want to perform vector search for my generative AI use case, when should I use Amazon DocumentDB native vector search capabilities versus zero-ETL integration with Amazon OpenSearch Service?"]}}
{"content": "You should use S3 Object Lambda if you want to process data inline with an S3 GET, LIST, or HEAD request. You can use S3 Object Lambda to share a single copy of your data across many applications, avoiding the need to build and operate custom processing infrastructure or to store derivative copies of your data. For example, by using S3 Object Lambda to process S3 GET requests, you can mask sensitive data for compliance purposes, restructure raw data for the purpose of making it compatible with machine learning applications, filter data to restrict access to specific content within an S3 object, or to address a wide range of additional use cases. You can use S3 Object Lambda to enrich your object lists by querying an external index that contains additional object metadata, filter and mask your object lists to only include objects with a specific object tag, or add a file extension to all the object names in your object lists. For example, if you have an S3 bucket with multiple discrete data sets, you can use S3 Object Lambda to filter an S3 LIST response depending on the requester.  S3 Object Lambda can be set up with just a few clicks in the Amazon S3 Management Console. Read the\nuser guide\nto learn more.\n\nS3 Object Lambda uses Lambda functions specified by you to process the output of GET, LIST, and HEAD requests. Once you have defined a Lambda function to process requested data, you can attach that function to an S3 Object Lambda Access Point. GET, LIST, and HEAD requests made through an S3 Object Lambda Access Point will now invoke the specified Lambda function. Lambda will then fetch the S3 object requested by the client and process that object. Once processing has completed, Lambda will stream the processed object back to the calling client. Read the S3 Object Lambda\nuser guide to learn more\n.\n\n", "token_count": 402, "metadata": {"service": "S3", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "s3-faq-116", "chunk_start": 45011, "chunk_end": 45413, "heading_hierarchy": ["Amazon S3 FAQs", "Data processing", "Why should I use S3 Object Lambda?"]}}
{"content": "Yes. Variables can be added to any existing workflow by assigning Variables. Enhanced data transformations with JSONata are available in any existing workflow by declaring \"QueryLanguage\":\"JSONata\" as the query language on individual states or for the entire workflow. Current customers with production workloads can take advantage of Variables and JSONata in existing workflows, and they can choose to opt in to use the improved data directives in new workflows. The capabilities introduced by Variables and JSONata, applied collectively, provide an easier learning path for new customers as they onboard to Step Functions within the current version of ASL. With this approach, all future Step Functions features will be available to all customers without requiring them to upgrade their workflows to use a new version of ASL.\n\nInstead of using JSONPath and Intrinsic Functions to perform data selection and transformation as you do today, you will use JSONata. To write a JSONata expression using \"QueryLanguage\":\"JSONata\", surround it with {% %} like this: \"{% JSONata expression %}\". For example, you can construct a date for today like this: \"DateStamp\": \"{% $now() %}\".\n\nJSONPath uses varying combinations of five primary fields to query and transform data within each state (InputPath, Parameters, ResultSelector, ResultPath and OutputPath), which are challenging to reason about for developers. Now, with JSONata, you can now opt-in to use \"QueryLanguage\":\"JSONata\" replacing the five fields with two new fields (Arguments and Output). The new Arguments field provides additional capabilities for constructing the value to send to an API or sub-workflow, and the new Output field provides additional capabilities for constructing the state output. Both fields accept JSONata for data manipulation. \"QueryLanguage\":\"JSONata\" provides a simplified set of rules that no longer use the ”.$” convention for field names, and instead enclose JSONata expressions in {% %}. You can now write Choice state conditions on a single line using the new Condition field. The Condition field accepts either a boolean value or a string value which must be a JSONPath or JSONata expression depending on the QueryLanguage setting.\n\n", "token_count": 473, "metadata": {"service": "STEP", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "step-faq-5", "chunk_start": 1839, "chunk_end": 2312, "heading_hierarchy": ["AWS Step Functions FAQs", "Overview", "Can I use Variables and JSONata in my existing workflows?"]}}
{"content": "Domains purchased through all domain registrars can be connected to an app by defining a custom domain. For developers using Amazon Route53 as their registrar, AWS Amplify automatically updates the DNS records to point to their deployed app. For 3rd party registrars, AWS Amplify provides instructions on how to update their DNS records.\n\nAWS Amplify web hosting generates a free HTTPS on all sites and will enable it automatically on all Route53-managed domains. The SSL certificate is generated by Amazon Certificate Manager and has wildcard domain support. ACM handles the complexity of creating and managing public SSL/TLS certificates for your AWS based websites and applications. With the wildcard option, the main domain and all subdomains can be covered by a single certificate.\n\nAll web deployments can be password protected with basic access authentication. When working on new features, developers can share updates with internal stakeholders by setting up a username and password for a branch deployment.\n\nA redirect is a client-side request to have the web browser go to another URL. This means that the URL that you see in the browser will update to the new URL. A rewrite is a server-side rewrite of the URL. This will not change what you see in the browser because the changes are hidden from the user. Reverse proxies are cross-origin rewrites. From the AWS Amplify console settings, developers can specify redirects, HTTP response code, custom 404s, and proxies to external services.\n\nAWS Amplify web hosting is priced for two features – build & deploy, and web hosting. For the build & deploy feature the price per build minute is $0.01. For the hosting feature the price per GB served is $0.15 and price per GB stored is $0.023.With the AWS Free Usage Tier, you can get started for free. Upon sign up, new AWS customers receive 1,000 build minutes per month for the build and deploy feature, and 15 GB served per month and 5 GB data storage per month for the hosting feature.\n\nExcept as otherwise noted, our prices are exclusive of applicable taxes and duties, including VAT and applicable sales tax. For customers with a Japanese billing address, use of AWS services is subject to Japanese Consumption Tax. Learn more.\n\n", "token_count": 509, "metadata": {"service": "AMPLIFY", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "amplify-faq-6", "chunk_start": 2603, "chunk_end": 3112, "heading_hierarchy": ["AWS Amplify FAQs", "Tools & Features", "What domain registrars does AWS Amplify web hosting support?"]}}
{"content": "and indicate that your request is for “free data transfer to move off AWS.” AWS Customer Support will ask that you provide information, so they can review your moving plans, evaluate whether you qualify for free data transfer out, and calculate a proper credit amount. 4) If AWS Customer Support approves your move, you will receive a temporary credit for the cost of data transfer out based on the volume of all data you have stored across AWS services at the time of AWS’ calculation. AWS Customer Support will notify you if you are approved, and you will then have 60 days to complete your move off of AWS. The credit will count against data transfer out usage only, and it will not be applied to other service usage. After your move away from AWS services, within the 60-day period, you must delete all remaining data and workloads from your AWS account, or you can close your AWS account. Free data transfers for moving IT providers are also subject to the following criteria: a) Only customers with an active AWS account in good standing are eligible for free data transfer out. b) If you have less than 100 GB of data stored in your AWS account you may move this data off of AWS for free under AWS’s existing 100 GB monthly free tier for data transfer out. Customers with less than 100 GB of data stored in their AWS account are not eligible for additional credits. c) AWS will provide you with free data transfer out to the internet when you move all of your data off of AWS. If you only want to move your total usage of a single service, but not everything, contact AWS Customer Support. d) If your plans change, or you cannot complete your move off of AWS within 60 days, you must notify AWS Customer Support. e) Standard services charges for use of AWS services are not included. Only data transfer out charges in support of your move off of AWS are eligible for credits. However, data transfer out from specialized data transfer services, such as Amazon CloudFront, AWS Direct Connect, AWS Snowball, and AWS Global Accelerator, are not included. f) AWS may review your service usage to verify compliance with these requirements. If we determine your use of data transfer out was for a purpose other than moving off of AWS, we may charge you for the data transfer out that had been credited. g) AWS may make changes with respect to free data transfers out to the internet at any time.\n\n", "token_count": 511, "metadata": {"service": "S3", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "s3-faq-13", "chunk_start": 4972, "chunk_end": 5483, "heading_hierarchy": ["Amazon S3 FAQs", "Billing", "I want to move my data out of AWS. How do I request free data transfer out to the internet?"]}}
{"content": "You cannot use metrics like bytes pending, operations pending, and replication latency to track S3 Batch Replication progress. However, you can use the operations failed replication metric to monitor existing objects that do not replicate successfully with S3 Batch Replication. Additionally, you can also use S3 Batch Operations completion reports to keep track of objects replicating with S3 Batch Replication.\n\nAmazon S3 Replication Time Control is designed to replicate 99.99% of your objects within 15 minutes, and is backed by a service level agreement. If fewer than 99.9% of your objects are replicated in 15 minutes for each replication region pair during a monthly billing cycle, the S3 RTC SLA provides a service credit on any object that takes longer than 15 minutes to replicate. The service credit covers a percentage of all replication-related charges associated with the objects that did not meet the SLA, including the RTC charge, replication bandwidth and request charges, and the cost associated with storing your replica in the destination region in the monthly billing cycle affected. To learn more, read the\nS3 Replication Time Control SLA\n.\n\n", "token_count": 263, "metadata": {"service": "S3", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "s3-faq-109", "chunk_start": 42409, "chunk_end": 42672, "heading_hierarchy": ["Amazon S3 FAQs", "Replication", "Can I use Amazon S3 Replication metrics and events to track S3 Batch Replication?"]}}
{"content": "Cross-account observability in CloudWatch lets you monitor and troubleshoot applications that span across multiple accounts within a Region. Using cross-account observability, you can seamlessly search, visualize, and analyze your metrics, logs, and traces, without having to worry about account boundaries. You can start with an aggregated cross-account view of your application to visually identify the resources exhibiting errors and dive deep into correlated traces, metrics, and logs to root cause the issue. The seamless cross-account data access and navigation enabled by cross-account monitoring helps you reduce the manual effort required to troubleshoot issues and save valuable time in resolution. Cross-account observability is an addition to CloudWatch’s unified observability capability.\n\nCross-account observability introduces two new account concepts. “Monitoring account” is a central AWS account that can view and interact with observability data generated across other accounts. A “source account” is an individual AWS account that generates observability data for the resources that reside in it. Once you identify your monitoring and source accounts, you complete your cross-account monitoring configuration by selecting which telemetry data to share with your monitoring account. Within minutes, you can easily setup central monitoring accounts from which you have a complete view of the health and performance of your applications deployed across many related accounts or an entire AWS organization. With cross-account observability in CloudWatch, you can get a birds-eye view of your cross-application dependencies that can impact service availability, and you can pinpoint issues proactively and troubleshoot with reduced mean time to resolution.\n\n", "token_count": 335, "metadata": {"service": "CLOUDWATCH", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "cloudwatch-faq-6", "chunk_start": 2458, "chunk_end": 2793, "heading_hierarchy": ["Amazon CloudWatch FAQs", "Cross-account observability", "What is cross-account observability in CloudWatch?"]}}
{"content": "You can delete a read replica with a few steps of the AWS Management Console or by passing its DB Instance identifier to the DeleteDBInstance API.\nAn Amazon Aurora replica will stay active and continue accepting read traffic even after its corresponding source DB Instance has been deleted. One of the replicas in the cluster will automatically be promoted as the new primary and will start accepting write traffic.\nAn Amazon RDS for MySQL or MariaDB read replica will stay active and continue accepting read traffic even after its corresponding source DB instance has been deleted. If you desire to delete the Read Replica in addition to the source DB instance, you must explicitly do so using the DeleteDBInstance API or AWS Management Console.\nIf you delete an Amazon RDS for PostgreSQL DB Instance that has read replicas, all Read Replicas will be promoted to standalone DB Instances and will be able to accept both read and write traffic. The newly promoted DB Instances will operate independently of one another. If you desire to delete these DB Instances in addition to the original source DB Instance, you must explicitly do so using the DeleteDBInstance API or AWS Management Console.\n\nA read replica is billed as a standard DB Instance and at the same rates. Just like a standard DB instance, the rate per “DB Instance hour” for a read replica is determined by the DB instance class of the read replica – please see\npricing page\nfor up-to-date pricing. You are not charged for the data transfer incurred in replicating data between your source DB instance and read replica within the same AWS Region.\nBilling for a read replica begins as soon as the replica has been successfully created (i.e. when the status is listed as “active”). The read replica will continue being billed at standard Amazon RDS DB instance hour rates until you issue a command to delete it.\n\nCloudWatch Database Insights\nis a monitoring and metrics solution that simplifies and enhances database troubleshooting. It automates telemetry collection, including metrics, logs, and traces, removing the need for manual setup and configuration. By consolidating this telemetry into\nAmazon CloudWatch\n, CloudWatch Database Insights provides a unified view of database performance and health.\n\n", "token_count": 483, "metadata": {"service": "RDS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "rds-faq-45", "chunk_start": 17487, "chunk_end": 17970, "heading_hierarchy": ["Amazon RDS FAQs", "Read Replicas", "How do I delete a read replica? Will it be deleted automatically if its source DB Instance is deleted?"]}}
{"content": "Yes, reserved node discounted rates will apply to your existing RIs when switching from ElastiCache for Redis to ElastiCache Valkey. Scenario 5 in this\nblog\nprovides additional details.\n\nWith Redis OSS reserved nodes, in addition to being able to apply your benefits within the cache node family and engine, you can also choose to upgrade to the Valkey engine and receive more incremental value. Valkey is priced at a 20% discount relative to Redis OSS, and with reserved node flexibility, you can apply your Redis OSS reserved nodes to 20% more Valkey reserved nodes. For more information about reserved node flexibility, see this\nblog\n.\n\nThere are several performance benefits.\nElastiCache provides enhanced I/O threads that deliver significant improvements to throughput and latency at scale through multiplexing, presentation layer offloading, and more. Enhanced I/O threads improve performance by using more cores for processing I/O and dynamically adjusting to the workload. ElastiCache improves the throughput of TLS-enabled clusters by offloading encryption to the same enhanced I/O threads. This enables ElastiCache for Valkey to deliver up to 100% more throughput and 50% lower P99 latency than ElastiCache version 7.0 for Redis OSS. You can achieve over 1 million requests per second per node, or 500 million request per second per cluster, on r7g.4xlarge nodes or larger.\nIn addition, ElastiCache version 8.1 for Valkey provides you a new hash table that results in up to 40% lower memory usage for node-based clusters with Cluster Mode compared to ElastiCache version 7.2 for Valkey and version 7.1 for Redis OSS. The Serverless configuration has enhanced performance, scaling to 5 million requests per second per cache in minutes, up to 5x faster than Valkey 7.2, with microsecond read latency.\n\n", "token_count": 450, "metadata": {"service": "ELASTICACHE", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "elasticache-faq-9", "chunk_start": 4183, "chunk_end": 4633, "heading_hierarchy": ["Amazon ElastiCache FAQs", "Valkey engine", "When you upgrade from ElastiCache for Redis OSS to ElastiCache for Valkey, do you retain your existing discounted reserved node rates across all node sizes within the same family?"]}}
{"content": "D2 and H1 instances provide notifications for hardware failures. Like all instance storage, Dense HDD-storage volumes persist only for the life of the instance. Hence, we recommend that you build a degree of redundancy (e.g. RAID 1/5/6) or use file systems (e.g. HDFS and MapR-FS) that support redundancy and fault tolerance. You can also back up data periodically to more data storage solutions such as Amazon EBS or Amazon S3.\nQ: How do dense HDD-storage instances differ from Amazon EBS?\nAmazon EBS offers simple, elastic, reliable (replicated), and persistent block level storage for Amazon EC2 while abstracting the details of the underlying storage media in use. Amazon EC2 instance instances with local HDD or NVMe storage provide directly attached, high performance storage building blocks that can be used for a variety of storage applications. Dense-storage instances are specifically targeted at customers who want high sequential read/write access to large data sets on local storage, e.g. for Hadoop distributed computing and massively parallel processing data warehousing.\nQ: Can I launch dense HDD-storage instances as Amazon EBS optimized instances?\nEach HDD-storage instance type (H1, D2, D3, and D3en) is EBS optimized by default. Since this feature is always enabled, launching one of these instances explicitly as EBS optimized will not affect the instance's behavior. For more information, see\nAmazon EBS–optimized instances\n.\nQ: Can I launch D2 instances as Amazon EBS optimized instances?\nEach D2 instance type is EBS optimized by default. D2 instances 500 Mbps to 4,000 Mbps to EBS above and beyond the general-purpose network throughput provided to the instance. Since this feature is always enabled on D2 instances, launching a D2 instance explicitly as EBS optimized will not affect the instance's behavior.\nQ: What is a High I/O instance?\n", "token_count": 458, "metadata": {"service": "EC2", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "ec2-faq-93", "chunk_start": 40703, "chunk_end": 41161, "heading_hierarchy": ["Amazon EC2 FAQs", "Instance Types", "Accelerated Computing instances"]}}
{"content": "The CloudTrail Processing Library is a Java library that makes it easier to build an application that reads and processes CloudTrail log files. You can download the CloudTrail Processing Library from\nGitHub\n.\n\nCloudTrail Processing Library provides functionality to handle tasks such as continually polling an SQS queue and reading and parsing Amazon Simple Queue Service (Amazon SQS) messages It can also download log files stored in S3, and parse and serialize log file events in a fault-tolerant manner. For more information, go to the\nuser guide\nin the CloudTrail documentation.\n\nYou need aws-java-sdk version 1.9.3 and Java 1.7 or higher.\n\nCloudTrail helps you view, search, and download the last 90 days of your account’s management events for free. You can deliver one copy of your ongoing management events to S3 for free by creating a trail. Once a CloudTrail trail is set up, S3 charges apply based on your usage.\nYou can deliver additional copies of events, including data events and network activity events, using trails. You will be charged for data events, network activity events, and additional copies of management events. Learn more on the\npricing page\n.\n\nNo. The first copy of management events is delivered free of charge in each Region.\n\nYes. You will be charged for only the data events. The first copy of management events is delivered free of charge.\n\nWhen you use CloudTrail Lake, you pay for ingestion and storage together, where the billing is based on the amount of uncompressed data ingested and the amount of compressed data stored. When you create an event data store, you choose the pricing option you want to use for the event data store. The pricing option determines the cost for ingesting events and the maximum and default retention period for the event data store. Querying charges are based on the compressed data you choose to analyze. Learn more on the\npricing page\n.\n\nYes. Each CloudTrail event, on average, is around 1500 bytes. Using this mapping, you will be able to estimate the CloudTrail Lake ingestion based on past month’s CloudTrail usage in trails by number of events.\n\n", "token_count": 497, "metadata": {"service": "CLOUDTRAIL", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "cloudtrail-faq-17", "chunk_start": 7614, "chunk_end": 8111, "heading_hierarchy": ["AWS CloudTrail FAQs", "CloudTrail Processing Library", "What is the CloudTrail Processing Library?"]}}
{"content": "With AWS Data Exchange you can publish data simultaneously to all your customers and spend more time growing your business rather than managing logistics.\nAWS Data Exchange allows you to publish data to your customers through an easy-to-use API and console. The data model for AWS Data Exchange simplifies managing and publishing data through three reusable constructs: data sets, revisions, and assets. AWS customers can subscribe to these published data products directly from AWS Marketplace. The service automatically grants entitlements to customers who have an active subscription to the product, which means that you don’t have to manually configure and maintain custom permissions to an Amazon Simple Storage Service (S3) bucket for each subscription.\nAWS Data Exchange automatically sends an Amazon CloudWatch Events event to all subscribers when new revisions are published, which allows subscribers to automate their consumption of new data. When a subscription is no-longer active, AWS Data Exchange revokes that subscriber's entitlement to your data.\nIf your product is already available on AWS Data Exchange, you can simply use Bring Your Own Subscription (BYOS) to configure an entitlement to your existing subscribers for no additional cost and without any additional programming work. For more information about using BYOS to migrate and fulfill existing subscriptions with AWS customers, see\nCreate Bring Your Own Subscription offers\n.\n\nAs a provider, you first need to set up an AWS account and\nregister as an AWS Marketplace seller\n. You can then publish an API product by following the steps detailed in the\nPublishing a product containing APIs\ntopic in the\nAWS Data Exchange User Guide\n.\n\nAWS Data Exchange for APIs does not require providers to offer an uptime or availability SLA. Providers and subscribers can negotiate custom terms as part of a DSA. See\nPublishing Products\nfor further information.\n\n", "token_count": 412, "metadata": {"service": "DATA", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "data-faq-12", "chunk_start": 5395, "chunk_end": 5807, "heading_hierarchy": ["FAQs", "Provider", "I currently publish data directly into my subscribers' S3 buckets. Why should I consider using AWS Data Exchange to publish data to third parties instead?"]}}
{"content": "Amazon S3 is secure by default. Upon creation, only you have access to Amazon S3 buckets that you create, and you have complete control over who has access to your data. Amazon S3 supports user authentication to control access to data. You can use access control mechanisms, such as bucket policies, to selectively grant permissions to users and groups of users. The Amazon S3 console highlights your publicly accessible buckets, indicates the source of public accessibility, and also warns you if changes to your bucket policies or bucket ACLs would make your bucket publicly accessible. You should enable\nAmazon S3 Block Public Access\nfor all accounts and buckets that you do not want publicly accessible. All new buckets have Block Public Access turned on by default.\nYou can securely upload/download your data to Amazon S3 via SSL endpoints using the HTTPS protocol.\nAmazon S3 automatically encrypts all object uploads to your bucket (as of January 5, 2023)\n. Alternatively, you can use your own encryption libraries to encrypt data before storing it in Amazon S3.\nFor more information on security in AWS, refer to the\nAWS security page\n, and for S3 security information, visit the\nS3 security page\nand the\nS3 security best practices guide\n.\n\nCustomers can use a number of mechanisms for controlling access to Amazon S3 resources, including AWS Identity and Access Management (IAM) policies, bucket policies, access point policies, access control lists (ACLs), Query String Authentication, Amazon Virtual Private Cloud (Amazon VPC) endpoint policies, service control policies (SCPs) in AWS Organizations, and Amazon S3 Block Public Access.\n\nYes, customers can optionally configure an Amazon S3 bucket to create access log records for all requests made against it. Alternatively, customers who need to capture IAM/user identity information in their logs can configure\nAWS CloudTrail Data Events\n. These access log records can be used for audit purposes and contain details about the request, such as the request type, the resources specified in the request, and the time and date the request was processed.\n\n", "token_count": 464, "metadata": {"service": "S3", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "s3-faq-22", "chunk_start": 8924, "chunk_end": 9388, "heading_hierarchy": ["Amazon S3 FAQs", "Security", "How secure is my data in Amazon S3?"]}}
{"content": "on the 32xlarge and metal sizes, enabling low-latency and high-scale inter-node communication. For optimal networking performance on these new instances, Elastic Network Adapter (ENA) driver update may be required. For more information about an optimal ENA driver for R6i, see\n\"What do I need to do before migrating my EC2 instance to a sixth-generation instance?\"\non Knowledge Center.\nQ: What are Amazon EC2 R5b instances?\nR5b instances are EBS-optimized variants of memory-optimized R5 instances that deliver up to 3x better EBS performance compared to same sized R5 instances. R5b instances deliver up to 60 Gbps bandwidth and 260K IOPS of EBS performance, the fastest block storage performance on EC2. They are built on the AWS Nitro System, which is a combination of dedicated hardware and Nitro hypervisor.\nQ: What are some of the ideal use cases for R5b instances?\nR5b instances are ideal for large relational database workloads, including Microsoft SQL Server, SAP HANA, IBM DB2, and Oracle that run performance intensive applications such as commerce platforms, ERP systems, and health record systems. Customers looking to migrate large on-premises workloads with large storage performance requirements to AWS will find R5b instances to be a good fit.\nQ: What are the various storage options available on R5b instances?\nR5b instances are EBS-optimized by default and offer up to 60,000 Mbps of dedicated EBS bandwidth and 260K IOPS for both encrypted and unencrypted EBS volumes. R5b instances only support Non-Volatile Memory Express (NVMe) interface to access EBS storage volumes. R5b is supported by all volume types, with the exception of io2 volumes.\nQ: When should I use R5b instances?\n", "token_count": 438, "metadata": {"service": "EC2", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "ec2-faq-86", "chunk_start": 37734, "chunk_end": 38172, "heading_hierarchy": ["Amazon EC2 FAQs", "Instance Types", "Accelerated Computing instances"]}}
{"content": "Bedrock Data Automation supports MOV and MKV with H.264, VP8, VP9, a max video duration of 4 hours, and a max file size of 2 GB per API request. By default, BDA supports a max concurrency of 20 videos at 10 transactions per second (TPS) per customer.\nAudio\nBedrock Data Automation supports both standard output for audio.\nStandard output will provide summarization including chapter summarization, full transcription, and detect explicit content moderation for audio files.\nBedrock Data Automation supports FLAC, M4A, MP3, MP4, Ogg, WebM, WAV, a max audio duration of 4 hours, and a max file size of 2 GB per API request.\n\n", "token_count": 174, "metadata": {"service": "BEDROCK", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "bedrock-faq-22", "chunk_start": 8614, "chunk_end": 8788, "heading_hierarchy": ["Amazon Bedrock FAQs", "Data Automation", "What features and file formats are supported per modality by Amazon Bedrock Data Automation"]}}
{"content": "Q: What is Optimize CPUs?\nOptimize CPUs gives you greater control of your EC2 instances on two fronts. First, you can specify a custom number of vCPUs when launching new instances to save on vCPU-based licensing costs. Second, you can disable Intel Hyper-Threading Technology (Intel HT Technology) for workloads that perform well with single-threaded CPUs, such as certain HPC applications.\nQ: Why should I use Optimize CPUs feature?\nYou should use Optimize CPUs if:\nYou are running EC2 workloads that are not compute bound and are incurring vCPU-based licensing costs. By launching instances with a custom number of vCPUs you may be able to optimize your licensing spend.\nYou are running workloads that will benefit from disabling hyper-threading on EC2 instances.\nQ: How will the CPU optimized instances be priced?\nCPU optimized instances will be priced the same as equivalent full-sized instances.\nQ: How will my application performance change when using Optimize CPUs on EC2?\nYour application performance change with Optimize CPUs will be largely dependent on the workloads you are running on EC2. We encourage you to benchmark your application performance with Optimize CPUs to arrive at the right number of vCPUs and optimal hyper-threading behavior for your application.\nQ: Can I use Optimize CPUs on EC2 Bare Metal instance types (such as i3.metal)?\nNo. You can use Optimize CPUs with only virtualized EC2 instances.\nQ: How can I get started with using Optimize CPUs for EC2 Instances?\nFor more information on how to get started with Optimize CPUs and supported instance types, please visit the Optimize CPUs documentation page\nhere\n.\n\n", "token_count": 399, "metadata": {"service": "EC2", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "ec2-faq-175", "chunk_start": 73575, "chunk_end": 73974, "heading_hierarchy": ["Amazon EC2 FAQs", "Platform", "Optimize CPUs"]}}
{"content": "ACM makes it easier to enable SSL/TLS for a website or application on AWS, hybrid and multicloud environments. ACM eliminates many of the manual processes previously associated with using and managing SSL/TLS certificates. ACM can also help you avoid downtime due to misconfigured, revoked, or expired certificates by managing renewals. You get SSL/TLS protection and easy certificate management. Enabling SSL/TLS for Internet-facing sites can help improve the search rankings for your site and help you meet regulatory compliance requirements for encrypting data in transit.\nWhen you use ACM to manage certificates, certificate private keys are securely protected and stored using strong encryption and key management best practices. ACM lets you use the AWS Management Console, AWS CLI, or ACM APIs to centrally manage all of the SSL/TLS ACM certificates in an AWS Region. ACM is integrated with other AWS services, so you can request an SSL/TLS certificate and provision it with your Elastic Load Balancing load balancer or Amazon CloudFront distribution from the AWS Management Console, through AWS CLI commands, or with API calls.\n\n", "token_count": 244, "metadata": {"service": "CERTIFICATE", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "certificate-faq-2", "chunk_start": 970, "chunk_end": 1214, "heading_hierarchy": ["AWS Certificate Manager FAQs", "General", "What are the benefits of using AWS Certificate Manager (ACM)?"]}}
{"content": "When you exchange one Convertible RI for another, EC2 ensures that the total value of the Convertible RIs is maintained through a conversion. So, if you are converting your RI with a total value of $1000 for another RI, you will receive a quantity of Convertible RIs with a value that’s equal to or greater than $1000. You cannot convert your Convertible RI for Convertible RI(s) of a lesser total value.\nQ: Can you define total value?\nThe total value is the sum of all expected payments that you’d make during the term for the RI.\nQ: Can you walk me through how the true-up cost is calculated for a conversion between two All Upfront Convertible RIs?\nSure, let’s say you purchased an All Upfront Convertible RI for $1000 upfront, and halfway through the term you decide to change the attributes of the RI. Since you’re halfway through the RI term, you have $500 left of prorated value remaining on the RI. The All Upfront Convertible RI that you want to convert into costs $1,200 upfront today. Since you only have half of the term left on your existing Convertible RI, there is $600 of value remaining on the desired new Convertible RI. The true-up charge that you’ll pay will be the difference in upfront value between original and desired Convertible RIs, or $100 ($600 - $500).\nQ: Can you walk me through a conversion between No Upfront Convertible RIs?\nUnlike conversions between Convertible RIs with an upfront value, since you’re converting between RIs without an upfront cost, there will not be a true-up charge. However, the amount you pay on an hourly basis before the exchange will need to be greater than or equal to the amount you pay on a total hourly basis after the exchange.\n", "token_count": 437, "metadata": {"service": "EC2", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "ec2-faq-132", "chunk_start": 55489, "chunk_end": 55926, "heading_hierarchy": ["Amazon EC2 FAQs", "Billing and purchase options", "Convertible Reserved Instances"]}}
{"content": "Amazon SNS makes it easy for users with and without AWS IDs to receive notifications. The owner of the topic can grant/restrict access to subscribers by setting appropriate permissions for the topic using Access Control policies. Users can receive notifications from Amazon SNS in two ways:\nUsers with AWS IDs: Subscribers with valid AWS IDs (please refer to this link for details on obtaining AWS IDs) can subscribe to any topic directly – as long as the topic owner has granted them permissions to do so. The AWS IDs will be validated as part of the subscription registration.\nOther users: Topic owners can subscribe and register end-points on behalf of users without AWS IDs.\nIn both cases, the owner of the subscription endpoint needs to explicitly opt-in and confirm the subscription by replying to confirmation message sent by Amazon SNS.\n\nAll API calls made to Amazon SNS will validate authenticity by requiring that requests be signed with the secret key of the AWS ID account and verifying the signature included in the requests.\n\n", "token_count": 224, "metadata": {"service": "SNS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "sns-faq-12", "chunk_start": 5176, "chunk_end": 5400, "heading_hierarchy": ["Amazon SNS FAQs", "Security", "How does a topic owner give access to subscribers? Do subscribers have to have valid AWS IDs?"]}}
{"content": "connection to your VPC.\nQ: How many Amazon EC2 instances can connect to a file system?\nAmazon EFS supports one to thousands of Amazon EC2 instances connecting to a file system concurrently.\nQ: Where can I learn more about EFS?\nYou can visit the\nAmazon EFS FAQ page\n.\n\n", "token_count": 74, "metadata": {"service": "EC2", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "ec2-faq-101", "chunk_start": 44091, "chunk_end": 44165, "heading_hierarchy": ["Amazon EC2 FAQs", "Storage", "Amazon Elastic File System (Amazon EFS)"]}}
{"content": "With MemoryDB version 7.0 for Redis OSS, we introduced enhanced IO multiplexing, which delivers additional improvements to throughput and latency at scale. MemoryDB version 7.2 for Valkey supports enhanced IO multiplexing as well. Enhanced IO multiplexing is ideal for throughput-bound workloads with multiple client connections, and its benefits scale with the level of workload concurrency. As an example, when using r6g.4xlarge node and running 5200 concurrent clients, you can achieve up to 46% increased throughput (read and write operations per second) and up to 21% decreased P99 latency, compared with MemoryDB version 6 for Redis OSS. For these types of workloads, a node's network IO processing can become a limiting factor in the ability to scale. With enhanced IO multiplexing, each dedicated network IO thread pipelines commands from multiple clients into the MemoryDB engine, taking advantage of the engine's ability to efficiency process commands in batches.\nFor more information see the\ndocumentation\n.\n\nTo write data to and read data from your MemoryDB cluster, you connect to your cluster using one of the supported Valkey or Redis OSS clients. For a list of supported Valkey or Redis OSS clients, please see the Valkey or Redis OSS documentation. For instructions on how to connect to your MemoryDB cluster using a Valkey or Redis OSS client, see\nthe MemoryDB documentation\n. Valkey will work with existing Redis OSS clients so you don't need to change clients when you move from Redis OSS to Valkey.\n\nYou create a MemoryDB cluster with up to 500 nodes. This gives a maximum memory storage capacity of ~100 TB, assuming you have 250 primary nodes each with one replica for high availability (500 nodes total).\n\n", "token_count": 423, "metadata": {"service": "MEMORYDB", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "memorydb-faq-6", "chunk_start": 2657, "chunk_end": 3080, "heading_hierarchy": ["Amazon MemoryDB FAQs", "Performance and durability", "How does MemoryDB performance compare to Valkey and Redis OSS?"]}}
{"content": "AWS Direct Connect Gateway private ASN will be used as the AWS side ASN for the Border Gateway Protocol (BGP) session between your network and AWS.\n\nYou can select your own private ASN in the AWS Direct Connect gateway console. Once the AWS Direct Connect gateway is configured with an AWS side ASN, the private virtual interfaces associated with the AWS Direct Connect gateway use your configured ASN as the AWS side ASN.\n\nYou will not have to make any changes.\n\nWe support 32-bit ASNs from 4200000000 to 4294967294.\n\nNo, you cannot modify the AWS side ASN after creation. You can delete the AWS Direct Connect gateway and recreate a new AWS Direct Connect gateway with the desired private ASN.\n\nMACsec is not intended as a replacement for any specific encryption technology. For simplicity, and for defense in depth, you should continue to use any encryption technologies that you already use. We offer MACsec as an encryption option you can integrate into your network in addition to other encryption technologies you currently use.\n\nMACsec is supported on 10 Gbps, 100 Gbps, and 400 Gbps dedicated AWS Direct Connect connections at selected\npoints of presence\n. For MACsec to work, your dedicated connection must be transparent to Layer 2 traffic and the device terminating the Layer 2 adjacency must support MACsec. If you are using a last-mile connectivity partner, check that your last-mile connection can support MACsec. MACsec is not supported on 1 Gbps dedicated connections or any hosted connections.\n\nYes. You will need a MACsec-capable device on your end of the Ethernet connection to an AWS Direct Connect location. Refer to the\nMAC Security\nsection of our user guide to verify supported operation modes and required MACsec features.\n\n", "token_count": 421, "metadata": {"service": "DIRECTCONNECT", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "directconnect-faq-22", "chunk_start": 10487, "chunk_end": 10908, "heading_hierarchy": ["AWS Direct Connect FAQs", "AWS Direct Connect Gateway - Private ASN", "I'm attaching multiple Virtual Private Gateways with their own private ASN to a single AWS Direct Connect gateway configured with its own private ASN. Which private ASN takes precedence, VGW or AWS Direct Connect Gateway?"]}}
{"content": "S3 Multi-Region Access Points accelerate and simplify storage for your multi-region applications. By dynamically routing S3 requests made to a replicated data set, S3 Multi-Region Access Points reduce request latency, so that applications run up to 60% faster. S3 Multi-Region Access Points can also help you build resilient, multi-region and multi-account applications that are more protected against accidental or unauthorized data deletion. With S3 Multi-Region Access Points, you are able to take advantage of the global infrastructure of AWS while maintaining a simple region-agnostic architecture for your applications.\n\n", "token_count": 133, "metadata": {"service": "S3", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "s3-faq-111", "chunk_start": 43133, "chunk_end": 43266, "heading_hierarchy": ["Amazon S3 FAQs", "Replication", "Why should I use S3 Multi-Region Access Points?"]}}
{"content": "Security, Identity, and Compliance\n›\nAWS Audit Manager\n›\nFAQs\n\nGeneral\n13\nCore Concepts\n7\nGetting started\n3\nWorking in AWS Audit Manager\n9\n\nWorking in AWS Audit Manager\n\nAWS Audit Manager helps you continuously audit your AWS usage to simplify how you assess risk and compliance with regulations and industry standards. Audit Manager automates evidence collection to make it easier to assess if your policies, procedures, and activities, also known as controls, are operating effectively. When it is time for an audit, AWS Audit Manager helps you manage stakeholder reviews of your controls and enables you to build audit-ready reports with much less manual effort.\n\nEasily map your AWS usage to controls\n- AWS Audit Manager provides prebuilt frameworks that include mappings of AWS resources to control requirements for well-known industry standards and regulations, such as the Health Insurance Portability and Accountability Act (HIPAA), the General Data Protection Regulation (GDPR), and the Payment Card Industry Data Security Standard (PCI DSS).\nSave time with automated collection of evidence\n- AWS Audit Manager saves you time by automatically collecting and organizing evidence as defined by each control requirement.\nStreamline collaboration across teams\n- AWS Audit Manager helps you streamline audit stakeholder collaboration. For example, the delegation feature enables you to assign controls in your assessment to a subject matter expert to review.\nBe continually prepared to produce audit-ready reports\n- The evidence Audit Manager continuously collects and securely stores becomes a record containing the information needed to demonstrate compliance with the requirements specified by a control.\nEnsure assessment report and evidence integrity\n- AWS Audit Manager stores evidence in its own managed storage repository with read-only permissions to your end-users. When you generate audit-ready reports, Audit Manager produces a report file checksum so you can validate that the report evidence remains unaltered.\n\nAWS Audit Manager’s prebuilt frameworks help map your AWS resource usage to the requirements in industry standards or regulations, such as CIS AWS Foundations Benchmark, the General Data Protection Regulation (GDPR), and the Payment Card Industry Data Security Standard (PCI DSS). You can also make an editable copy of a prebuilt framework and its controls to help meet your unique business requirements.\n\n", "token_count": 505, "metadata": {"service": "AUDIT", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "audit-faq-0", "chunk_start": 0, "chunk_end": 505, "heading_hierarchy": []}}
{"content": ": Slates are sections, typically at the beginning of a video, that contain text metadata about the episode, studio, video format, audio channels, and more. Amazon Rekognition can identify the start and end such slates, making it easy for operators to use the text metadata or to simply remove the slate when preparing content for final viewing.\nStudio logos\n: Studio logos are sequences that show the logos or emblems of the production studio involved in making the show. Amazon Rekognition can identify such sequences, making it easy for operators to review them for identifying studios.\nContent\n: Content refers to the portions of the TV show or movie that contain the program or related elements. Black frames, credits, color bars, slates, and studio logos are not considered to be content. Amazon Rekognition Video enables you to detect the start and end of each content segment in the video, which enables multiple uses such as finding the program run time or finding certain segments that serve specific purposes. For example, a quick recap of the previous episode at the beginning of the video is a type of content. Similarly, bonus post-credit content can appear after the credits have finished. And, some videos may have ‘textless’ content at the end of the video, which are a set of all program content that contains overlaid text, but with that text removed to enable internationalization in another language. Once all the content segments are detected with Amazon Rekognition Video, you can apply specific domain knowledge such as ‘my videos always start with a recap’ to further categorize each segment or to send them for human review.\n\n", "token_count": 347, "metadata": {"service": "REKOGNITION", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "rekognition-faq-23", "chunk_start": 9442, "chunk_end": 9789, "heading_hierarchy": ["Amazon Rekognition FAQs", "Media Analysis using Amazon Rekognition Video", "Q: What types of media analysis segments can Amazon Rekognition Video detect?"]}}
{"content": "Amazon EventBridge Scheduler is a serverless task scheduler that simplifies creating, executing, and managing millions of schedules across AWS services without provisioning or managing underlying infrastructure.\n\nLog in to your AWS account, navigate to the EventBridge console and select the\nCreate Schedule\nbutton. Follow the step-by-step workflow and fill in required fields. Select a scheduling format including a time window for the task to implement, fixed rate, cron, or a specific date and time. Select your target from a list of AWS services and configure retry policies for maximum control of your schedule implementation. Review your schedule and select\nCreate\n.\n\nEventBridge Scheduler builds upon the scheduling functionality offered within Scheduled Rules. EventBridge Scheduler includes support for time zones, increased scale, customized target payloads, added time expressions, and a dashboard for monitoring schedules. Schedules can be created independently without the need to create an event bus with a scheduled rule.\n\nScheduled rules will continue to be available, however EventBridge Scheduler offers a richer feature set providing more flexibility when creating, executing, and managing your schedules. You can also get started for free, see\npricing page\nfor more details.\n\nEventBridge Scheduler has deep integrations with AWS services and can create schedules for any service with an AWS API action. Configurations for time patterns and retries are uniform across AWS for a consistent scheduling experience. Monitoring schedules is easier through the EventBridge Scheduler console delivering a view of your schedules in a dashboard or with a “ListSchedule” API request. You will be able to see critical information on your schedules such as start time, last run, and the assigned AWS target. For more granular details, you can review execution logs available in CloudWatch Logs or they can be sent to S3 or Kinesis Firehose.\n\nYou can update your schedules in the EventBridge Scheduler console by selecting the schedule to modify. A new panel will display your options.\n\nYes, with EventBridge Scheduler you can select what time zone a schedule will operate. These schedules will automatically adjust to Daylight Savings Time (DST) and back to standard time.\n\n", "token_count": 481, "metadata": {"service": "EVENTBRIDGE", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "eventbridge-faq-8", "chunk_start": 3639, "chunk_end": 4120, "heading_hierarchy": ["Amazon EventBridge FAQs", "Scheduler", "What is Amazon EventBridge Scheduler?"]}}
{"content": "Yes. Automatic key rotation is supported by the XKS specification, and it is a capability provided by most vendors that support XKS. Automatic rotation for XKS keys occurs entirely in the external key manager and works in a similar way to the automatic key rotation for AWS KMS keys created and managed within KMS. When you use a rotated KMS XKS key to encrypt data, your external key manager uses the current key material. When you use the rotated XKS key to decrypt ciphertext, your external key manager uses the version of the key material that was used to encrypt it. As long as previous XKS keys used to create earlier ciphertexts are still enabled in external key manager, you will be able to successfully make Decrypt API request under those versions of you XKS keys.\n\nFor services that do not cache keys, the next API call using this XKS KMS key will fail. Some services implement data key caching or other key derivation schemes for performance, latency, or KMS cost management. Caching of these keys can vary from 5 mins to 24 hrs. Any protected resource that is currently in use (such as RDS database or EC2 instance) will respond differently after you deny access to the key. See the relevant AWS service documentation for details.\n\nTo authenticate to your external key store proxy, AWS KMS signs all requests to the proxy using AWS SigV4 credentials that you configure on your proxy and provide to KMS. AWS KMS authenticates your external key store proxy using server-side TLS certificates. Optionally, your proxy can enable mutual TLS for additional assurance that it only accepts requests from AWS KMS.\n\n", "token_count": 362, "metadata": {"service": "KMS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "kms-faq-20", "chunk_start": 8837, "chunk_end": 9199, "heading_hierarchy": ["AWS Key Management Service FAQs", "External key store", "Is automatic key rotation possible with XKS keys?"]}}
{"content": "CloudFront Virtual Private Cloud (VPC) origins is a new feature that allows you to use CloudFront to deliver content from applications hosted in a VPC private subnet. With VPC origins, you can have your applications in a private subnet in your VPC, that is accessible only through your CloudFront distributions. This removes the requirement for the origin to have an externally resolvable Domain Name Service (DNS) name. You can set up VPC origins with applications running on Application Load Balancer (ALB), Network Load Balancer (NLB), and EC2 instances. VPC origins are available in AWS Commercial Regions only, and the full list of supported AWS Regions are available\nhere\n.\n\nYou should use VPC origins with CloudFront if you want to enhance your web applications' security while maintaining high performance and global scalability. With VPC origins, you can restrict access to your origins in a VPC only to your CloudFront distributions without having complex configurations like secret headers, or Access Control Lists. VPC origins also allows you to optimize your IPv4 costs by allowing you to route to origins in a private subnet with internal IPv4 IP addresses which free of cost. VPC origins is perfect if you want to streamline your security management, allowing you to focus more on growing your core business rather than managing intricate security measures.\n\n", "token_count": 289, "metadata": {"service": "CLOUDFRONT", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "cloudfront-faq-20", "chunk_start": 8431, "chunk_end": 8720, "heading_hierarchy": ["Amazon CloudFront FAQs", "VPC origins", "What is VPC origins?"]}}
{"content": "We will take the following two actions ahead of the August 15, 2022 retirement date:\nWe will stop issuing 3-year reserved instances (RI) and 1-year RI for the EC2-Classic environment on Oct 30, 2021. RIs already in place on the EC2-Classic environment will not be affected at this time. RIs that are set to expire after 8/15/2022 will need to be modified to use the Amazon VPC environment for the remaining period of the lease. For information on how to modify your RIs, please visit our\ndocument\n.\nOn Aug 15, 2022, we will no longer allow the creation of new instances (Spot or on-demand) or other AWS services in the EC2-Classic environment. Any workloads or services in running state will gradually loose access to all AWS services on EC2-Classic as we retire them beginning August 16, 2022.\n\nNetwork interfaces can only be attached to instances residing in the same Availability Zone.\n\nNetwork interfaces can only be attached to instances in VPCs in the same account.\n\nYes, however, this is not a use case best suited for multiple interfaces. Instead, assign additional private IP addresses to the instance and then associate EIPs to the private IPs as needed.\n\nNo. You can attach and detach secondary interfaces (eth1-ethn) on an EC2 instance, but you can’t detach the eth0 interface.\n\nYes. Peering connections can be created with VPCs in different regions. Inter-region VPC peering is available globally in all commercial regions (excluding China).\n\nYes, assuming the owner of the other VPC accepts your peering connection request.\n\nThere is no charge for creating VPC peering connections, however, data transfer across peering connections is charged. See the Data Transfer section of the\nEC2 Pricing page\nfor data transfer rates.\n\nNo. VPC peering connections do not require an Internet Gateway.\n\nNo. Traffic between instances in peered VPCs remains private and isolated – similar to how traffic between two instances in the same VPC is private and isolated.\n\n", "token_count": 505, "metadata": {"service": "VPC", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "vpc-faq-24", "chunk_start": 10297, "chunk_end": 10802, "heading_hierarchy": ["Amazon VPC FAQs", "EC2 Classic", "What are the important dates I should be aware of?"]}}
{"content": "Hypervisor support a maximum of 27 additional PCI devices for EBS volumes and VPC ENIs. Each EBS volume or VPC ENI uses a PCI device. For example, if you attach 3 additional network interfaces to an instance that uses the Nitro Hypervisor, you can attach up to 24 EBS volumes to that instance.\nQ. Will the Nitro Hypervisor change the APIs used to interact with EC2 instances?\nNo, all the public facing APIs for interacting with EC2 instances that run using the\nNitro\nHypervisor will remain the same. For example, the “hypervisor” field of the DescribeInstances response will continue to report “xen” for all EC2 instances, even those running under the Nitro Hypervisor. This field may be removed in a future revision of the EC2 API.\nQ. Which AMIs are supported on instances that use the Nitro Hypervisor?\nEBS backed HVM AMIs with support for ENA networking and booting from NVMe storage can be used with instances that run under the\nNitro\nHypervisor. The latest Amazon Linux AMI and Windows AMIs provided by Amazon are supported, as are the latest AMI of Ubuntu, Debian, Red Hat Enterprise Linux, SUSE Enterprise Linux, CentOS, and FreeBSD.\nQ. Will I notice any difference between instances using Xen hypervisor and those using the Nitro Hypervisor?\nYes. For example, instances running under the\nNitro\nHypervisor boot from EBS volumes using an NVMe interface. Instances running under Xen boot from an emulated IDE hard drive, and switch to the Xen paravirtualized block device drivers.\nOperating systems can identify when they are running under a hypervisor. Some software assumes that EC2 instances will run under the Xen hypervisor and rely on this detection. Operating systems will detect they are running under KVM when an instance uses the Nitro Hypervisor, so the process to identify EC2 instances should be used to identify EC2 instances that run under both hypervisors.\n", "token_count": 466, "metadata": {"service": "EC2", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "ec2-faq-172", "chunk_start": 72377, "chunk_end": 72843, "heading_hierarchy": ["Amazon EC2 FAQs", "Platform", "Nitro Hypervisor"]}}
{"content": "You should run your jobs on Fargate when you want AWS Batch to handle provisioning of compute completely abstracted from ECS infrastructure. You should run your jobs on ECS if you need access to particular instance configurations (particular processors, GPUs, or architecture) or for very-large scale workloads. If you have chosen Kubernetes as your container orchestration technology, you can standardize your batch workloads using Batch integration with EKS.\nDepending on your use case, currently Fargate jobs will start faster in the case of initial scale-out of work, as there is no need to wait for EC2 instance or pods to launch. However, for larger workloads EKS or ECS may be faster as Batch reuses instances and container images to run subsequent jobs.\n\nYou should run your jobs on Fargate when you want AWS Batch to handle provisioning of compute completely abstracted from EC2 infrastructure. You should run your jobs on EC2 if you need access to particular instance configurations (particular processors, GPUs, or architecture) or for very-large scale workloads.\nDepending on your use case, your jobs may start faster using either EC2 or Fargate. Fargate jobs will start faster in the case of initial scale-out of work, as there is no need to wait for EC2 instance to launch. However, for larger workloads EC2 instances may be faster as Batch reuses instances and container images to run subsequent jobs.\n\n", "token_count": 318, "metadata": {"service": "BATCH", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "batch-faq-1", "chunk_start": 383, "chunk_end": 701, "heading_hierarchy": ["AWS Batch FAQs", "General information", "When should I run my jobs in EKS vs. Fargate vs. ECS?"]}}
{"content": "Using EFS for Lambda is ideal for building machine learning applications or loading large reference files or models, processing or backing up large amounts of data, hosting web content, or developing internal build systems. Customers can also use EFS for Lambda to keep state between invocations within a stateful microservice architecture, in a Step Functions workflow, or sharing files between serverless applications and instance or container-based applications.\n\nYes. Data encryption in transit uses industry-standard Transport Layer Security (TLS) 1.2 to encrypt data sent between AWS Lambda functions and the Amazon EFS file systems.\n\nCustomers can provision Amazon EFS to encrypt data at rest. Data encrypted at rest is transparently encrypted while being written, and transparently decrypted while being read, so you don’t have to modify your applications. Encryption keys are managed by the AWS Key Management Service (KMS), eliminating the need to build and maintain a secure key management infrastructure.\n\nThere is no additional charge for using Amazon EFS for AWS Lambda. Customers pay the standard price for AWS Lambda and for Amazon EFS. When using Lambda and EFS in the same availability zone, customers are not charged for data transfer. However, if they use VPC peering for Cross-Account access, they will incur data transfer charges. To learn more, please see\nPricing\n.\n\nNo. Each Lambda function will be able to access one EFS file system.\n\nYes. Amazon EFS supports Lambda functions, ECS and Fargate containers, and EC2 instances. You can share the same file system and use IAM policy and Access Points to control what each function, container, or instance has access to.\n\nYes. Lambda functions can be configured with a function URL, a built-in HTTPS endpoint that can be invoked using the browser, curl, and any HTTP client. Function URLs are an easy way to get started building HTTPS accessible functions.\n\nYou can configure a function URL for your function through the AWS Management Console, the AWS Lambda API, the AWS CLI, AWS CloudFormation, and the AWS Serverless Application Model. Function URLs can be enabled on the $LATEST unqualified version of your function, or on any function alias. To learn more about configuring a function URL,\nsee the documentation\n.\n\n", "token_count": 508, "metadata": {"service": "LAMBDA", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "lambda-faq-24", "chunk_start": 10932, "chunk_end": 11440, "heading_hierarchy": ["AWS Lambda FAQs", "Amazon EFS for AWS Lambda", "Q: Who should use Amazon EFS for Lambda?"]}}
{"content": "Amazon EBS encryption offers seamless encryption of EBS data volumes, boot volumes and snapshots, eliminating the need to build and maintain a secure key management infrastructure. EBS encryption enables data at rest security by encrypting your data using Amazon-managed keys, or keys you create and manage using the\nAWS Key Management Service (KMS)\n. The encryption occurs on the servers that host EC2 instances, providing encryption of data as it moves between EC2 instances and EBS storage. For more details, see Amazon EBS encryption in the\nAmazon EC2 User Guide\n.\n\nAWS KMS\nis a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. AWS Key Management Service is integrated with other AWS services including Amazon EBS, Amazon S3, and Amazon Redshift, to make it simple to encrypt your data with encryption keys that you manage. AWS Key Management Service is also integrated with AWS CloudTrail to provide you with logs of all key usage to help meet your regulatory and compliance needs. To learn more about KMS, visit the AWS Key Management Service product page.\n\nYou can use Amazon EBS encryption to meet security and encryption compliance requirements for data at rest encryption in the cloud. Pairing encryption with existing IAM access control policies improves your company’s defense-in-depth strategy.\n\nAmazon EBS encryption handles key management for you. Each newly created volume gets a unique 256-bit AES key; Volumes created from the encrypted snapshots share the key. These keys are protected by our own key management infrastructure, which implements strong logical and physical security controls to prevent unauthorized access. Your data and associated keys are encrypted using the industry-standard AES-256 algorithm.\n\nYes, using\ncustomer master keys (CMKs)\nthat are either AWS-managed or customer-managed. You can specify the volume details and encryption through a\nRunInstances API\ncall with the\nBlockDeviceMapping\nparameter or through the Launch Wizard in the EC2 Console.\n\nYes, you can create encrypted data volume with either default or custom CMK encryption at the time of instances launch. You can specify the volume details and encryption through\nBlockDeviceMapping\nobject in\nRunInstances API\ncall or through Launch Wizard in EC2 Console.\n\n", "token_count": 503, "metadata": {"service": "EBS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "ebs-faq-14", "chunk_start": 5607, "chunk_end": 6110, "heading_hierarchy": ["Amazon EBS FAQs", "Encryption", "What is Amazon EBS encryption?"]}}
{"content": "Sharing of applications within an organizational unit isn’t supported. You can use policies to keep your app private, grant cross-account access, grant organization access or make it available publicly.\n\nTo provide access to your application for certain accounts in an organization, simply update the resource-based policy to include AWS accounts along with the AWS organization ID with whom you would like to share the application.\n\nYes. You can set actions on your resource-based policy which can restrict the type of operations someone can take on an application that you have shared. Updating actions for a resource-based policy is supported through the AWS Serverless Application Repository APIs and console. For more details on actions you can set for resource-based polices, see our documentation\nhere\n.\n\n", "token_count": 161, "metadata": {"service": "SERVERLESS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "serverless-faq-4", "chunk_start": 1941, "chunk_end": 2102, "heading_hierarchy": ["Serverless Application Repository FAQs and Terms", "Sharing applications", "Can I share an application with a particular organizational unit within my organization?"]}}
{"content": "Depending on the amount of data and the size of the cluster, upgrades can take anywhere from a few minutes to a few hours to complete.\n\nNo. With in-place version upgrade, all the data in your cluster is also restored as part of the upgrade process. If you only wish to upgrade the domain alone, you can take a snapshot of your data, delete all your indexes from the domain and then trigger an in-place version upgrade. Alternatively, you can create a separate domain with the newer version and then restore your data to that domain.\n\nNo. If you need to downgrade to an older version, contact AWS Support to restore the automatic, pre-upgrade snapshot on a new domain. If you took a manual snapshot of the original domain, you can perform this step yourself.\n\nMulti-AZ with Standby is a new deployment option for Amazon OpenSearch Service that enables high-availability and consistent performance for business-critical workloads. With Multi-AZ with Standby, OpenSearch Service managed clusters are resilient to infrastructure failures like node drops, or a single Availability Zone failure, assuring no impact to performance or availability, even in the event of a single Availability Zone failure. Multi-AZ with Standby provides the added benefit of simplifying cluster configuration and management by enforcing best practices and reducing complexity.\n\nTo enable Multi-AZ with Standby, managed clusters need to meet the following conditions:\nRun OpenSearch 1.3 or more recent version.\nDeploy in AWS Regions with 3-AZ. Currently, AWS North California region does not support 3-AZ and are therefore not suitable for Multi-AZ with Standby.\nNumber of data nodes needs to be in multiples of three.\nNumber of data copies (primary + replica) should be in multiples of three.\nFollow sizing guidelines for the leader (recommended size based on number of nodes,  number of shards, and number of mappings in your cluster).\n\n", "token_count": 434, "metadata": {"service": "OPENSEARCH", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "opensearch-faq-11", "chunk_start": 5038, "chunk_end": 5472, "heading_hierarchy": ["Amazon OpenSearch Service FAQs", "Administration", "How long does the in-place version upgrade take?"]}}
{"content": "Yes, all Amazon Neptune Database instances must be created in a VPC. With Amazon VPC, you can define a virtual network topology that closely resembles a traditional network that you might operate in your own datacenter. This gives you complete control over who can access your Neptune databases.\n\nCurrently, encrypting an existing unencrypted Neptune instance is not supported. To use Neptune encryption for an existing unencrypted database, create a new database instance with encryption enabled and migrate your data into it.\n\nAccess to Neptune databases must be done through the HTTPS port entered on database creation within your VPC. This is done to provide an additional layer of security for your data. Step-by-step instructions on how to connect to your Neptune database are provided in the\nAmazon Neptune User Guide\n.\n\nYou can use openCypher, an open-source project that makes it easy to use the Cypher language for graph processing, invoking the Neptune Analytics algorithms, and for vector similarity search.\n\nNeptune Analytics is well suited for graph queries that access large parts of a graph or whole graphs. Neptune Analytics is an in-memory engine, and it can load these large graphs into memory to deliver a response in seconds. In addition, Neptune Analytics can serve thousands of analytic queries per second using a library of popular graph analytics algorithms for operations such as ranking social influencers, detecting groups for fraud, or finding patterns in network activity. For generative AI applications, Neptune Analytics can store vector embeddings and provide vector similarity searches.\n\nYou can select an existing Neptune cluster as the data source, which will be automatically loaded into Neptune Analytics.\n\nNeptune Analytics supports 12 algorithms for path finding, detecting communities (clustering), identifying important data (centrality), and quantifying similarity. Path finding algorithms are used for use cases such as route planning for supply chain optimization, while centrality algorithms such as page rank identify the most influential sellers in a graph. Similarly, algorithms such as connected components, clustering, and centrality algorithms can be used for fraud-detection use cases to determine whether the connected network is a group of friends or a fraud ring formed by a set of coordinated fraudsters.\n\n", "token_count": 512, "metadata": {"service": "NEPTUNE", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "neptune-faq-9", "chunk_start": 4067, "chunk_end": 4579, "heading_hierarchy": ["Amazon Neptune FAQs", "Neptune Database", "Can I use Neptune Database in Amazon Virtual Private Cloud (Amazon VPC)?"]}}
{"content": "Yes. You can continue to use Volume Gateway’s existing snapshot capabilities to create Amazon EBS snapshots and use your previously created snapshots for restore purposes. AWS Backup’s backup schedule operates independently from the Volume Gateway scheduled snapshots, and provides you an additional way to centrally manage all your backup and retention policies.\n\nYes. AWS Backup will back up KMS-encrypted volumes on Volume Gateway with the same key as the one used for volume encryption.\n\nAWS Backup supports backup of Volume Gateway volumes within the same region in which AWS Backup operates.\n\nAWS Storage Gateway is available as a hardware appliance\n, which has Storage Gateway software pre-installed on a validated server configuration. You manage the appliance from the AWS Console.\n\nThe hardware appliance supports Amazon S3 File Gateway with NFS and SMB interfaces, Amazon FSx File Gateway with SMB, Volume Gateway cached volumes with iSCSI, and Tape Gateway with iSCSI-VTL.\n\nThe hardware appliance further simplifies deployment and management of AWS Storage Gateway on-premises for IT environments such as remote offices and departments that lack existing virtual server infrastructure, adequate disk and memory resources, or staff with hypervisor management skills. It avoids having to procure additional infrastructure necessary for a virtual environment in order to operate the local Storage Gateway VM appliance.\n\nThere are two models that offer 5 TB or 12 TB of local SSD cache.\n\nThe hardware appliance is based on validated server configurations. Please refer to the\nStorage Gateway Hardware Appliance\nproduct page for specifications.\n\nThe hardware appliance is supported in 16 AWS Regions including US East (Northern Virginia, Ohio), US West (Northern California, Oregon), Canada (Central), South America (São Paulo), Europe (Ireland, Frankfurt, London, Paris, Stockholm), and Asia Pacific (Mumbai, Seoul, Singapore, Sydney, Tokyo).\n\n", "token_count": 444, "metadata": {"service": "STORAGEGATEWAY", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "storagegateway-faq-26", "chunk_start": 11373, "chunk_end": 11817, "heading_hierarchy": ["AWS Storage Gateway FAQs", "Volume Gateway", "If I use AWS Backup, can I also continue to use Volume Gateway snapshot schedules and existing snapshots?"]}}
{"content": "Yes. One or more producers can send messages to a FIFO queue. Messages are stored in the order that they were successfully received by Amazon SQS.\nIf multiple producers send messages in parallel, without waiting for the success response from SendMessage or SendMessageBatch actions, the order between producers might not be preserved. The response of SendMessage or SendMessageBatch actions contains the final ordering sequence that FIFO queues use to place messages in the queue, so your multiple-parallel-producer code can determine the final order of messages in the queue.\n\nBy design, Amazon SQS FIFO queues don't serve messages from the same message group to more than one consumer at a time. However, if your FIFO queue has multiple message groups, you can take advantage of parallel consumers, allowing Amazon SQS to serve messages from different message groups to different consumers.\n\nBy default, FIFO queues support up to 3,000 messages per second with batching or up to 300 messages per second (300 send, receive, or delete operations per second) without batching. If you require higher throughput, you can enable high throughput mode for FIFO on the Amazon SQS console, which will support up to 70,000 messages per second without batching and even higher with batching. For a detailed breakdown of FIFO high throughput mode quotas per region, please see\nAWS Documentation\n.\n\nThe name of a FIFO queue must end with the .fifo suffix. The suffix counts towards the 80-character queue name limits. To determine whether a queue is FIFO, you can check whether the queue name ends with the suffix.\n\nFair queues are an extension to Amazon SQS standard queues that mitigate noisy neighbor impact in multi-tenant systems. They maintain consistent time between sending and receiving messages across all tenants. When one tenant sends a disproportionately large volume of messages or has messages that require longer processing time, fair queues ensure other tenants' messages maintain low dwell time. This preserves quality of service for all tenants while maintaining the scalability and throughput of standard queues.\n\n", "token_count": 477, "metadata": {"service": "SQS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "sqs-faq-10", "chunk_start": 4373, "chunk_end": 4850, "heading_hierarchy": ["Amazon SQS FAQs", "FIFO queues", "Do Amazon SQS FIFO queues support multiple producers?"]}}
{"content": ". Once you purchase an EC2 Capacity Block, a reservation is created in your account. When the EC2 Capacity Block start time arrives, EC2 will emit an event through Amazon EventBridge to indicate that the reservation is now active and available for use. To use an active EC2 Capacity Block, select the “Capacity Block” purchase option and target the capacity reservation ID for your EC2 Capacity Block while launching EC2 instances. As your EC2 Capacity Block end time approaches, EC2 will emit event through EventBridge letting you know your reservation is ending soon so you can checkpoint your workload. Around 30 minutes before your EC2 Capacity Block expires, AWS will begin terminating any running instances. The amount you are charged for your EC2 Capacity Block does not include the last 30 minutes of the reservation.\nQ: Which instance types do EC2 Capacity Blocks support, and which AWS Regions are they available in?\nEC2 Capacity Blocks are available for the following instance types in the following regions:\nEC2 p5e.48xlarge instances in US East (Ohio) region\nEC2 p5.48xlarge instances in US East (N. Virginia), US East (Ohio), US West (Oregon), and Asia Pacific (Tokyo) regions\nEC2 p4d.24xlarge instances in US East (Ohio) and US West (Oregon) regions\nEC2 trn1.32xlarge instances in Asia Pacific (Melbourne) region\nQ: What size options are available with EC2 Capacity Blocks?\nEC2 Capacity Blocks are available in cluster sizes of 1, 2, 4, 8, 16, 32, and 64 instances, and they can be reserved for up to 14 days in one-day increments, or up to 182 days in seven-day increments.\nQ: How far in advance can I reserve an EC2 Capacity Block?\n", "token_count": 441, "metadata": {"service": "EC2", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "ec2-faq-137", "chunk_start": 57589, "chunk_end": 58030, "heading_hierarchy": ["Amazon EC2 FAQs", "Billing and purchase options", "Amazon EC2 Capacity Blocks for ML"]}}
{"content": "New versions of Grafana introduce breaking changes, which may impact your visualizations or automation workflows. Manual control over Grafana workspace versioning lets you validate your Grafana experience against new versions of Grafana before upgrading production workspaces.\n\n", "token_count": 55, "metadata": {"service": "MANAGED", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "managed-faq-7", "chunk_start": 3188, "chunk_end": 3243, "heading_hierarchy": ["Amazon Managed Grafana FAQs", "Versions and Updates", "Why would I want manual control over Grafana version updates?"]}}
{"content": "Any content processed by Amazon Comprehend is encrypted and stored at rest in the AWS region where you are using Amazon Comprehend. Some portion of content processed by Amazon Comprehend may be stored in another AWS region solely in connection with the continuous improvement and development of your Amazon Comprehend customer experience and other Amazon machine-learning/artificial-intelligence technologies. This does not apply Amazon Comprehend Medical. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see the\nhttps://aws.amazon.com/compliance/data-privacy-faq/\nfor more information.\n\nYes, subject to your compliance with the Amazon Comprehend Service Terms, including your obligation to provide any required notices and obtain any required verifiable parental consent under COPPA, you may use Amazon Comprehend in connection with websites, programs, or other applications that are directed or targeted, in whole or in part, to children under age 13.\n\nFor information about the requirements of COPPA and guidance for determining whether your website, program, or other application is subject to COPPA, please refer directly to the resources provided and maintained by the\nUnited States Federal Trade Commission\n. This site also contains information regarding how to determine whether a service is directed or targeted, in whole or in part, to children under age 13.\n\n", "token_count": 338, "metadata": {"service": "COMPREHEND", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "comprehend-faq-3", "chunk_start": 1277, "chunk_end": 1615, "heading_hierarchy": ["Amazon Comprehend FAQs", "Data privacy", "Is the content processed by Amazon Comprehend moved outside the AWS region where I am using Amazon Comprehend?"]}}
{"content": "No, at this time you cannot use Amazon RDS Blue/Green Deployments to rollback changes.\n\nMySQL protects users from data loss by writing data in 16KiB pages in memory twice to durable storage—first to the “doublewrite buffer” and then to table storage\n. Amazon RDS Optimized Writes\nwrite your 16KiB data pages directly to your data files reliably and durably in one step using the\nTorn Write Prevention\nfeature of the AWS Nitro System.\n\nAmazon RDS Optimized Writes\nare available for\nMySQL major version 8.0.30\nand higher.\n\nAmazon RDS Optimized Writes\nare available in db.r6i and db.r5b instances. They are available in all Regions where these instances are available, excluding AWS China Regions.\n\nAll\nAmazon RDS for MySQL\nusers should implement\nAmazon RDS Optimized Writes\nfor up to 2x improved write transaction throughput. Applications with write-heavy workloads, such as digital payments, financial trading, and online gaming applications will find this feature especially helpful.\n\nNo.\nAmazon Aurora MySQL-Compatible Edition\nalready avoids the use of the “doublewrite buffer.” Instead, Amazon Aurora replicates data six ways across three Availability Zones (AZs) and uses a quorum-based approach to durably write data and correctly read it thereafter.\n\nAt this time, this initial release does not support enabling\nAmazon RDS Optimized Writes\nfor your existing database instances even if the instance class supports Optimized Writes.\n\nAmazon RDS Optimized Writes\nare available to RDS for MySQL customers at no additional cost.\n\nWorkloads that use temporary objects in MySQL and MariaDB for query processing benefit from\nAmazon RDS Optimized Reads\n. Optimized Reads place temporary objects on the database instance's NVMe-based instance storage, instead of the Amazon Elastic Block Store volume. This helps to speed up complex query processing by up to 2X.\n\n", "token_count": 474, "metadata": {"service": "RDS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "rds-faq-58", "chunk_start": 22750, "chunk_end": 23224, "heading_hierarchy": ["Amazon RDS FAQs", "Amazon RDS Blue/Green Deployments", "Can I use Amazon RDS Blue/Green Deployments to rollback changes?"]}}
{"content": "AWS East/West and GovCloud (US) Regions are FISMA compliant. When AWS Transfer Family is authorized for FedRAMP, it will be FISMA compliant within the respective regions. This compliance is demonstrated through FedRAMP Authorization of these two regions to FedRAMP Moderate and FedRAMP High. We demonstrate compliance through annual assessments and documenting compliance with in-scope NIST SP 800-53 controls within our System Security Plans. Templates are available on Artifact along with our customer responsibility matrix (CRM) which demonstrates at a detailed level or responsibility to meet these NIST controls as required by FedRAMP. Artifact is available through the management console accessible by an AWS account for both East/West and GovCloud. If you have any further questions on this topic, please consult the\nconsole\n.\n\nFiles uploaded through services are verified by comparing the file’s pre- and post-upload MD5 checksum.\n\nYou can use AWS Transfer Family managed workflows to automatically decrypt files using PGP keys when they are uploaded to your AWS Transfer Family SFTP, FTPS or FTP server endpoints. For more information, refer to managed workflows documentation. Alternatively, you can subscribe to AWS Transfer Family event notifications published in Amazon Eventbridge to orchestrate granular and event-driven processing of transferred files using your own encryption/ decryption logic.\n\nYou can monitor your end users’ and their file transfer activities using JSON formatted logs that are delivered to Amazon CloudWatch. Within CloudWatch, you can parse and query your logs using CloudWatch Log Insights, which automatically discovers JSON formatted fields. You can also track top users, total number of unique users, and their ongoing usage with CloudWatch Contributor Insights. We also provide pre-built CloudWatch metrics and graphs that are accessible within the AWS Transfer Family Management Console. Visit the\ndocumentation\nto learn more.\n\nYes. You can combine log streams from multiple AWS Transfer Family servers into a single CloudWatch log group. This allows you to create consolidated log metrics and visualizations, which can be added to CloudWatch dashboards for tracking server usage and performance.\n\n", "token_count": 472, "metadata": {"service": "AWS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "aws-faq-33", "chunk_start": 14771, "chunk_end": 15243, "heading_hierarchy": ["AWS Transfer Family FAQs", "Security and compliance", "Is AWS Transfer Family FISMA compliant?"]}}
{"content": "The framework library is the central place from which you can access and manage frameworks in AWS Audit Manager. It contains a catalog of standard frameworks pre-built by Audit Manager such as PCI DSS, CIS Foundation Benchmark, and HIPAA and custom frameworks you define. There are two ways to create a custom framework. You can make an editable copy of an existing framework, or you can create a new framework from scratch. When creating a custom framework, you can add controls from Audit Manager control library and organize controls into control sets in a way that suits your unique requirements.\n\nThe control library is the central place from which you can access and manage controls in AWS Audit Manager. It contains a catalog of standard controls pre-built by Audit Manager and custom controls you define. There are two ways to create a custom control. You can make an editable copy of an existing framework, or you can create a new control from scratch. When creating a custom control, you can specify the control name, description, testing information, and which evidence sources you want Audit Manager to automatically collect evidence from. You can also create a custom control that only asks for manual evidence to support those controls that require non-system evidence such as people organization and operation procedures.\n\n", "token_count": 267, "metadata": {"service": "AUDIT", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "audit-faq-5", "chunk_start": 2395, "chunk_end": 2662, "heading_hierarchy": ["AWS Audit Manager FAQs", "Working in AWS Audit Manager", "How do I create a custom framework?"]}}
{"content": "Every voice is unique, so it’s important that we learn more about your goals to accurately scope a Brand Voice engagement. If you are interested in building a Brand Voice using Amazon Polly, please reach out to your AWS Account Manager or\ncontact us\nfor more information.\n\nPlease see the\nAmazon Polly Pricing Page\nfor current pricing information.\n\nYes, you can. The service does not restrict this and there are no additional costs for doing so.\n\nYes, you can. The service does not restrict this and there are no additional costs for doing so.\n\nYes. You will be charged for every request for speech or Speech Marks based on the number of characters you send to the service.\n\nYes, as part of the\nAWS Free Usage Tier\n, you can get started with Amazon Polly for free. Upon sign-up, new Amazon Polly customers can synthesize millions of characters for free each month for the first 12 months. Please see the\nAmazon Polly Pricing Page\nfor current pricing information.\nStarting July 15, 2025, new AWS customers will receive up to $200 in AWS Free Tier credits, which can be applied towards eligible AWS services, including Amazon Polly. At account sign-up, you can choose between a free plan and a paid plan. The free plan will be available for 6 months after account creation. If you upgrade to a paid plan, any remaining Free Tier credit balance will automatically apply to your AWS bills. All Free Tier credits must be used within 12 months of your account creation date. To learn more about the AWS Free Tier program, refer to\nAWS Free Tier website\nand\nAWS Free Tier documentation\n.\n\nFor details on taxes, please see Amazon Web Services Tax Help.\n\n", "token_count": 398, "metadata": {"service": "POLLY", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "polly-faq-4", "chunk_start": 1781, "chunk_end": 2179, "heading_hierarchy": ["Amazon Polly FAQs", "General", "What is the cost and timeline to build a Brand Voice?"]}}
{"content": "AWS DataSync reduces the complexity and cost of online data transfer, making it simple to transfer datasets to and from on-premises storage, edge locations, other cloud providers and AWS Storage services. DataSync connects to existing storage systems and data sources with standard storage protocols (NFS, SMB), as an HDFS client, using the Amazon S3 API, or using other cloud storage APIs. It uses a purpose-built network protocol and scale-out architecture to accelerate data transfer between storage systems and AWS services. DataSync handles moving files and objects, scheduling data transfers, monitoring the progress of transfers, encryption, verification of data transfers, and notifying you of any issues.\n\nDataSync supports the following storage location types: Network File System (NFS) shares, Server Message Block (SMB) shares, Hadoop Distributed File Systems (HDFS), self-managed object storage, object storage in other clouds such as Google Cloud Storage and Wasabi Cloud Storage (see the\nfull list of support clouds\n), Azure Files, Azure Blob Storage (including Azure Data Lake Storage Gen2), Amazon S3 compatible storage on Snow, Amazon Simple Storage Service (Amazon S3), Amazon Elastic File System (Amazon EFS) file systems, Amazon FSx for Windows File Server file systems, Amazon FSx for Lustre file systems, Amazon FSx for OpenZFS file systems, and Amazon FSx for NetApp ONTAP file systems.\n\n", "token_count": 321, "metadata": {"service": "DATASYNC", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "datasync-faq-1", "chunk_start": 469, "chunk_end": 790, "heading_hierarchy": ["AWS DataSync FAQs", "General", "What problem does AWS DataSync solve for me?"]}}
{"content": "Normal Amazon S3 rates apply for every version of an object stored or requested. For example, let’s look at the following scenario to illustrate storage costs when utilizing Versioning (let’s assume the current month is 31 days long): 1) Day 1 of the month: You perform a PUT of 4 GB (4,294,967,296 bytes) on your bucket.\n2) Day 16 of the month: You perform a PUT of 5 GB (5,368,709,120 bytes) within the same bucket using the same key as the original PUT on Day 1.\nWhen analyzing the storage costs of the above operations, note that the 4 GB object from Day 1 is not deleted from the bucket when the 5 GB object is written on Day 15. Instead, the 4 GB object is preserved as an older version and the 5 GB object becomes the most recently written version of the object within your bucket. At the end of the month: Total Byte-Hour usage\n[4,294,967,296 bytes x 31 days x (24 hours / day)] + [5,368,709,120 bytes x 16 days x (24 hours / day)] = 5,257,039,970,304 Byte-Hours. Conversion to Total GB-Months\n5,257,039,970,304 Byte-Hours x (1 GB / 1,073,741,824 bytes) x (1 month / 744 hours) = 6.581 GB-Month The cost is calculated based on the current rates for your region on the\nAmazon S3 pricing page\n.\n\n", "token_count": 411, "metadata": {"service": "S3", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "s3-faq-42", "chunk_start": 16707, "chunk_end": 17118, "heading_hierarchy": ["Amazon S3 FAQs", "Billing", "How am I charged for using Versioning?"]}}
{"content": "Throughput Optimized HDD (st1) and Cold HDD (sc1) volumes attached to EBS-optimized instances are designed to offer consistent performance, delivering within 10% of the expected throughput performance 99% of the time in a given year. There are several factors that could affect the level of consistency you see. For example, the relative balance between random and sequential I/O operations on the volume can impact your performance. Too many random small I/O operations will quickly deplete your I/O credits and lower your performance down to the baseline rate. Your throughput rate may also be lower depending on the instance selected. Although st1 can drive throughput up to 500 MB/s, performance will be limited by the separate instance-level limit for EBS traffic. Another factor is taking a snapshot which will decrease expected write performance down to the baseline rate, until the snapshot completes. This is specific to st1 and sc1.\nYour performance can also be impacted if your application isn’t sending enough I/O requests. This can be monitored by looking at your volume’s queue depth and I/O size. The queue depth is the number of pending I/O requests from your application to your volume. For maximum consistency, HDD-backed volumes must maintain an average queue depth (rounded to the nearest whole number) of four or more for every 1 MB sequential I/O. For more information about ensuring consistent performance of your volumes, see\nIncreasing EBS Performance\n.\n\nYes. You can stripe multiple volumes together to achieve up to 400,000 IOPS or 12,500 Mbps when attached to larger EC2 instances. We recommend using io2 Block Express volumes for higher performance requirements without needing the operational management of striping multiple volumes. Performance for st1 and sc1 scales linearly with volume size so there may not be as much of a benefit to stripe these volumes together.\n\n", "token_count": 425, "metadata": {"service": "EBS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "ebs-faq-6", "chunk_start": 2215, "chunk_end": 2640, "heading_hierarchy": ["Amazon EBS FAQs", "Performance", "What level of performance consistency can I expect to see from my HDD-backed volumes?"]}}
{"content": "You should use the multi-container jobs feature if you want to model your AWS Batch workload as a set of logically distinct elements, for example, the simulation environment and system under test (SUT), main application, or telemetry sidecar. Using this feature will simplify your operations, make it easier to follow best architectural practices, and allow you to align simulations with the multi-container architecture of your production system-of-systems. Whether you are looking to run separate containers for your SUTs and simulation environment or need to add an auxiliary sidecar, you no longer need to combine all workload elements into a monolithic container and rebuild it after every code change. As a result, you can simplify DevOps, keep containers small and fast to download, and facilitate parallelization of work.\n\nAWS Batch supports running multiple containers in all job types including single-node regular jobs, array jobs, and multi-node parallel (MNP) jobs.\n\nYou can run multi-container jobs in all AWS Batch compute environments including Amazon ECS, Amazon EC2, AWS Fargate, and Amazon EKS.\n\nAWS Batch manages compute environments and job queues, allowing you to easily run thousands of jobs of any scale using\nAmazon ECS\n,\nAmazon EKS\n, and\nAWS Fargate\nwith an option between Spot or on-demand resources. You simply define and submit your batch jobs to a queue. In response, AWS Batch chooses where to run the jobs, launching additional AWS capacity if needed. AWS Batch carefully monitors the progress of your jobs. When capacity is no longer needed, AWS Batch will remove it. AWS Batch also provides the ability to submit jobs that are part of a pipeline or workflow, enabling you to express any interdependencies that exist between them as you submit jobs.\n\nAWS Batch supports any job that can executed as a Docker container. Jobs specify their memory requirements and number of vCPUs.\n\nAn AWS Batch Compute Resource is an EC2 instance or AWS Fargate compute resource.\n\n", "token_count": 454, "metadata": {"service": "BATCH", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "batch-faq-3", "chunk_start": 1048, "chunk_end": 1502, "heading_hierarchy": ["AWS Batch FAQs", "Multi-container jobs", "Why should I use multi-container jobs for AWS Batch?"]}}
{"content": "By subscribing AWS Lambda functions to Amazon SNS topics, you can perform custom message handling. You can invoke an AWS Lambda function to provide custom message delivery handling by first publishing a message to an AWS Lambda function, have your Lambda function modify a message (e.g. localize language) and then filter and route those messages to other topics and endpoints. Apps and services that already send Amazon SNS notifications, such as Amazon CloudWatch, can now immediately take advantage of AWS Lambda without having to provision or manage infrastructure for custom message handling. You can also use delivery to an AWS Lambda function as a way to publish to other AWS services such as Amazon Kinesis or Amazon S3. You can subscribe an AWS Lambda function to the Amazon SNS topic, and then have the Lambda function in turn write to another service.\n\nYou need to first create an AWS Lambda function via your AWS account and the\nAWS Lambda console\n, and then subscribe that AWS Lambda function to a topic using the\nAmazon SNS console\nor the\nAmazon SNS APIs\n. Once that is complete, any messages that you publish to the Amazon SNS topics which have Lambda functions subscribed to them will be delivered to the appropriate Lambda functions in addition to any other destinations subscribed to that topic.\n\nA message delivery from Amazon SNS to an AWS Lambda function creates an instance of the AWS Lambda function and invokes it with your message as an input. For more information on message formats, please refer to the\nAmazon SNS documentation\nand the AWS Lambda documentation.\n\nPublishing a message with Amazon SNS costs $0.50 per million requests. Aside from charges incurred in using AWS services, there are no additional fees for delivering a message to an AWS Lambda function. Amazon SNS has a Free Tier of 1 million requests per month. For more information, please refer to\nAmazon SNS pricing\n. AWS Lambda function costs are based on the number of requests for your functions and the time your code executes. The AWS Lambda Free-Tier includes 1M requests per month and 400,000 GB-seconds of compute time per month. For more information, please refer to\nAWS Lambda pricing\n.\n\n", "token_count": 497, "metadata": {"service": "SNS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "sns-faq-28", "chunk_start": 12616, "chunk_end": 13113, "heading_hierarchy": ["Amazon SNS FAQs", "SNS support for AWS Lambda", "What can I do with AWS Lambda functions and Amazon SNS?"]}}
{"content": "A dead-letter queue is an Amazon SQS queue to which a source queue can send messages if the source queue’s consumer application is unable to consume the messages successfully. Dead-letter queues make it easier for you to handle message consumption failures and manage the life cycle of unconsumed messages. You can configure an alarm for any messages delivered to a dead-letter queue, examine logs for exceptions that might have caused them to be delivered to the queue, and analyze message contents to diagnose consumer application issues. Once you recover your consumer application, you can redrive the messages from your dead-letter queue to the source queue.\n\nWhen you create your source queue, Amazon SQS allows you to specify a dead-letter queue (DLQ) and the condition under which SQS should move messages to the DLQ. The condition is the number of times a consumer can receive a message from the queue, defined as maxReceiveCount. This configuration of a dead-letter queue with a source queue and the maxReceiveCount is known as the redrive policy. When the ReceiveCount for a message exceeds the maxReceiveCount for a queue, Amazon SQS is designed to move the message to a dead-letter queue (with its original message ID). For example, if the source queue has a redrive policy with maxReceiveCount set to five, and the consumer of the source queue receives a message six times without successfully consuming it, SQS moves the message to the dead-letter queue.\nThe redrive policy manages the first half of the life cycle of unconsumed messages by moving them from a source queue to a dead-letter queue. Now the dead-letter queue redrive to source queue efficiently completes the cycle by moving those messages back to their source queue, as shown below.\n\nFirst, it allows you to investigate a sample of messages available in the dead-letter queue by showing message attributes and related metadata. Then, once you have investigated the messages, you can move them back to their source queue(s). You can also select the redrive velocity to configure the rate at which Amazon SQS will move the messages from the dead-letter queue to the source queue.\n\nYes. However, you must use a FIFO dead letter queue with a FIFO queue. (Similarly, you can use only a standard dead letter queue with a standard queue.)\n\n", "token_count": 503, "metadata": {"service": "SQS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "sqs-faq-20", "chunk_start": 8670, "chunk_end": 9173, "heading_hierarchy": ["Amazon SQS FAQs", "Dead-letter queues", "What are dead-letter queues?"]}}
{"content": "ACM attempts to validate ownership or control of each domain name in your certificate request, according to the validation method you chose, DNS or email, when making the request. The status of the certificate request is Pending validation while ACM attempts to validate that you own or control the domain. Refer to the\nDNS validation\nand\nEmail validation\nsections below for more information about the validation process. After all of the domain names in the certificate request are validated, the time to issue certificates may be several hours or longer. When the certificate is issued, the status of the certificate request changes to Issued and you can start using it with other AWS services that are integrated with ACM.\n\nYes. DNS Certificate Authority Authorization (CAA) records allow domain owners to specify which certificate authorities are authorized to issue certificates for their domain. When you request an ACM Certificate, AWS Certificate Manager looks for a CAA record in the DNS zone configuration for your domain. If a CAA record is not present, then Amazon can issue a certificate for your domain. Most customers fall into this category.\nIf your DNS configuration contains a CAA record, that record must specify one of the following CAs before Amazon can issue a certificate for your domain: amazon.com, amazontrust.com, awstrust.com, or amazonaws.com. Refer to\nConfigure a CAA Record\nor\nTroubleshooting CAA Problems\nin the AWS Certificate Manager User Guide for more information.\n\nWith DNS validation, you can validate your ownership of a domain by adding a CNAME record to your DNS configuration. DNS Validation makes it easy for you to establish that you own a domain when requesting public SSL/TLS certificates from ACM.\n\n", "token_count": 382, "metadata": {"service": "CERTIFICATE", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "certificate-faq-10", "chunk_start": 4584, "chunk_end": 4966, "heading_hierarchy": ["AWS Certificate Manager FAQs", "Provision Public Certificates", "What happens when I request a public certificate?"]}}
{"content": "Yes, it is possible to configure EMR Serverless applications across multiple AZs. The process to set up multiple AZs depends on the type of workers that you use.\nWhen using On-Demand workers only, EMR Serverless distributes jobs across multiple AZs by default, but each job runs only in one AZ. You can choose which AZs to use by associating subnets with the AZs. If an AZ fails, EMR Serverless automatically runs your job in another healthy AZ.\nWhen using Pre-Initialized workers, EMR Serverless selects a healthy AZ from the subnets that you specify. Jobs are submitted in that AZ until you stop the application. If an AZ becomes impaired, you can restart the application to switch to another healthy AZ.\n\nEMR Serverless can only access certain AWS resources in the same Region when configured without VPC connectivity. See\nconsiderations\n. To access AWS resources in a different Region or non-AWS resources, you will need to\nsetup VPC access and a NAT gateway\nto route to public endpoints for the AWS resources.\n\nAmazon EMR Serverless application- and job-level metrics are published every 30 seconds to Amazon CloudWatch.\n\nFrom EMR Studio, you can select a running or completed EMR Serverless job and then click on the Spark UI or Tez UI button to launch them.\n\nYes, you can configure Amazon EMR Serverless applications to access resources in your own VPC. See the\nConfiguring VPC access\nsection in the documentation to learn more.\n\nEach EMR Serverless application is isolated from other applications and runs on a secure Amazon VPC.\n\nAmazon EMR Serverless is introducing a new service quota called Max concurrent vCPUs per account. This vCPU-based quota allows you to set the maximum number of aggregate vCPUs your applications are able to scale up to within a Region. The existing application-level, worker-based quotas (Maximum active workers) will will reach end of support on February 1, 2023.\n\nYou can view, manage, and request quota increase in the AWS Service Quotas Management console. For more information, see Requesting a Quota Increase in the Service Quotas User Guide.\n\n", "token_count": 508, "metadata": {"service": "EMR", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "emr-faq-24", "chunk_start": 10921, "chunk_end": 11429, "heading_hierarchy": ["Amazon EMR FAQs", "EMR Serverless", "Q: Can I configure EMR Serverless applications in multiple Availability Zones (AZ)?"]}}
{"content": "Management and Governance\n›\nAWS Organizations\n›\nFAQs\n\nGeneral\n4\nCore Concepts\n7\nOrganizing AWS accounts\n15\nControl Management\n13\nBilling\n3\nIntegrated AWS Services\n3\n\nOrganizing AWS accounts\n\nIntegrated AWS Services\n\nAWS Organizations\nhelps you centrally govern your environment as you scale your workloads on AWS. Whether you are a growing startup or a large enterprise, Organizations helps you to programmatically create new accounts and allocate resources, simplify billing by setting up a single payment method for all of your accounts, create groups of accounts to organize your workflows, and apply policies to these groups for governance. In addition, AWS Organizations is integrated with other AWS services so you can define central configurations, security mechanisms, and resource sharing across accounts in your organization.\n\nAWS Organizations enables the following capabilities:\nAutomate AWS account creation and management, and provision resources with AWS CloudFormation Stacksets\nMaintain a secure environment with policies and management of AWS security services\nGovern access to AWS services, resources, and regions\nCentrally manage policies across multiple AWS accounts\nAudit your environment for compliance\nView and manage costs with consolidated billing\nConfigure AWS services across multiple accounts\n\nRefer to the AWS Organizations Region availability in the\ndocumentation\n.\n\nAWS Control Tower\nhelps you create and manage a multi-account AWS environment with built-in best practices. Built on AWS Organizations, it automatically sets up account structures and implements preventive guardrails using SCPs and RCPs. Whether you're new to AWS, starting a cloud initiative, or managing existing accounts, AWS Control Tower provides automated governance while maintaining agility. You can enhance baseline guardrails with custom AWS Organizations policies for centralized control across accounts via services integrated with AWS Organizations. AWS Control Tower combines automated setup, prescriptive guidance, and flexible controls to simplify multi-account management at any stage of cloud adoption.\n\nAn organization is a collection of AWS accounts that you can organize into a hierarchy and manage centrally.\n\n", "token_count": 463, "metadata": {"service": "ORGANIZATIONS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "organizations-faq-0", "chunk_start": 0, "chunk_end": 463, "heading_hierarchy": []}}
{"content": "When you find a product that meets your requirements in the portal, choose Launch. You will be guided through a series of questions about how you plan to use the product. The questions might be about your business needs or your infrastructure requirements (such as “Which EC2 instance type?”). When you have provided the required information, you’ll see the product in the AWS Service Catalog console. While the product is being provisioned, you will see that it is “in progress.” After provisioning is complete, you will see “complete” and information, such as endpoints or Amazon Resource Names (ARNs), that you can use to access the product.\n\nYes. You can see which products you are using in the AWS Service Catalog console. You can see all of the stacks that are in use, along with the version of the product used to create them.\n\nWhen a new version of a product is published, you can use the Update Stack command to use that version. If you are currently using a product for which there is an update, it continues to run until you close it, at which point you can choose to use the new version.\n\nYou can see the products that you are using and their health state in the AWS Service Catalog console.\n\nAWS Service Catalog enables customers using Terraform open source and Terraform Cloud to provide self-service provisioning with governance to their end users in AWS. Central IT can use a single tool to organize, govern, and distribute their Terraform configurations within AWS at scale. They can access AWS Service Catalog key features, including cataloging of standardized and pre-approved templates, access control, least privileges during provisioning, versioning, sharing to thousands of AWS accounts, and tagging. End-users simply see the list of products and versions they have access to, and can deploy them in a single action.\nTo get started, use the AWS-provided Terraform Reference Engine for Terraform open source or Terraform Reference Engine for Terraform Cloud that installs and configures the code and infrastructure required for the Terraform open-source engine to work with AWS Service Catalog. This one-time setup takes just minutes.\nTo learn how to catalog, govern, share, and deploy Terraform products using AWS Service Catalog, read our\ndocumentation\n.\n\n", "token_count": 496, "metadata": {"service": "SERVICECATALOG", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "servicecatalog-faq-6", "chunk_start": 2468, "chunk_end": 2964, "heading_hierarchy": ["AWS Service Catalog FAQs", "End user", "How do I deploy a product?"]}}
{"content": "Q: How does instance size flexibility work?\nEC2 uses the scale shown below to compare different sizes within an instance family. In the case of instance size flexibility on RIs, this scale is used to apply the discounted rate of RIs to the normalized usage of the instance family. For example, if you have an m5.2xlarge RI that is scoped to a region, then your discounted rate could apply towards the usage of 1 m5.2xlarge or 2 m5.xlarge instances.\nClick here\nto learn more about how instance size flexibility of RIs applies to your EC2 usage. And\nclick here\nto learn about how instance size flexibility of RIs is presented in the Cost and Usage Report.\nInstance Size\nNormalization Factor\nnano\n0.25\nmicro\n0.5\nsmall\n1\nmedium\n2\nlarge\n4\nxlarge\n8\n2xlarge\n16\n4xlarge\n32\n8xlarge\n64\n9xlarge\n72\n10xlarge\n80\n12xlarge\n96\n16xlarge\n128\n18xlarge\n144\n24xlarge\n192\n32xlarge\n256\nQ: Can I change my RI during its term?\nYes, you can modify the AZ of the RI, change the scope of the RI from AZ to Region (and the other way around), or modify instance sizes within the same instance family (on the Linux/Unix platform).\nQ: Can I change the instance type of my RI during its term?\nYes. Convertible RIs offer you the option to change the instance type, operating system, tenancy or payment option of your RI during its term. Please refer to the Convertible RI section of the FAQ for additional information.\nQ: What are the different payment options for RIs?\n", "token_count": 423, "metadata": {"service": "EC2", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "ec2-faq-144", "chunk_start": 60483, "chunk_end": 60906, "heading_hierarchy": ["Amazon EC2 FAQs", "Billing and purchase options", "Reserved Instances"]}}
{"content": "Container Insights with enhanced observability enables you to visually drill up and down across your Amazon EKS and Amazon ECS container layers and easily\nspot issues like memory leaks in individual containers, reducing mean time to resolution. Container level visibility comes out-of-the-box with enhanced observability. To enable enhanced observability, please follow the steps provided in the\nAmazon CloudWatch Container Insights documentation\n.\n\nYes. Using Container Insights with enhanced observability for Amazon Elastic Kubernetes Service (EKS) you can monitor your control plane status. You can use it to understand autoscaling status and plan your test cluster lifecycles in your automated test capabilities for example.\n\nContainer Insights with enhanced observability is an optional feature that provides out-of-the-box detailed health and performance metrics, which include container-level ECS and EKS performance metrics, EKS Kube-state metrics, and EKS control plane metrics for faster problem isolation and troubleshooting.  Without enhanced observability, Container Insights provides aggregated metrics at cluster and service levels.\n\nYes. You can decide to use Container Insights with or without enhanced observability on a per-cluster basis. For EKS you can enable enhanced observability for your clusters by installing the CloudWatch Observability add-on in your clusters after they are created using the add-ons tab in your cluster info view. For ECS you can toggle Enhanced under Monitoring Tab in cluster create workflow or update your existing clusters to do the same to onboard Container Insights with enhanced observability. You can also onboard enhanced observability at account level in ECS. This will enable any net new clusters under that account to onboard Container Insights with enhanced observability out-of-the-box. Please see the\nCloudWatch Container Insights documentation\nfor details.\n\nYou can get started collecting detailed performance metrics, logs, and metadata from your containers and clusters by installing CloudWatch Observability add-on into your EKS clusters or by opting in at cluster or account level for ECS. To start using Container Insights, please follow the steps provided in the\nAmazon CloudWatch Container Insights documentation\n.\n\nContainer Insights with enhanced observability supports Amazon EKS running on EC2, Amazon ECS running on EC2 and AWS Fargate.\n\n", "token_count": 501, "metadata": {"service": "CLOUDWATCH", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "cloudwatch-faq-13", "chunk_start": 5317, "chunk_end": 5818, "heading_hierarchy": ["Amazon CloudWatch FAQs", "Container Monitoring", "How can I monitor my container level health and performance?"]}}
{"content": "Q. What is a Spot Instance?\nSpot Instances are spare EC2 capacity that can save you up to 90% off of On-Demand prices that AWS can interrupt with a 2-minute notification. Spot uses the same underlying EC2 instances as On-Demand and Reserved Instances, and is best suited for fault-tolerant, flexible workloads. Spot Instances provides an additional option for obtaining compute capacity and can be used along with On-Demand and Reserved Instances.\nQ. How is a Spot Instance different than an On-Demand instance or Reserved Instance?\nWhile running, Spot Instances are exactly the same as On-Demand or Reserved instances. The main differences are that Spot Instances typically offer a significant discount off the On-Demand prices, your instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification, and Spot prices adjust gradually based on long term supply and demand for spare EC2 capacity.\nSee\nhere\nfor more details on Spot Instances.\nQ. How do I purchase and start up a Spot instance?\nSpot instances can be launched using the same tools you use to launch instances today, including AWS Management Console, Auto-Scaling Groups, Run Instances and Spot Fleet. In addition many AWS services support launching Spot instances such as EMR, ECS, Datapipeline, CloudFormation and Batch.\nTo start up a Spot Instance, you simply need to choose a Launch Template and the number of instances you would like to request.\nSee\nhere\nfor more details on how to request Spot Instances.\nQ. How many Spot Instances can I request?\nYou can request Spot Instances up to your Spot limit for each region. Note that customers new to AWS might start with a lower limit. To learn more about Spot Instance limits, please refer to the\nAmazon EC2 User Guide\n.\nIf you would like a higher limit, complete the\nAmazon EC2 instance request form\nwith your use case and your instance increase will be considered. Limit increases are tied to the region they were requested for.\nQ. What price will I pay for a Spot Instance?\nYou pay the Spot price that’s in effect at the beginning of each instance-hour for your running instance. If Spot price changes after you launch the instance, the new price is charged against the instance usage for the subsequent hour.\n", "token_count": 511, "metadata": {"service": "EC2", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "ec2-faq-155", "chunk_start": 65122, "chunk_end": 65633, "heading_hierarchy": ["Amazon EC2 FAQs", "Billing and purchase options", "Spot Instances"]}}
{"content": "No. VM Import/Export commands are available via EC2 CLI and API. You can also use the\nAWS Management Portal for vCenter\nto import VMs into Amazon EC2. Once imported, the resulting instances are available for use via the AWS Management Console.\n\n", "token_count": 58, "metadata": {"service": "EC2", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "ec2-faq-124", "chunk_start": 52532, "chunk_end": 52590, "heading_hierarchy": ["Amazon EC2 FAQs", "Management", "VM Import/Export"]}}
{"content": "All publicly available data sets on the\nRegistry of Open Data\non AWS are also available through\nAWS Data Exchange\n. On AWS Data Exchange, customers can now discover and access more than 100 petabytes of high-value, cloud-optimized data sets available for public use from leading organizations such as NOAA, NASA, or the UK Met Office. These include open data sets hosted by the\nAWS Open Data Sponsorship Program\nand in the\nAmazon Sustainability Data Initiative\n(ASDI) catalog. Open data sets are different than other commercial or free data sets in four key ways. First, AWS Data Exchange requires customers to explicitly agree to the Data Subscription Agreement outlining the terms that the data provider set when publishing their product, whereas open data does not have terms of use and is only governed by the provider specific open data license. Second, customers must authenticate using an AWS account to subscribe to commercial or free data sets whereas open data sets can be accessed without any authentication via S3 APIs. Third, AWS Data Exchange gives data providers access to daily, weekly and monthly reports detailing subscription activity, whereas with Registry of Open Data on AWS, data providers must analyze their own logs to track usage of data. Finally, to become a data provider on AWS Data Exchange, qualified customers must\nregister as a data provider\non the AWS Marketplace Management Portal to be eligible to list both free and commercial products, whereas any customer can add free data to\nRegistry of Open Data on AWS through GitHub\nand may apply to the Open Data Sponsorship Program for AWS to sponsor the costs of storage and bandwidth for select open data sets.\n\nAWS Data Exchange for APIs is a feature that enables customers to find, subscribe to, and use third-party API products from providers on AWS Data Exchange. With AWS Data Exchange for APIs, customers can use AWS-native authentication and governance, explore consistent API documentation, and utilize supported AWS SDKs to make API calls. Now, by adding their APIs to the AWS Data Exchange catalog, data providers can reach millions of AWS customers that consume API-based data and more easily manage subscriber authentication, entitlement, and billing.\n\n", "token_count": 470, "metadata": {"service": "DATA", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "data-faq-1", "chunk_start": 307, "chunk_end": 777, "heading_hierarchy": ["FAQs", "General", "What is the difference between AWS Data Exchange and the Registry of Open Data on AWS?"]}}
{"content": "Artificial Intelligence\n›\nGenerative AI\n›\nAmazon Q\n\nThe most capable generative AI–powered assistant for accelerating software development and leveraging companies' internal data\n\nAmazon Q is a generative AI assistant that transforms how work gets done in your organization. With specialized capabilities for software developers, business intelligence analysts, contact center employees, supply chain analysts, and anyone building with AWS, Amazon Q helps every employee get insights on their data and accelerate their tasks. Leveraging Amazon Q's advanced agentic capabilities, companies can streamline processes, get to decisions faster, and help employees be more productive.\n\nAmazon Q Business makes generative AI securely accessible to everyone in your organization. Leveraging your own company's content, data, and systems, Amazon Q Business makes it easier to get you fast, relevant answers to pressing questions, solve problems, generate content, and take actions on your behalf. Amazon Q Business easily and securely connects to commonly used systems and tools so it can synthesize everything and provide tailored assistance empowering your teams to be more data-driven, creative, and productive.\n\nAmazon Q Developer is the most capable generative AI-powered assistant for building, operating, and transforming software, with advanced capabilities for managing data and AI/ML. Amazon Q Developer goes beyond coding to help developers and IT professionals with all of their tasks— from coding, testing, and deploying, to troubleshooting, performing security scanning and fixes, modernizing applications, optimizing AWS resources, and creating data engineering pipelines. Data scientists can get guidance to quickly and easily build analytics, AI/ML, and generative AI applications.\n\nAmazon QuickSight\nAmazon Connect\nAWS Supply Chain\n\n", "token_count": 377, "metadata": {"service": "Q", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "q-faq-0", "chunk_start": 0, "chunk_end": 377, "heading_hierarchy": []}}
{"content": "AWS DMS is an affordable, low-cost option to migrate your databases and analytics workloads. You pay only for the replication instances and any additional log storage. Data transfer is free. You can find full pricing details on the\nDMS pricing page\n.\n\nAWS DMS Schema Conversion is free to use as a part of DMS. Pay only for the storage used.\n\nDuring a typical simple database migration, you will create a target database, migrate the database schema, set up the data replication process, initiate the full load and a subsequent change data capture and apply, and conclude with a switchover of your production environment to the new database once the target database is caught up with the source database.\n\nThe only difference is in the last step (the production environment switchover), which is absent for continuous data replication. Your data replication task will run until you change or terminate it.\n\nYes. AWS Database Migration Service has a variety of metrics displayed in the AWS Management Console. It provides an end-to-end view of the data replication process, including diagnostic and performance data for each point in the replication pipeline.\nAWS Database Migration Service also integrates with other AWS services such as\nCloudTrail\nand\nCloudWatch\nLogs. You can also leverage the\nAWS Database Migration Service API\nand\nAWS Command Line Interface (AWS CLI)\nto integrate with your existing tools or build custom monitoring tools to suit your specific needs.\n\nAWS Database Migration Service provides a provisioning API that allows creating a replication task directly from your development environment, or scripting their creation at scheduled times during the day.\nThe service API and CLI allows developers and database administrators to automate the creation, restart, management and termination of replication tasks.\n\nAWS Database Migration Service (DMS) supports a range of homogeneous and heterogeneous data replications.\nEither the source or the target database (or both) need to reside in RDS or on EC2. Replication between on-premises to on-premises databases is not supported.\nSupported DMS sources\nSupported DMS targets\n\n", "token_count": 471, "metadata": {"service": "DMS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "dms-faq-1", "chunk_start": 478, "chunk_end": 949, "heading_hierarchy": ["AWS Database Migration Service FAQs", "General", "How much does AWS DMS cost?"]}}
{"content": "PostgreSQL extensions are executed in the same process space for high performance. However, extensions might have software defects that can crash the database.\nTLE for PostgreSQL\noffers multiple layers of protection to mitigate this risk. TLE is designed to limit access to system resources. The\nrds_superuser\nrole can determine who is permitted to install specific extensions. However, these changes can only be made through the TLE API. TLE is designed to limit the impact of an extension defect to a single database connection. In addition to these safeguards, TLE is designed to provide DBAs in the\nrds_superuser\nrole fine-grained, online control over who can install extensions and they can create a permissions model for running them.\nOnly users with sufficient privileges will be able to run and create using the “CREATE EXTENSION” command on a TLE extension. DBAs can also allow-list “PostgreSQL hooks” required for more sophisticated extensions that modify the database’s internal behavior and typically require elevated privilege.\n\nTLE for PostgreSQL\nis available for\nAmazon Aurora PostgreSQL-Compatible Edition\nand\nAmazon RDS on PostgreSQL\non versions 14.5 and higher. TLE is implemented as a PostgreSQL extension itself and you can activate it from the rds_superuser role similar to other extensions supported on Aurora and Amazon RDS.\n\nYou can run\nTLE for PostgreSQL\nin PostgreSQL 14.5 or higher in\nAmazon Aurora\nand\nAmazon RDS\n.\n\nTLE for PostgreSQL\nis currently available in all AWS Regions (excluding AWS China Regions) and the AWS GovCloud Regions.\n\nTLE for PostgreSQL\nis available to\nAurora\nand\nAmazon RDS\ncustomers at no additional cost.\n\n", "token_count": 413, "metadata": {"service": "RDS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "rds-faq-53", "chunk_start": 20668, "chunk_end": 21081, "heading_hierarchy": ["Amazon RDS FAQs", "Trusted Language Extensions for PostgreSQL", "What are traditional risks of running extensions in PostgreSQL and how does TLE for PostgreSQL mitigate those risks?"]}}
{"content": "AWS WAF application layer (L7) DDoS protection is an AWS Managed Rule group that is designed to automatically defend applications against distributed denial of service (DDoS) events within seconds. This feature monitors traffic data to establish a baseline within minutes of activation, then leverages machine learning models to detect anomalies from normal traffic patterns. When traffic exceeds or deviates from the established baseline, the system automatically applies rules designed to help block malicious requests. It is designed to ensure your applications on Amazon CloudFront, Application Load Balancer and API Gateway remain available against emerging DDoS events.\nAWS WAF application layer (L7) DDoS protection allows you to protect your applications without the complexity of manually configuring and managing rules. This feature has customization options to fit the needs of your applications such as configuring rule sensitivity settings and inspection of specific application URI paths. The AWS WAF security dashboard provides visibility into application layer events and mitigations in one place. Here you can view event sources and top request labels added by this rule group.\nTo learn more, visit the\ntechnical documentation page\n.\n\nApplication layer (L7) DDoS protection is designed to automatically defend your cloud applications against layer 7 DDoS events within seconds. With this feature, you have application layer protection without the complexity of manual rule configuration and management.\nRapid Response:\nDesigned to mitigate application layer DDoS events within seconds.\nSave Time:\nAWS Managed Rule groups for AWS WAF are already configured to save you time.\nCustomizable Protection:\nTailor your DDoS defense to suit your application with sensitivity controls.\n\nCurrently, resources on Amazon CloudFront, Application Load Balancer (ALB) and API Gateway are protected by this feature. You can configure protection for additional resources with an AWS Shield Advanced subscription.\n\nOnce enabled, baselining typically takes minutes. This rule group is designed to help mitigate suspicious traffic within seconds by leveraging threat intelligence data and comparing against normal traffic baselines.\n\n", "token_count": 442, "metadata": {"service": "WAF", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "waf-faq-10", "chunk_start": 4429, "chunk_end": 4871, "heading_hierarchy": ["AWS WAF FAQs", "AWS WAF application layer (L7) DDoS protection", "What is AWS WAF application layer (L7) DDoS protection?"]}}
{"content": "The\nAWS Free Tier for Amazon RDS\noffer provides free use of Single-AZ Micro DB instances running MySQL, MariaDB, PostgreSQL, and SQL Server Express Edition. The free usage tier is capped at 750 instance hours per month. Customers also receive 20 GB of General Purpose (SSD) database storage and 20 GB of backup storage for free per month.\n\nNew AWS accounts receive 12 months of AWS Free Tier access. Please see the\nAWS Free Tier FAQs\nfor more information.\n\nYes. You can run more than one Single-AZ Micro DB instance simultaneously and be eligible for usage counted under the AWS Free Tier for Amazon RDS. However, any use exceeding 750 instance hours, across all Amazon RDS Single-AZ Micro DB instances and across all eligible database engines and regions, will be billed at standard Amazon RDS prices.\nFor example, if you run two Single-AZ Micro DB instances for 400 hours each in a single month, you will accumulate 800 instance hours of usage, of which 750 hours will be free. You will be billed for the remaining 50 hours at the standard Amazon RDS price.\n\nNo. A customer with access to the AWS Free Tier can use up to 750 instance hours of Micro instances running either MySQL, PostgreSQL, or SQL Server Express Edition. Any use exceeding 750 instance hours, across all Amazon RDS Single-AZ Micro DB instances and across all eligible database engines and regions, will be billed at standard Amazon RDS prices.\n\nYou are billed at standard Amazon RDS prices for instance hours beyond what the Free Tier provides. See the\nAmazon RDS pricing page\nfor details.\n\nAmazon RDS reserved instances\ngive you the option to reserve a DB instance for a one or three year term and in turn receive a significant discount compared to the on-demand instance pricing for the DB instance. There are three RI payment options -- No Upfront, Partial Upfront, All Upfront -- which enable you to balance the amount you pay upfront with your effective hourly price.\n\n", "token_count": 483, "metadata": {"service": "RDS", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "rds-faq-15", "chunk_start": 5490, "chunk_end": 5973, "heading_hierarchy": ["Amazon RDS FAQs", "Free Tier", "What does the AWS Free Tier for Amazon RDS offer?"]}}
{"content": "Amazon Personalize has a simple, three-step process, which only takes a few clicks in the AWS Management Console, or a set of simple API calls. First, point Amazon Personalize to your user interaction data (historical log of views, clicks, purchases, etc.) in Amazon S3, upload the data using an easy API call, or use SageMaker Data Wrangler to prep and import your data. Optionally, you can provide an items or users dataset that contains additional information about your catalog and customer base. Second, with just a few clicks in the console or an API call, train a custom private recommendation model for your data. Third, retrieve personalized recommendations.\nWatch this Amazon Personalize Deep Dive video series to learn more.\n\nGet started by creating an account and accessing the Amazon Personalize developer console, which walks you through an intuitive set-up wizard. You have the option of using a JavaScript API and Server-Side SDKs to send real-time activity stream data to Amazon Personalize or bootstrapping the service using a historical log of user events. You can also import your data via Amazon Simple Storage Service (S3) or by using SageMaker Data Wrangler. Then, with only a few API calls, you can train a personalization model, either by letting the service choose the right algorithm for your dataset with AutoML or manually choosing one of the several algorithm options available. Once trained, the models can be deployed with a single API call and can then be used by production applications. When deployed, call the service from your production services to get real-time recommendations, and Amazon Personalize will automatically scale to meet demand.\n\n", "token_count": 356, "metadata": {"service": "PERSONALIZE", "doc_type": "Guide", "source_file": "faq.html", "chunk_id": "personalize-faq-3", "chunk_start": 909, "chunk_end": 1265, "heading_hierarchy": ["Amazon Personalize FAQs", "Using Amazon Personalize", "How does Amazon Personalize work?"]}}
